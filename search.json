[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Regression Monkeys",
    "section": "",
    "text": "Welcome\nこのQuarto Bookは以下のシリーズと連動して運用されています:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Statistics for Regression Monkeys",
    "section": "References",
    "text": "References\n\n\n柳川堯 (2018), P値:\nその正しい理解と適用, 近代科学社.\n\n\n永田靖 (2003), サンプルサイズの決め方,\n朝倉書店.\n\n\n竹村彰通 (2020), 現代数理統計学,\n学術図書出版社.\n\n\n藤澤洋徳 (2017), ロバスト統計\n: 外れ値への対処の仕方（ISMシリーズ : 進化する統計数理 /\n統計数理研究所編, 6, 近代科学社.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html",
    "href": "posts/statistics101/averages.html",
    "title": "1  代表値",
    "section": "",
    "text": "平均\n▶  標本平均\nデータ \\(X = \\{x_1, \\cdots, x_n\\}\\) が与えられとき，標本平均（sample mean） \\(\\overline{x}\\) は次になります：\n\\[\n\\overline{x} = \\frac{x_1 + \\cdots + x_n}{n}\n\\]\n標本平均は分布の代表値として最も使用されるものだが，外れ値に対して弱い性質がある．\nimport numpy as np\n\nX_0 = np.array([5.6, 5.7, 5.4, 5.5, 5.8, 5.2, 5.3, 5.6, 5.4, 55.5])\nX_1 = np.array([5.6, 5.7, 5.4, 5.5, 5.8, 5.2, 5.3, 5.6, 5.4])\n\nprint(\n    \"\"\"X_0: sample mean = {}, median = {}\\nX_1: sample mean = {}, median = {}\n      \"\"\".format(\n        np.mean(X_0), np.median(X_0), np.mean(X_1), np.median(X_1)\n    )\n)\n\nX_0: sample mean = 10.5, median = 5.55\nX_1: sample mean = 5.5, median = 5.5\n上記の例のように，medianは外れ値の混入があってもその影響は軽微ですが，標本平均は大きく変わっており外れ値に対して弱いことがわかる．\n▶  刈り込み平均\n外れ値の影響を弱めて標本平均を推定する方法として，刈り込み平均(trimmed mean)があります． 上側 \\(100\\alpha \\%\\) と下側 \\(100\\alpha \\%\\) を使わないで推定する方法で，\\(x_{[i]}\\) を順序統計値として\n\\[\n\\hat\\mu_\\alpha = \\frac{1}{n-2m} \\sum_{i=m+1}^{n-m}x_{[i]}, \\  \\ m = \\lfloor n\\alpha \\rfloor\n\\]\nで推定する方法を刈り込み平均という．利用にあたって，外れ地の割合を事前に想定する必要がありますが， 少々適当に推定しても妥当な推定になりやすい特徴があります．\n\\(\\alpha = 0.1\\) としてPythonで計算してみると以下，\nfrom scipy import stats\nself_trimmed_mean = np.mean([5.7, 5.4, 5.5, 5.8, 5.4, 5.3, 5.6, 5.6])\ntrimmed_mean = stats.trim_mean(X_0, 0.1)\nprint(self_trimmed_mean, trimmed_mean)\n\n5.5375 5.5375\nTrimmed meanはARE(Asymptotic relative efficiency, 漸近相対効率)という観点からも大抵の裾の重さに対して（重すぎるのは厳しいですが．．．）高いパフォーマンスがあることが知られています．\n▶  幾何平均\n\\(x_i &gt; 0\\) となるようなデータについて，幾何平均は以下のように計算されます：\n\\[\n\\overline{x}_G = \\bigg(\\prod_{i=1}^n x_i \\bigg)^{\\frac{1}{n}}\n\\]\n2000年から2005年までのXこくのでの物価上昇率が2%, 5%, 2%, 5%, 10%とあるとき，年平均上昇率は算術平均ではなく幾何平均で計算すべきで\n\\[\n(1.02 \\times 1.05 \\times 1.02 \\times 1.05 \\times 1.1)^{1/5} \\approx 1.0476\n\\]\nすなわち年平均約4.8%の増加と報告すべきとなります．なお，相加相乗平均より，幾何平均は算術平均より小さい値になることがわかります．もし大きい値を出してしまっていたら計算ミスを疑うべきです．\n幾何平均について対数をとると以下のように算術平均で表すことができます\n\\[\n\\log(\\overline{x}_G) = \\frac{1}{n}\\sum \\log(x_i)\n\\]\nここから，幾何平均を計算するときは一旦log transformationを実行し，算術平均を計算し，その後 \\(\\exp(\\cdot)\\) で元のスケールに戻すという形でよく計算されます．\nクラス分類の評価指標との関係では，sensitivity(感応度)とspecificity(特異度)の幾何平均を用いたG-Mean(geometric mean)という指標があります．\n\\[\n\\begin{align*}\n\\operatorname{G-mean} &= \\sqrt{\\operatorname{sensitivity} \\times \\operatorname{specificity}}\\\\\n                      &= \\sqrt{\\operatorname{recall} \\times \\operatorname{True Negative Rate}}\n\\end{align*}\n\\]\n▶  調和平均\n\\(x_i &gt; 0\\) となるようなデータについて，調和平均(harmonic mean)は以下のように計算されます：\n\\[\n\\frac{1}{\\overline{x}_H} = \\frac{1}{n}\\sum\\frac{1}{x_i}\n\\]\nとある車が距離 \\(\\alpha\\) の区間Aでは25km/h, 距離 \\(\\beta\\) の区間Bでは15km/hで走っていたとします．このとき，この車の平均時速は\n\\[\n\\frac{1}{\\text{平均時速}} = \\frac{\\alpha}{\\alpha + \\beta} \\frac{1}{25} + \\frac{\\beta}{\\alpha + \\beta} \\frac{1}{15}\n\\]\n\\(\\alpha = \\beta\\) のときは\n\\[\n\\frac{1}{\\text{平均時速}}  = \\frac{1}{2} \\bigg(\\frac{1}{25} + \\frac{1}{15}\\bigg)\n\\]\n平均を計算するにあたって，値が同じスケールの単位である必要であるため，上の平均時速の例では調和平均を利用することが 好ましいとされます．なお区間Aをx時間で25km/h, 区間Bをy時間で15km/hという場合はウェイトが時間単位で表されているので\n\\[\n\\text{平均時速} = \\frac{x}{x + y} \\times 25 + \\frac{y}{x + y} \\times 15\n\\]\nモデルの評価指標の１つにprecisionとrecallを用いたF1-scoreがありますが，precisionとrecallも分子はそれぞれTrue Positiveで共通していますが，分母がそれぞれ \\(\\operatorname{TP} + \\operatorname{FP}, \\operatorname{TP} + \\operatorname{FN}\\) と異なっているので，調和平均を用いて以下のように計算します：\n\\[\n\\begin{align*}\n\\operatorname{F1-score} &= \\frac{1}{\\frac{1}{2} \\left(\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}\\right)}\\\\\n&= \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}}\n\\end{align*}\n\\]\nなお，これはウェイトが等しい場合を意味しており，weighted harmonic meanへ拡張する場合は以下のように \\(\\operatorname{F_\\beta-score}\\) を用いて計算します\n\\[\n\\operatorname{F_\\beta-score} = \\frac{1 + \\beta^2}{\\frac{1}{\\text{precision}} + \\frac{\\beta^2}{\\text{recall}}}\n\\]\nウェイトが \\(\\frac{1}{1 + \\beta^2}, \\frac{\\beta^2}{1 + \\beta^2}\\) の形を取っているのは一見不自然に見えますが，その考察で面白いのがvan Rijs-bergen’s E (effectiveness) functionに基づいた説明です．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html#平均",
    "href": "posts/statistics101/averages.html#平均",
    "title": "1  代表値",
    "section": "",
    "text": "Theorem 1.1 \n定義域が \\(\\mathbb R_+\\) の確率変数 \\(X\\) を考える（つまり \\(X &gt; 0\\)）. このとき,\n\n\\(H_x\\): 調和平均\n\\(G_x\\): 幾何平均\n\\(\\overline{X}\\): 標本平均\n\nとして以下が常に成り立つ\n\\[\nH_x \\leq G_x \\leq \\overline{X}\n\\]\n\n\n\n\n\n\n\n\nProof: Jensen’s inequalityを用いた証明\n\n\n\n\n\nJensen’s inequalityより 関数 \\(g\\) を凸関数(convex function)とすると\n\\[\n\\frac{1}{n}\\sum_{i=1}^ng(x_i)\\geq g(\\overline{x})\n\\]\nという不等式が成り立つ.\n ▶  \\(\\overline{X} \\geq G_x\\) の証明\n\\(f(x) = -\\log(x)\\) とすると \\(f\\) は単調減少の凸関数であるので\n\\[\n\\begin{align*}\n-\\log(\\overline{x}) &\\leq -\\frac{1}{n}\\sum_{i=1}^n\\log(x_i)\\\\\n                    &= -\\log(\\prod_{i=1}^n x_i^{1/n})\\\\\n                    &= -\\log(G_x)\n\\end{align*}\n\\]\nつまり，\\(\\log(\\overline{x})\\geq \\log(G_x) \\Rightarrow \\overline{X} \\geq G_x\\)\n ▶  \\(G_x \\geq H_x\\) の証明\n\\(1/x_i = y_i\\) と変換すると\n\\[\n\\begin{align*}\nG_x &= \\left(\\prod \\frac{1}{y_i}\\right)^{1/n}\\\\\nH_x &= \\frac{1}{\\overline y}\n\\end{align*}\n\\]\nそれぞれについて \\(f(x) = \\log(x)\\) とすると\n\\[\n\\begin{align*}\n\\log(G_x) &= \\frac{1}{n}\\sum_{i=1}^n(-\\log(y_i))\\\\\n\\log(H_x) &= -\\log(\\overline y)\n\\end{align*}\n\\]\n\\(-\\log(\\cdot)\\) は凸関数であるので\n\\(\\log(G_x) \\geq \\log(H_x) \\Rightarrow G_x \\geq H_x\\) を得る．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html#中央値",
    "href": "posts/statistics101/averages.html#中央値",
    "title": "1  代表値",
    "section": "中央値",
    "text": "中央値\n\nDef: median \nデータ \\(x_1, \\cdots, x_n\\) を小さい順に並び替えた順序統計量\n\\[\nx_{[1]} &lt; \\cdots &lt; x_{[n]}\n\\]\nについて，真ん中の値を中央値という．つまり，\n\\[\n\\operatorname{Med}(X) = \\left\\{\\begin{array}{cl}\nx_{[k]} & \\text{where } n = 2k-1\\\\\n\\displaystyle \\frac{x_{[k]} + x_{[k+1]}}{2} & \\text{where } n = 2k\n\\end{array}\\right.\n\\]\n\n ▶  Hodges-Lehmann推定量\n標本の中からペアを選び，そのヘアの平均の中央値を用いて中央値を推定するのがホッジスレーマン推定値です．\n\\[\n\\hat\\mu_{HL} = \\operatorname{Med}\\bigg(\\bigg\\{\\frac{x_i + x_j}{2}\\bigg\\}_{1\\leq i \\leq j \\le n}\\bigg)\n\\]\ncomputation上少し重たいですが計算例として以下，\n\nimport itertools\n\ndef HL_mean(x: list[tuple]):\n    return np.median([np.mean(t) for t in itertools.combinations(x, 2)])\n\nX_0 = np.array([5.6, 5.7, 5.4, 5.5, 5.8, 5.2, 5.3, 5.6, 5.4, 55.5])\nprint(HL_mean(X_0))\n\n5.55\n\n\n\n\nTheorem 1.2 : L1距離最小化推定量としての中央値 \n標本 \\(X_1, \\cdots, X_n\\) についてmedianを以下のように定義する\n\\[\n\\operatorname{Med}(X) = \\left\\{\\begin{array}{cl}\nx_{[k]} & \\text{where } n = 2k-1\\\\\n\\displaystyle \\frac{x_{[k]} + x_{[k+1]}}{2} & \\text{where } n = 2k\n\\end{array}\\right.\n\\]\nこのとき，medianは次の性質を満たす\n\\[\n\\operatorname{Med}(X) = \\arg\\min_a \\sum \\vert X_i - a\\vert\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n昇順にソートし，\\(x_1 \\leq x_2 \\leq \\cdots \\leq x_m \\leq \\cdots \\leq x_n, a=x_m\\) とする．\nこのとき，目的関数は以下のように変形できる\n\\[\n\\begin{align*}\ng(a) &= \\sum_{i=1}^n|x_i-a| \\\\\n     &= \\sum_{i=1}^m(a-x_i)+\\sum_{i=m+1}^n(x_i-a) \\\\\n     &= ma-\\sum_{i=1}^m x_i + \\sum_{i=m+1}^n x_i -(n-m)a \\\\\n     &= (2m-n)a+n\\bar{x}-2\\sum_{i=1}^m x_i\n\\end{align*}\n\\]\n\\(g(x_{m-1})\\geq g(x_m)\\) かつ \\(g(x_{m+1})\\geq g(x_m)\\) となる \\(a=x_m\\) を見つければ良い．\n\\[\n\\begin{align}\ng(x_{m-1}) - g(x_m)=& [2(m-1)-n]a+n\\bar{x}-2\\sum_{i=1}^{m-1} x_i \\\\\n                    &-(2m-n)a-n\\bar{x}+2\\sum_{i=1}^m x_i \\\\\n                   =&[2(m-1)-n](x_{m-1}-x_m)\n\\end{align}\n\\]\nここで \\((x_{m-1}-x_m) \\leq 0\\) なので，\\(2(m-1)-n \\leq 0\\) でなれけばならない．\n\\[\nm\\leq \\frac{n+2}{2} \\Longrightarrow g(x_{m-1}) - g(x_m)\\geq 0\n\\]\n同様に\n\\[\n\\begin{align}\ng(x_{m+1}) - g(x_m)=& [2(m+1)-n]a+n\\bar{x}-2\\sum_{i=1}^{m+1} x_i \\\\\n                    &-(2m-n)a-n\\bar{x}+2\\sum_{i=1}^m x_i \\\\\n                   =&[2m-n](x_{m+1}-x_m)\n\\end{align}\n\\]\nここで \\((x_{m+1}-x_m) \\geq 0\\) なので，\\(2m-n \\geq 0\\) でなれけばならない．\n\\[\n\\frac{n}{2}\\leq m \\Longrightarrow g(x_{m+1}) - g(x_m)\\geq 0.\n\\]\n以上より\n\\[\n\\frac{n}{2}\\leq m \\leq \\frac{n+2}{2} \\Longrightarrow g(x_{m-1})\\geq g(x_m) \\land g(x_{m+1})\\geq g(x_m).\n\\]\n\n\n\n\n\nTheorem 1.3 : L1距離最小化推定量としての中央値 - 連続確率変数の場合 \n\\(F(x)\\) を分布関数に持つ連続確率変数 \\(X\\) について，\n\\[\n\\operatorname{Med}(X) = \\arg\\min_a \\mathbb E[\\vert X - a\\vert]\n\\]\nが成り立つ．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align}\ng(a) =& \\int_{-\\infty}^\\infty|x-a|f(x)\\mathrm{d}x \\\\\n     =& \\int_{-\\infty}^a(a-x)f(x)\\mathrm{d}x + \\int_a^{\\infty}(x-a)f(x)\\mathrm{d}x \\\\\n     =& a\\left[\\int_{-\\infty}^af(x)\\mathrm{d}x-\\int_{a}^\\infty f(x)\\mathrm{d}x\\right]\\\\\n        &+\\int_{a}^\\infty xf(x)\\mathrm{d}x - \\int_{-\\infty}^axf(x)\\mathrm{d}x\\\\\n     =& a[2F(a)-1]+\\mathbb E[X]-2\\int_{-\\infty}^axf(x)\\mathrm{d}x\n\\end{align}\n\\]\n\\(a\\) についてFOCを求めると\n\\[\n\\begin{align*}\n\\frac{\\mathrm{d}}{\\mathrm{d}a}g = 2F(a)-1+2af(a)-2af(a)\n\\end{align*}\n\\]\n従って，\n\\[\n2F(a^*) - 1 = 0 \\Rightarrow F(a^*)=\\frac{1}{2}\n\\]\nまた，2階条件を確認すると\n\\[\n\\frac{\\mathrm{d}^2}{\\mathrm{d}a^2}g\\bigg|_{F(a^*)=\\frac{1}{2}}=2f\\left(a^*\\right)&gt;0\n\\]\n従って，\\(a^* = \\operatorname{Med}(X)\\) が成り立つ．\n\n\n\n\nTheorem: Asymptotic distribution of sample quantile-p \n\\(y_1, \\cdots, y_n\\) を density function \\(f\\) 及びquantile function \\(Q^{(p)}\\) を持つ分布からのi.i.dとします．このとき, sample quantile \\(\\hat Q^{(p)}\\) は\n\\[\n\\sqrt{n}(\\hat Q^{(p)} - Q^{(p)}) \\rightarrow_d \\mathbb N\\left(0,\\frac{p(1-p)}{f(Q^{(p)})^2}\\right)\n\\]\n\nここでは，連続変数分布を想定して解説します．連続なdensity function \\(f_x\\) を持つ連続確率分布 \\(F\\) という分布について\n\\[\nX = \\{x_1, \\cdots, x_n\\}  \\overset{\\mathrm{iid}}{\\sim} F\n\\]\nと確率変数列が与えられたとします．この確率変数に対して\n\\[\nZ_i\\equiv 1\\{x_i \\leq x\\}\n\\]\nという変数を考えます．この \\(Z_i\\) は Bernoulli分布に従うと考えられるので，\\(F\\) のCDFを \\(F_X\\) とおくと\n\\[\n\\begin{align*}\n\\mathbb E(Z_i) &=  \\mathbb E\\left(I\\{X_i\\le x\\}\\right) = P(X_i\\le x)=F_X(x)\\\\\n\\operatorname{Var}(Z_i) &= F_X(x)[1-F_X(x)]\n\\end{align*}\n\\]\nここで，\\(Z_i\\) のsample meanを以下のように定義する．\n\\[\nY_n(x) =  \\frac 1n\\sum_{i=1}^nZ_i\n\\]\nこのように定義した \\(Y_n(x)\\) はいわゆる経験分布関数 \\(F_n(x)\\) であるとみなせます．また，定義より\n\\[\n\\begin{align*}\n&E[F_n(x)] = F_X(x)\\\\\n&\\operatorname{Var}(F_n(x)) = (1/n)F_X(x)[1-F_X(x)]\\\\\n&\\sqrt n\\Big(F_n(x) - F_X(x)\\Big) \\rightarrow_d \\mathbb N\\left(0,F_X(x)[1-F_X(x)]\\right) \\because{\\text{CLT}}\n\\end{align*}\n\\]\nここでCDFの逆関数 \\(F^{-1}_X\\) とする(monotonicityより自明)と delta methodを用いると\n\\[\n\\begin{align*}\n&\\frac {d}{dt}F^{-1}_X(t) = \\frac 1{f_x\\left(F^{-1}_X(t)\\right)}\\\\\n&\\sqrt n\\Big(F^{-1}_X(F_n(x)) - F^{-1}_X(F_X(x))\\Big) \\rightarrow_d \\mathbb N\\left(0,\\frac {F_X(x)[1-F_X(x)]}{\\left[f_x\\left(F^{-1}_X(F_X(x))\\right)\\right]^2} \\right)\n\\end{align*}\n\\]\nつまり，\n\\[\n\\sqrt n\\Big(F^{-1}_X(F_n(x)) - x\\Big) \\rightarrow_d \\mathbb N\\left(0,\\frac {F_X(x)[1-F_X(x)]}{\\left[f_x(x)\\right]^2} \\right)\n\\]\nここで \\(x = m\\)(population median)と設定すると\n\\[\n\\sqrt n\\Big(F^{-1}_X(F_n(m)) - m\\Big) \\rightarrow_d \\mathbb N\\left(0,\\frac {1}{\\left[2f_x(m)\\right]^2} \\right)\n\\]\nまた，\n\\[\nF^{-1}_X(\\hat F_n(m)) = \\inf\\{x : F_X(x) \\geq \\hat F_n(m)\\} = \\inf\\{x : F_X(x) \\geq \\frac 1n \\sum_{i=1}^n I\\{X_i\\leq m\\}\\}\n\\]\nより, 不等式のRHSは 1/2 に収束するので \\(F^{-1}_X(\\hat F_n(m))\\) はsample mean \\(\\hat m\\) に収束することがわかる．従って，\n\\[\n\\sqrt n\\Big(\\hat m - m\\Big) \\rightarrow_d \\mathbb N\\left(0,\\frac {1}{\\left[2f_x(m)\\right]^2} \\right)\n\\]\n\n\nTheorem 1.4 標本平均とMedianの距離 \n独立に同一の分布に従う確率変数列 \\(\\{X_i\\}_{i=1}^n\\) を考える．この標本平均を \\(\\overline{X}\\), 不偏分散を \\(S^2\\), メディアン \\(X_{\\operatorname{med}}\\) とすると\n\\[\n\\vert \\overline{X} - X_{\\operatorname{med}}\\vert &lt; S\n\\]\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(X_{\\operatorname{med}}\\) はL1ノルムの和を最小にするような値であるので\n\\[\n\\begin{align*}\n\\vert \\overline{X} - X_{\\operatorname{med}}\\vert\n    &= \\bigg\\vert \\frac{1}{n}\\sum_{i=1}^n(X_i - X_{\\operatorname{med}})\\bigg\\vert\\\\\n    &\\leq \\frac{1}{n}\\sum_{i=1}^n\\bigg\\vert X_i - X_{\\operatorname{med}}\\bigg\\vert\\\\\n    &\\leq\\frac{1}{n}\\sum_{i=1}^n\\bigg\\vert X_i - \\overline{X}\\bigg\\vert\\\\\n    &\\leq\\sqrt{\\frac{1}{n}\\sum_{i=1}^n( X_i - \\overline{X})^2}\\\\\n    &=\\sqrt{\\frac{n-1}{n}}S\\\\\n    &\\leq S\n\\end{align*}\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html#分散と標準偏差",
    "href": "posts/statistics101/averages.html#分散と標準偏差",
    "title": "1  代表値",
    "section": "分散と標準偏差",
    "text": "分散と標準偏差\n\nDef: Variance \nmean \\(\\mu\\) をもつ確率変数 \\(X\\) の分散は\n\\[\n\\begin{align*}\n\\operatorname{Var}(X) &= \\mathbb E[(X - \\mu)^2]\\\\\n                      &= \\mathbb E[X^2] - \\mathbb E[X]^2\n\\end{align*}\n\\]\n標準偏差(standard deviation)は分散のsquare rootで定義される．\n\n上の定義より標準偏差について以下のことがわかります：\n\n\\(X\\) と同じ単位で表される\n\\(X - \\mathbb E[X]\\) の L2ノルムと解釈できる\npopulation meanからどれだけ分布がバラついているか(dispersion)を示す指標の一つ\n\n\n\nTheorem 1.5 \n確率変数 \\(X\\sim D(\\mu, \\sigma^2)\\) とする．このとき\n\\[\n\\begin{align*}\n\\mathbb E[\\vert X - \\mu\\vert] \\leq \\sigma\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof: Jensen’s Inequality\n\n\n\n\n\n\\(g(x) = \\sqrt{x}\\) という関数はconcaveなので，Jensen’s Inequalityより\n\\[\n\\begin{align*}\n\\mathbb E[\\vert X - \\mu \\vert]\n    &= \\mathbb E[\\sqrt{( X - \\mu)^2}]\\\\\n    &\\leq \\sqrt{\\mathbb E[( X - \\mu)^2]}\\\\\n    &= \\sigma\n\\end{align*}\n\\]\n\n\n\n\n変動係数\n確率変数のバラツキを表す指標として範囲(Range), 四分位範囲（IQR）, 分散がありますが，分布の中心の位置が著しく異なるような場合 には，これらを用いて分布の散らばり具合を比較することは難しいです．このような場合に，変動係数(Coefficient of variation) という単位のない統計量を用いたりします．\n\nDef: Coefficient of variation \n比例尺度にもとづく測定が行われ，その測定結果をnon-negative 確率変数 \\(X_i &gt;0\\) で表すとする．\\(\\{X_i\\}_{i=1}^n\\) の標本平均を \\(\\overline{X}\\), 標本標準偏差を \\(S\\) としたとき，変動係数は以下のように計算される\n\\[\n\\operatorname{C_V} = \\frac{S}{\\overline{X}}\n\\]\n\n上記の定義より以下のことがわかります\n\n変動係数は，実際のゼロ点を持つ測定値（i.e., 比率尺度）に対してのみ用いることができます\n変動係数の計算対象となる測定は，non-negativeである必要がある\nサイズ \\(N\\) のfinite sampleにおける \\(\\operatorname{C_V}\\) のレンジは \\(\\operatorname{CV}\\in[0, \\sqrt{N-1}]\\) である\n\n\nExample 1.1 東京 日平均気温の月平均値（℃） \n国土交通省気象庁より以下のように東京都の気象データを取得します．\n\n\nCode\nimport polars as pl\nimport plotly.express as px\n\ndata = {\n    \"Year\": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023],\n    \"Jan\": [5.8, 6.1, 5.8, 4.7, 5.6, 7.1, 5.4, 4.9, 5.7],\n    \"Feb\": [5.7, 7.2, 6.9, 5.4, 7.2, 8.3, 8.5, 5.2, 7.3],\n    \"Mar\": [10.3, 10.1, 8.5, 11.5, 10.6, 10.7, 12.8, 10.9, 12.9],\n    \"Apr\": [14.5, 15.4, 14.7, 17.0, 13.6, 12.8, 15.1, 15.3, 16.3],\n    \"May\": [21.1, 20.2, 20.0, 19.8, 20.0, 19.5, 19.6, 18.8, 19.0],\n    \"Jun\": [22.1, 22.4, 22.0, 22.4, 21.8, 23.2, 22.7, 23.0, 23.2],\n    \"Jul\": [26.2, 25.4, 27.3, 28.3, 24.1, 24.3, 25.9, 27.4, 28.7],\n    \"Aug\": [26.7, 27.1, 26.4, 28.1, 28.4, 29.1, 27.4, 27.5, 29.2],\n    \"Sep\": [22.6, 24.4, 22.8, 22.9, 25.1, 24.2, 22.3, 24.4, 26.7],\n    \"Oct\": [18.4, 18.7, 16.8, 19.1, 19.4, 17.5, 18.2, 17.2, 18.9],\n    \"Nov\": [13.9, 11.4, 11.9, 14.0, 13.1, 14.0, 13.7, 14.5, 14.4],\n    \"Dec\": [9.3, 8.9, 6.6, 8.3, 8.5, 7.7, 7.9, 7.5, 9.4],\n}\n\ndf = pl.DataFrame(data)\ndf_unpivoted = df.unpivot(index=\"Year\", variable_name=\"Month\", value_name=\"Celsius\")\npx.line(df_unpivoted, x=\"Month\", y=\"Celsius\", color=\"Year\", title=\"東京都月別平均気温\")\n\n\n                                                \n\n\nここで，以下のようにFahrenheitに変換してみます．\n\n\nCode\ndf_unpivoted = df_unpivoted.with_columns(\n    (pl.col(\"Celsius\") * 9 / 5 + 32).alias(\"Fahrenheit\")\n)\ndf_unpivoted.head()\n\n\n\nshape: (5, 4)\n\n\n\nYear\nMonth\nCelsius\nFahrenheit\n\n\ni64\nstr\nf64\nf64\n\n\n\n\n2015\n\"Jan\"\n5.8\n42.44\n\n\n2016\n\"Jan\"\n6.1\n42.98\n\n\n2017\n\"Jan\"\n5.8\n42.44\n\n\n2018\n\"Jan\"\n4.7\n40.46\n\n\n2019\n\"Jan\"\n5.6\n42.08\n\n\n\n\n\n\nここで，Celsius, Fahrenheit両方のカラムについて変動係数を計算します．\n\n\nCode\ndef compute_cv(df, col: str) -&gt; float:\n    return df_unpivoted[col].std() / df_unpivoted[col].mean()\n\n\nprint(\n    \"\"\"Celsius CV: {:.2f}, Fahrenheit CV: {:.2f}\"\"\".format(\n        compute_cv(df_unpivoted, \"Celsius\"), compute_cv(df_unpivoted, \"Fahrenheit\")\n    ))\n\n\nCelsius CV: 0.45, Fahrenheit CV: 0.22\n\n\nこのように，Celsius, Fahrenheitともに同じ温度を単位の違う方法で表しているのにも関わらず変動係数は異なります．Celsius, Fahrenheitともに間隔尺度であること，及び変数変換の観点からもlocation/scale parameterを異なる値で調整しているので同じデータを扱っているにも関わらずCVが一致しないという現象が発生してしまいます．\n\n ▶  Bias Correction\nサイズ \\(N\\) のsampleベースで計算された変動係数はpopulation変動係数 \\(\\gamma_V\\) と比較して過小推定されているということが知られています．population変動係数のunbiased estimate \\(\\widehat{\\operatorname{C_V}}\\) は以下のように計算されます\n\\[\n\\widehat{\\operatorname{C_V}} = \\left(1 + \\frac{1}{4N}\\right)\\operatorname{C_V}\n\\]\n ▶  \\(\\sqrt{n}\\) の法則\n確率変数 \\(X_1, \\cdots, X_n\\) 独立に同一の分布に従うとし，これらの期待値と分散を \\(\\mu, \\sigma^2\\) とします．\nこのとき，\\(\\tilde X_n = \\sum^n_{i=1} X_i\\) とすると，\n\\[\n\\begin{align*}\n\\mathbb E[\\tilde X_n] &= n\\mu\\\\\n\\operatorname{Var}(\\tilde X_n) &= n\\sigma^2\\\\\n\\operatorname{std}(\\tilde X_n) &= \\sqrt{n}\\sigma\n\\end{align*}\n\\]\n期待値は \\(n\\) のオーダーで増える一方，標準偏差は \\(\\sqrt{n}\\) オーダーなので\n\\[\n\\begin{align*}\n\\operatorname{CV}(\\tilde X_n) = \\frac{\\sqrt{n}\\sigma}{n\\mu} = \\frac{\\sigma}{\\sqrt{n}\\mu}\n\\end{align*}\n\\]\nここから，平均と標準偏差の比率は \\(n\\) が増えるほど，小さくなる = データのバラツキが相対的に小さくなることがわかります．\nとある店AとBについて，それぞれの各月の来店者人数は各店舗が立地している地域の人口に比例すると解釈して， \\(X_A\\sim\\operatorname{Binom}(200, 0.5), Y_B\\sim\\operatorname{Binom}(1000, 0.5)\\) である場合を考えます．\nそれぞれの店舗について来月来店人数を予測したいとき，\\(p_A = p_B = 0.5, N_A= 200, N_B= 1000\\) であると判断し，その下で期待値をとって \\(100, 500\\) と予測したとします． このとき，予測値を基準に予測誤差 5% を上回りそうな確率はそれぞれ\n\\[\n\\begin{align*}\n1 - \\Pr(\\vert X_A - 100\\vert &lt; 5) &\\approx 0.481\\\\\n1 - \\Pr(\\vert X_B - 500\\vert &lt; 25) &\\approx 0.114\n\\end{align*}\n\\]\nと店舗 B のほうがMAPE的により良い予測精度が得られそうなことがわかります．母集団parameterもそれぞれ正しく判断できているにも関わらず, MAPEという指標では店舗 B のほうが相対的に良さそうな結果がでてしまうと推察できます．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html#references",
    "href": "posts/statistics101/averages.html#references",
    "title": "1  代表値",
    "section": "References",
    "text": "References\n\nYutaka Sasaki, The truth of the F-measure, 2007",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/correlation.html",
    "href": "posts/statistics101/correlation.html",
    "title": "2  相関関係",
    "section": "",
    "text": "相関係数\n▶  Pearson相関係数の値域\nPearson相関係数 \\(\\vert\\operatorname{Corr}(X, Y) \\vert\\leq 1\\) という性質があります．\n\\(\\tilde X = X - \\mu_x, \\tilde Y = Y - \\mu_y\\) とおきます．シュワルツの不等式を用いると\n\\[\n\\begin{align*}\n\\vert\\operatorname{Corr}(X, Y)\\vert^2\n    &= \\frac{\\vert\\sigma_{XY}\\vert^2}{\\sigma^2_X\\sigma^2_Y}\\\\\n    &= \\frac{\\mathbb E[\\tilde X \\tilde Y]^2}{E[\\tilde X]^2E[\\tilde Y]^2}\\\\\n    &\\leq 1\n\\end{align*}\n\\]\n従って，\\(\\vert\\operatorname{Corr}(X, Y)\\vert \\leq 1\\)\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n▶  互いに独立な確率変数と相関係数\n互いに独立な確率変数 \\(X, Y\\) について，\n\\[\n\\begin{align*}\n\\operatorname{Cov}(X, Y)\n    &= \\mathbb E[XY] - \\mathbb E[X]\\mathbb E[Y]\\\\\n    &= \\mathbb E[X]\\mathbb E[Y] - \\mathbb E[X]\\mathbb E[Y]\\\\\n    &= 0\n\\end{align*}\n\\]\nよりPeasorn相関係数の分子が0になるので，\\(\\operatorname{Corr}(X, Y) = 0\\) となることがわかります． ただし，Peasorn相関係数が 0 だとしても，独立性が成立するとは限りません．\nPearson相関係数はスケール及び平行移動について不変である性質を確認できましたが，２つの確率変数の関係性を捉える指標として相関係数の他に共分散があります． 共分散はスケールに依存じます．上の例をベースに確認してみると．\n\\[\n\\begin{align*}\n\\operatorname{Cov}(U, V) = a_1a_2\\operatorname{Cov}(X, Y)\n\\end{align*}\n\\]\nより平行移動に関しては不変だが，スケール変換に対して不変ではありません．\n標本Peason相関係数の値域もpopulationPeason相関係数と同様に \\([-1, 1]\\) となります． シュワルツの不等式より\n\\[\n\\sum a_i^2 \\sum b_i^2 \\geq \\bigg(\\sum a_ib_i\\bigg)^2\n\\]\nここで，\\(a_i = (x_i - \\overline{x}), y_i = (y_i - \\overline{y})\\) とすると\n\\[\n\\sum_{i=1}^n (x_i - \\bar x)^2 \\sum_{i=1}^n (y_i - \\bar y)^2 \\geq \\bigg(\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y)\\bigg)^2\n\\tag{2.2}\\]\n従って，\\(\\rho_{xy}^2 \\leq 1\\) が成立することがわかります．\nまた，シュワルツの不等式のの等号成立条件の必要十分条件は, すべての \\(i\\) について \\(a_i:b_i\\) が一定(ここではその比率を \\(k\\) とおく) なので,\n\\[\ny_i = kx_i + \\bar y - k\\bar x = kx_i + c \\quad (c:\\text{定数})\n\\]\nのとき，\\(\\vert\\rho_{xy}\\vert = 1\\)．また，変数感に１次式の関係があるとき \\(\\vert\\rho_{xy}\\vert = 1\\) が成立することがわかります．\n▶  ベクトルのなす角との関係\nEquation 2.1 について，ノルムで表現することもできます．\\(n\\) 次元実ベクトル \\(\\pmb{u}, \\pmb{v}\\) に対し，その内積を\n\\[\n(\\pmb{u}, \\pmb{v}) = \\sum_{i=1}^n u_iv_i\n\\]\nとします．\\(\\pmb{x} = (x_1 - \\overline{x}, \\cdots, x_n - \\overline{x})^T, \\pmb{y} = (y_1 - \\overline{y}, \\cdots, y_n - \\overline{y})^T\\) とすると，\n\\[\n\\rho_{xy} = \\frac{(\\pmb{x}, \\pmb{y})}{\\|\\pmb{x}\\|\\|\\pmb{y}\\|}\n\\tag{2.3}\\]\nEquation 2.3 について，ベクトル \\(\\pmb{x}, \\pmb{y}\\) のなす角と理解することができます．実際に２つのベクトルのなす角は\n\\[\n\\cos\\theta =  \\frac{(\\pmb{u}, \\pmb{v})}{\\|\\pmb{u}\\|\\|\\pmb{v}\\|}\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>相関関係</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/correlation.html#相関係数",
    "href": "posts/statistics101/correlation.html#相関係数",
    "title": "2  相関関係",
    "section": "",
    "text": "Def: Pearson相関係数 \n確率変数 \\(X, Y\\) のPearson相関係数は\n\\[\n\\operatorname{Corr}(X, Y) = \\frac{\\sigma_{XY}}{\\sigma_X\\sigma_Y}\n\\]\n\\(\\sigma_{XY}\\) は \\(X, Y\\) の共分散, \\(\\sigma_X, \\sigma_Y\\) は標準偏差を表す．\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2.1 : 単位円周上の点 \n一様分布に従う確率変数 \\(\\Theta\\sim\\operatorname{U}(0, 2\\pi)\\) を考えます．この確率変数を変数変換し，\n\\[\n\\begin{align*}\nX & = \\cos\\Theta\\\\\nY & = \\sin\\Theta\n\\end{align*}\n\\]\nこのとき，\\(X, Y\\) の確率密度関数はそれぞれ\n\\[\nf_X(t) = f_Y(t) = \\frac{1}{2\\pi}\\frac{1}{\\sqrt{1 - t^2}}\n\\]\nとなります．独立でない関係は定義より \\(X^2 + Y^2 = 1\\) と明らかですが，同時密度関数 \\(f(x, y)\\) について\n\\[\nf(0, 0) \\neq f_X(0)f_Y(0)\n\\]\nで確認できます．このときのPearson相関係数は\n\\[\n\\begin{align*}\n\\mathbb E[X] &=\\frac{1}{2\\pi}\\int^{2\\pi}_{0}\\cos\\theta\\mathrm{d}\\theta =0\\\\\n\\mathbb E[Y] &=\\frac{1}{2\\pi}\\int^{2\\pi}_{0}\\sin\\theta\\mathrm{d}\\theta =0\\\\\n\\mathbb E[XY] &= \\frac{1}{2\\pi} \\int^{2\\pi}_{0}\\cos\\theta \\sin\\theta \\mathrm{d}\\theta\\\\\n&= \\frac{1}{4\\pi} \\int^{2\\pi}_{0}\\sin(\\theta+\\theta) - \\sin(\\theta-\\theta) \\mathrm{d}\\theta\\\\\n&= \\frac{1}{4\\pi} \\int^{2\\pi}_{0}\\sin(2\\theta)\\mathrm{d}\\theta\\\\\n&= 0\n\\end{align*}\n\\]\n従って，\\(\\operatorname{Corr}(X, Y) = 0\\)\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n図で確認してみると以下のように，一つの \\(x\\) に対して，２つの \\(y\\) の値が存在しています．このように \\(y\\) が \\(x\\) に対してユニークにおおよその位置が定まらないときはPearson相関係数は0に近い値が出力される傾向にあります．\n\n\nCode\nimport numpy as np\nimport plotly.io as pio\nimport plotly.express as px\nimport regmonkey_style.stylewizard as sw\n\n\n# Register custom template\nsw.set_templates(\"regmonkey_scatter\")\n\n# Data preparation\nnp.random.seed(42)\n\nN = 200\ntheta = np.random.uniform(0, 2 * np.pi, N)\nX, Y = np.cos(theta), np.sin(theta)\n\n# Create the figure with the template\nfig = px.scatter(\n    x=X,\n    y=Y,\n    title=\"一様分布乱数の単位円周上の点への変数変換&lt;br&gt;&lt;sup&gt;Sample size = {}, Pearson相関係数={:.2f}&lt;/sup&gt;\".format(\n        N, np.corrcoef(X, Y)[0, 1]\n    ),\n)\n\n# equal xy scale\nsw.equal_xy_scale(fig)\nfig.show()\n\n\n\n\n\n                                                \n\n\n\n\n\nTheorem 2.1 \n\\(\\operatorname{Var}(X) &gt;0, \\operatorname{Var}(Y) &gt;0\\) の確率変数 \\(X, Y\\) について，\n\\[\nf_{X,Y}(x ,y) = f_{X,Y}(-x ,y)\n\\]\nが成立するとする．このとき，\n\\[\n\\operatorname{Corr}(X, Y) = 0\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\operatorname{Cov}(X, Y) = 0\\) を示せば十分です．\n\\[\n\\begin{align*}\n\\operatorname{Cov}(X, Y)\n    &= \\int\\int f_{X,Y}(x ,y)(x-\\mu_x)(y-\\mu_y)\\,\\mathrm{d}x\\mathrm{d}y\\\\\n    &= \\int (y-\\mu_y)(\\int \\underbrace{f_{X,Y}(x ,y)}_{\\text{偶関数}}\\underbrace{(x-\\mu_x)}_{\\text{奇関数}}\\,\\mathrm{d}x)\\mathrm{d}y\\\\\n    &= \\int (y-\\mu_y)\\times 0\\, \\mathrm{d}y\\\\\n    &= 0\n\\end{align*}\n\\]\n従って，\\(\\operatorname{Corr}(X, Y) = 0\\)\n\n\n\n\n\nTheorem 2.2 : 平行移動 & スケール変換に対して不変 \n\\(a_1 \\neq 0, a_2\\neq 0, b_1, b_2\\) を定数として, 確率変数 \\(X, Y\\) に対して以下のような変数変換を考える\n\\[\n\\begin{align*}\nU & = a_1 X + b_1\\\\\nV & = a_2 Y + b_2\n\\end{align*}\n\\]\nこのとき，\n\\[\n\\operatorname{Corr}_{uv} = \\text{sgn}(a_1a_2)\\operatorname{Corr}_{xy}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\operatorname{Corr}_{uv}\n    &= \\frac{\\operatorname{Cov}(U, V)}{\\sqrt{\\operatorname{Var}(U)}\\sqrt{\\operatorname{Var}(V)}}\\\\\n    &= \\frac{a_1a_2\\operatorname{Cov}(X, Y)}{\\sqrt{a_1^2\\operatorname{Var}(X)}\\sqrt{a_2^2\\operatorname{Var}(Y)}}\\\\\n    &= \\frac{a_1a_2}{\\vert a_1a_2\\vert}\\operatorname{Corr}(X, Y)\\\\\n    &= \\operatorname{sgn}(a_1a_2)\\operatorname{Corr}(X, Y)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nTheorem 2.3 正の相関係数の非推移性 \n３つの確率変数 \\(X, Y, Z\\) について\n\\[\n\\begin{alignat*}{2}\n\\operatorname{Corr}(X, Y) &gt; 0 \\quad&& \\operatorname{Corr}(Y, Z) &gt; 0\n\\end{alignat*}\n\\]\nが成立したとしても，\\(\\operatorname{Corr}(X, Z) &gt; 0\\) とは限らない\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n3つの独立な確率変数 \\(X, Y, Z\\) にたいして\n\\[\n\\begin{gather*}\nU = X + Y\\\\\nV = Y + Z\\\\\nW = Z - X\n\\end{gather*}\n\\]\nとおくと\n\\[\n\\begin{align*}\n\\text{Cov}(U, V) &= \\text{Var}(Y) &gt; 0\\\\\n\\text{Cov}(V, W) &= \\text{Var}(Z) &gt; 0\\\\\n\\text{Cov}(U, W) &= -\\text{Var}(X) &lt; 0\n\\end{align*}\n\\]\nとなり, 正の推移性が成立しないことがわかる．\n\n\n\n\n\nTheorem 2.4 : A measure of Linear relation \n確率変数 \\(X, Y\\) について，\\(\\vert\\operatorname{Corr}(X, Y)\\vert = 1\\) とする．このとき，\\(X, Y\\) は完全な線形関係で表せる．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\operatorname{Corr}(X, Y)^2 = \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(X)}\\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)} = 1\n\\]\n\\(Y\\) を \\(X\\) に線形回帰するとき, \\(X\\) の係数 \\(\\beta\\) について\n\\[\n\\beta = \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(X)}\n\\]\nであるので，\\(\\vert\\operatorname{Corr}(X, Y)\\vert = 1\\) より\n\\[\n\\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)} = \\frac{1}{\\beta}\n\\]\nである必要があります．\\(e = Y - \\mu_y - \\beta(X - \\mu_x)\\) と定義すると\n\\[\n\\operatorname{Var}(Y) = \\beta^2\\operatorname(Var)(X) + \\operatorname{Var}(e)\n\\]\nであるので，\n\\[\n\\begin{align*}\n\\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(Y)}\n    &= \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(X)}\\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(Y)}\\\\\n    &= \\beta\\frac{\\operatorname{Var}(X)}{\\beta^2\\operatorname{Var}(X) + \\operatorname{Var}(e)}\\\\\n    &= \\frac{1}{\\beta}\\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(X) + \\operatorname{Var}(e)/\\beta^2}\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(e) = 0\n\\]\nが必要条件であることが分かります．従って，\n\\[\n\\begin{gather}\nY = \\alpha + \\beta X\\\\\n\\text{where}\\quad \\alpha = \\mu_y - \\beta\\mu_x\n\\end{gather}\n\\]\nが導けます．\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n ▶  Pythonでの確認\nPearson相関係数が強いほど，線形回帰及び逆線形回帰の予測ラインが近づいていくことを以下で確認しています．\n\n\nCode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport statsmodels.api as sm\n\n\nX1, Y1 = np.random.multivariate_normal([0, 0], [[1, 0.4], [0.4, 1]], N).T\n\nX2, Y2 = np.random.multivariate_normal([0, 0], [[1, 0.95], [0.95, 1]], N).T\n\n# Set common x and y ranges\nx_range = np.array([-3, 3])  # Define your desired x range\ny_range = np.array([-3, 3])  # Define your desired y range\n\n# regression\nresult_1_yx = sm.OLS(Y1, sm.add_constant(X1)).fit()\nresult_1_xy = sm.OLS(X1, sm.add_constant(Y1)).fit()\nresult_2_yx = sm.OLS(Y2, sm.add_constant(X2)).fit()\nresult_2_xy = sm.OLS(X2, sm.add_constant(Y2)).fit()\n\nfig = make_subplots(rows=1, cols=2)\nfig.add_trace(go.Scatter(x=X1, y=Y1, name=\"相関係数:0.4\"), row=1, col=1)\nfig.add_trace(\n    go.Scatter(\n        x=x_range,\n        y=result_1_yx.predict(sm.add_constant(x_range)),\n        mode=\"lines\",\n        line=dict(color='gray'),\n        showlegend=False,\n        name=\"coef: {:.2f}\".format(result_1_yx.params[1]),\n    ),\n    row=1,\n    col=1,\n)\nfig.add_trace(\n    go.Scatter(\n        x=result_1_xy.predict(sm.add_constant(y_range)),\n        y=y_range,\n        mode=\"lines\",\n        line=dict(color='gray'),\n        showlegend=False,\n        name=\"coef: {:.2f}\".format(result_1_xy.params[1]),\n    ),\n    row=1,\n    col=1,\n)\n\nfig.add_trace(go.Scatter(x=X2, y=Y2, name=\"相関係数:0.95\"), row=1, col=2)\nfig.add_trace(\n    go.Scatter(\n        x=x_range,\n        y=result_2_yx.predict(sm.add_constant(x_range)),\n        mode=\"lines\",\n        line=dict(color='gray'),\n        showlegend=False,\n        name=\"coef: {:.2f}\".format(result_2_yx.params[1]),\n    ),\n    row=1,\n    col=2,\n)\nfig.add_trace(\n    go.Scatter(\n        x=result_2_xy.predict(sm.add_constant(y_range)),\n        y=y_range,\n        mode=\"lines\",\n        line=dict(color='gray'),\n        showlegend=False,\n        name=\"coef: {:.2f}\".format(result_2_xy.params[1]),\n    ),\n    row=1,\n    col=2,\n)\n\nfig.update_layout(title='相関係数水準に応じた回帰係数と逆回帰係数の比較')\nfig.update_xaxes(\n    range=x_range, scaleanchor=\"y\", scaleratio=1, row=1, col=1\n)  # Link x-axis of plot 1 to y-axis\nfig.update_yaxes(\n    range=y_range, scaleanchor=\"x\", scaleratio=1, row=1, col=1\n)  # Link y-axis of plot 1 to x-axis\nfig.update_xaxes(\n    range=x_range, scaleanchor=\"y2\", scaleratio=1, row=1, col=2\n)  # Link x-axis of plot 2 to y-axis\nfig.update_yaxes(range=y_range, scaleanchor=\"x2\", scaleratio=1, row=1, col=2)\nfig.show()\n\n\n                                                \n\n\n\n\n\n\nExample 2.2 \n単位円周上の一様分布に従う確率変数 \\(\\Theta\\) を考えます．円周がこの \\(\\Theta\\) によって分割されるとき，ふたつの線分ができます．このうち長い方を \\(X\\), 短い方を \\(Y\\) とすると，\n\\[\n\\begin{align*}\nX &\\sim U(\\pi, 2\\pi)\\\\\nY &\\sim U(0, \\pi)\\\\\nX & = 2\\pi - Y\n\\end{align*}\n\\]\nと表せます．このとき，\\(X, Y\\) の相関係数は\n\\[\n\\begin{align*}\n\\operatorname{Corr}(X, Y) &= \\frac{\\mathbb E[XY] - \\mathbb E[X]\\mathbb E[Y]}{\\sqrt{\\mathbb V(X)\\mathbb V(Y)}}\\\\\n&= \\frac{\\mathbb E[Y(2\\pi - Y)] - \\pi/2 \\cdot 3\\pi/2}{\\pi^2/12}\\\\\n&= -1\n\\end{align*}\n\\]\nと計算できます．\n\n\nDef: 標本Pearson相関係数 \n確率変数の組 \\(\\{(x_i, y_i)\\}_{i=1}^n\\) について，それぞれの標本平均を \\(\\overline{x}, \\overline{y}\\) とする．このとき標本Pearson相関係数 \\(\\rho_{xy}\\) は\n\\[\n\\rho_{xy} = \\frac{\\sum (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum (x_i - \\overline{x})^2}\\sqrt{\\sum (y_i - \\overline{y})^2}}\n\\tag{2.1}\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>相関関係</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/correlation.html#appendix",
    "href": "posts/statistics101/correlation.html#appendix",
    "title": "2  相関関係",
    "section": "Appendix",
    "text": "Appendix\n ▶  母集団相関係数の絶対値が1以下になる証明\n\n\n\n\n\n\nProof: 判別式を用いた証明\n\n\n\n\n\n任意の \\(t\\in\\mathbb R\\) について\n\\[\n\\begin{align*}\n\\operatorname{Var}(tX + Y) &= t^2\\operatorname{Var}(X) + 2t\\operatorname{Cov}(X, Y) + \\operatorname{Var}(Y)\\\\\n                   &\\geq 0\n\\end{align*}\n\\]\n非負の２次形式の判別式 \\(D\\) は0以下なので\n\\[\n\\begin{align*}\nD &= \\operatorname{Cov}(X, Y)^2 - \\operatorname{Var}(X)\\operatorname{Var}(Y)\\\\\n  &\\leq 0\n\\end{align*}\n\\]\n整理すると\n\\[\n\\begin{align*}\n&\\frac{\\operatorname{Cov}(X, Y)^2 }{\\operatorname{Var}(X)\\operatorname{Var}(Y)} \\leq 1\\\\[3pt]\n&\\Rightarrow \\vert \\operatorname{Corr}_{xy}\\vert^2 \\leq 1\\\\[3pt]\n&\\Rightarrow -1 \\leq \\operatorname{Corr}_{xy} \\leq 1\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nProof: 分散標準化した変数の和\n\n\n\n\n\n確率変数 \\(X, Y\\) の標準偏差をそれぞれ \\(\\sigma_x, \\sigma_y\\) とします．\n\\[\n\\begin{align*}\n0 &\\leq \\operatorname{Var}\\bigg(\\frac{X}{\\sigma_x} \\pm \\frac{Y}{\\sigma_y}\\bigg)\\\\[3pt]\n  &= \\operatorname{Var}\\bigg(\\frac{X}{\\sigma_x}\\bigg) \\pm 2\\operatorname{Cov}\\bigg(\\frac{X}{\\sigma_x},\\frac{Y}{\\sigma_y}\\bigg) + \\operatorname{Var}\\bigg(\\frac{Y}{\\sigma_y}\\bigg)\\\\[3pt]\n  &= \\frac{1}{\\sigma^2_x}\\operatorname{Var}(X) \\pm \\frac{2}{\\sigma_x\\sigma_y}\\operatorname{Cov}(X, Y) + \\frac{1}{\\sigma^2_y}\\operatorname{Var}(Y)\\\\[3pt]\n  &= 2 \\pm \\frac{2}{\\sigma_x\\sigma_y}\\operatorname{Cov}(X, Y)\\\\[3pt]\n  &= 2\\pm 2 \\operatorname{Corr}_{xy}\n\\end{align*}\n\\]\n従って，\\(-1 \\leq \\operatorname{Corr}_{xy} \\leq 1\\) を得ます．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>相関関係</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/random_variable.html",
    "href": "posts/statistics101/random_variable.html",
    "title": "3  確率変数",
    "section": "",
    "text": "確率変数\n\\[\n\\{\\omega\\in\\Omega\\vert X(\\omega)\\leq x\\} \\subset \\Omega\n\\]\nが可測集合であるならば，つまり，\n\\[\n\\{\\omega\\in\\Omega\\vert X(\\omega)\\leq x\\} \\in \\mathcal{B}\n\\]\nであるとき，任意の実数 \\(x\\) に対して，\\(X\\leq x\\) である確率は\n\\[\n\\Pr(X\\leq x) = P(\\{\\omega\\in\\Omega\\vert X(\\omega)\\leq x\\})\n\\]\nとして確率 \\(P\\) を用いて与えることができます．なお，この \\(x\\) を実現値 といい，一般的には確率変数を大文字， 実現値を小文字で表します．実現値の全体を，\\(X\\) の標本空間といい，\n\\[\n\\mathcal{X} = \\{X(\\omega)\\vert\\omega\\in\\Omega\\}\n\\]\nと表されます．\n任意の \\(a\\in\\mathbb R\\) に対して, \\(x\\) を右から \\(a\\) に近づけると \\(F_X(x)\\) が \\(F_X(a)\\) に収束するとき， \\(F_X(x)\\) は右連続といいます．\\(F_X(x)\\) が右連続かつ左連続であるとき，\\(F_X(x)\\) は点 \\(a\\) で連続であるといいます．\nCode\nimport numpy as np\nfrom scipy.stats import binom\nimport plotly.graph_objs as go\nimport plotly.io as pio\n\nn, p = 5, 0.4\nx = np.arange(-1, 7)\nprob = binom.cdf(x, n, p)\n\n# Create a step plot with right-continuous steps\ntrace = go.Scatter(\n    x=x,\n    y=prob,\n    mode='lines+markers',\n    line_shape='hv',  # Horizontal then vertical step-wise\n    marker=dict(symbol='0'),\n    name='CDF is Right-Continuous',\n)\n\n# Layout for the plot\nlayout = go.Layout(\n    title='CDF is Right-Continuous',\n    xaxis=dict(title='X'),\n    yaxis=dict(title='累積確率')\n)\n\n# Create the figure\nfig = go.Figure(data=[trace], layout=layout)\n\n# Display the plot\npio.show(fig)\n上記は \\(X\\sim\\operatorname{Binom}(5, 0.4)\\) のCDFを描いたもので，右側連続で階段関数の形状になっています．一般的に，CDFの形状が階段関数のとき，\\(X\\) は離散型確率変数，\\(F_X(x)\\) が連続関数のとき \\(X\\) は連続型確率変数と分類します．\nQuantile関数は，累積分布関数の逆関数に相当する関数ですが，左連続という点で違いがあります． \\(X\\sim\\operatorname{Bernoulli}(p)\\) を考えたとき，\n\\[\n\\begin{align*}\nF_X(x) &= (1-p)\\mathbb 1(x\\geq 0) + p\\mathbb 1(x\\geq 1)\\\\\nQ_X(u) &= \\mathbb 1(u &gt; 1-p)\n\\end{align*}\n\\]\nと定義されます．\\(p=0.5\\) とすると，\\(F_X(1) = 1, Q_X(0.6) = 1\\) となります．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>確率変数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/random_variable.html#確率変数",
    "href": "posts/statistics101/random_variable.html#確率変数",
    "title": "3  確率変数",
    "section": "",
    "text": "Def: 確率変数 \n\\(\\Omega\\) を全事象，\\(\\mathcal{B}\\) を \\(\\Omega\\) の可測集合族，\\(P\\) を \\((\\Omega, \\mathcal{B})\\) 上の確率とするとき， \\(\\omega\\in\\Omega\\) に対して実数値 \\(X(\\omega) \\in \\mathbb{R}\\) を対応させる関数 \\(X\\) を確率変数という．\n\n\n\n\n\n\n\n\n\n\nDef: 累積分布関数 \n確率変数 \\(X\\) の累積分布関数を \\(F_X(x)\\) で表し，\n\\[\nF_X(x) = \\Pr(X\\leq x) \\quad x\\in \\mathbb{R}\n\\]\nで定義する．\n\n\n\nTheorem 3.1 \n関数 \\(F\\) がある確率変数の分布関数になるために必要十分条件は，３つの条件が成り立つことである\n\n\\(\\lim_{x\\to-\\infty}F(x) = 0, \\lim_{x\\to\\infty}F(x) = 1\\)\n\\(F(x)\\) は \\(x\\) の非減少関数である\n\\(F(x)\\) は右連続関数である\n\n\n\n\n\n\n\n\nTheorem 3.2 \n連続型確率変数 \\(X\\) について，その分布関数を \\(F_X(x)\\) とする．新たに \\(Y = F_X(x)\\) という確率変数を考えたとき，\n\\[\nY \\sim \\operatorname{Uniform}(0, 1)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(F_X: \\mathbb R\\to [0, 1]\\) なので, \\(\\operatorname{support}(Y) = [0, 1]\\). \\(Y\\) についての累積分布関数を \\(G(y)\\) とすると，\n\\[\n\\begin{align*}\nG(y) &= \\Pr(Y \\leq y)\n\\end{align*}\n\\]\n関数 \\(F_X(\\cdot)\\) は単調増加関数で，区間 \\(y\\in [0, 1]\\) で逆関数 \\(X = F^{-1}_X(Y)\\) が定義できるため\n\\[\n\\begin{align*}\nG(y) &= \\Pr(Y \\leq y)\\\\\n     &= \\Pr(F_X(X)\\leq y)\\\\\n     &= \\Pr(X\\leq F^{-1}_X(y))\\\\\n     &= F_X(F^{-1}_X(y))\n\\end{align*}\n\\]\nこのとき，両辺を \\(y\\) で微分する．\n\\[\n\\frac{\\mathrm{d}G(y)}{\\mathrm{d}y}=g(y)\n\\]\nとすると，\\(g(y)\\) はpdfに相当する．RHSについて，\n\\[\n\\begin{align*}\n\\frac{\\mathrm{d}F_X(F^{-1}_X(y))}{\\mathrm{d}y}\n    &= f_X(F^{-1}_X(y))[f_X(F^{-1}_X(y))]^{-1} = 1\n\\end{align*}\n\\]\n\\(y\\in [0, 1]\\) において \\(g(y) = 1\\) が成立することから，\n\\[\nY\\sim\\operatorname{Uniform}(0, 1)\n\\]\n\n\n\n\nDef: Quantile function（分位点関数） \n確率変数 \\(X\\) についての quantile function \\(Q_X: (0, 1)\\to \\mathbb R\\) は\n\\[\nQ_X(u) = \\inf\\{x\\in\\mathbb R: F_X(x) \\geq u\\}\n\\]\nと左連続で定義される．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>確率変数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/random_variable.html#連続確率変数",
    "href": "posts/statistics101/random_variable.html#連続確率変数",
    "title": "3  確率変数",
    "section": "連続確率変数",
    "text": "連続確率変数\n\nDef: 絶対連続型の確率変数 \n累積分布関数 \\(F\\) をもつ確率変数 \\(X\\) が次の条件を満たす確率密度関数 \\(f\\) を持つとき，絶対連続(absolutely continuous)という：\n\\[\n\\begin{gather}\nf(x) \\geq 0 \\quad \\forall x\\\\\nF(b) - F(a) = \\int^b_a f(x)\\mathrm{d}x \\quad \\text{where } a\\leq b\n\\end{gather}\n\\]\n\n \\(f\\) のnon-negativity性質は，累積分布関数はnon-decreasingであること，及び \\(F^\\prime(x) = f(x)\\) であることから分かる． また確率変数 \\(X\\) が確率密度関数 \\(f(x)\\) を持つとき，「\\(X\\) は \\(f(x)\\) に従う」とよく言われる．\n\n\nTheorem 3.3 変数変換と確率密度関数 \n\\(f\\) を確率密度関数，\\(a &gt; 0\\) とし，\n\\[\ng(x) = af(ax)\n\\]\nと関数 \\(g\\) を定義すると\n\\[\n\\int^\\infty_{-\\infty} g(x)\\mathrm{d} x = 1\n\\]\nとなる\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(a &gt; 0, f\\geq 0\\) より \\(g\\geq 0\\) は自明．また，\\(ax = z\\) と変数変換すると\n\\[\n\\begin{align*}\n\\int^\\infty_{-\\infty} g(x)\\mathrm{d} x &= \\int^\\infty_{-\\infty} af(ax)\\mathrm{d} x \\\\\n                                       &= \\int^\\infty_{-\\infty} af(z) \\frac{\\mathrm{d} x}{\\mathrm{d} z}\\mathrm{d} z\\\\\n                                       &= \\int^\\infty_{-\\infty} af(z) \\frac{1}{a}\\mathrm{d} z = 1\n\\end{align*}\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>確率変数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/expectation.html",
    "href": "posts/statistics101/expectation.html",
    "title": "4  期待値",
    "section": "",
    "text": "期待値の性質\n定義より確率密度関数で重みづけた平均が確率変数の期待値になると解釈することができます．meanは分布の位置を表すパラメーターとも解釈できるので location parameter（位置母数）と呼ぶこともあります．一方，標準偏差 \\(\\sigma\\) はscale parameter（尺度母数）といいます．\n▶  離散確率変数の変数変換と期待値\n離散型確率変数 \\(X\\) について \\(Y = g(X)\\) を考えたとき，\\(A_y = \\{x\\vert g(x)=y\\}\\) とおくと\n\\[\n\\begin{align*}\n\\mathbb E[g(X)]\n    &= \\sum_x p(x)g(x)\\\\\n    &= \\sum_y\\sum_{A_y} p(x)g(x)\\\\\n    &= \\sum_y\\sum_{A_y} p(x)y\\\\\n    &= \\sum_y y \\sum_{A_y} p(x)\\\\\n    &= \\sum_y \\Pr(Y=y)y\\\\\n    &= \\mathbb E[Y]\n\\end{align*}\n\\]\n上の式展開では, \\(\\Pr(X=x) = p(x)\\) としています．\n▶  期待値と重心\n期待値の解釈の１つとして 確率変数 \\(X\\) の重心と考えるパターンがあります．\\(x_i\\) の値を \\(p_i\\) の確率でとる離散型確率変数 \\(X\\) の場合，\nこのとき，重心 \\(\\mu\\) はモーメントの釣り合い = 右回りモーメントが0になる地点となりますが\n\\[\n\\begin{align*}\n\\text{右回りモーメント}\n    &= \\sum_i (x_i - \\mu)p_i\\\\\n    &= \\sum x_ip_i - \\sum_i p_i \\mu\\\\\n    &= \\mathbb E[X] - \\mu = 0\n\\end{align*}\n\\]\n従って，\\(\\mathbb E[X] = \\mu\\) より，期待値と重心が対応することがわかります．連続型確率変数でも確率密度関数を重さのある棒の断面積と みなすことで離散型と同じく期待値と重心が対応することを確かめることができます．\n\\(\\lim_{b\\to\\infty}\\bigg[x(1 - F(x))\\bigg]^b_0\\) について，\\(\\lim_{b\\to\\infty}b(1 - F(b))=0\\) とは限らない点に注意が必要です．\n期待値の線型性定理の応用として, 期待値が定義できるという前提の下，\n\\[\n\\begin{align*}\n\\mathbb E[g(X, Y) + h(X, Y)] &= \\mathbb E[g(X, Y)] + \\mathbb E[h(X, Y)]\\\\\n\\mathbb E[ag(X, Y) + b] &= a\\mathbb E[g(X, Y)] + b \\quad (a, b \\text{: constants})\n\\end{align*}\n\\] が成り立ちます．\n\\[\n\\begin{align*}\n\\mathbb E[g(X, Y) + h(X, Y)]\n    &= \\int_{\\mathbb R}\\int_{\\mathbb R}(g(x, y) + h(x, y))f(x, y) \\,\\mathrm{d}x\\,\\mathrm{d}y\\\\\n    &= \\int_{\\mathbb R}\\int_{\\mathbb R}g(x, y)f(x, y) \\,\\mathrm{d}x\\,\\mathrm{d}y + \\int_{\\mathbb R}\\int_{\\mathbb R}h(x, y)f(x, y) \\,\\mathrm{d}x\\,\\mathrm{d}y\\\\\n    &= \\mathbb E[g(X, Y)] + \\mathbb E[h(X, Y)]\n\\end{align*}\n\\]\n上記のように確認できます．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>期待値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/expectation.html#期待値の性質",
    "href": "posts/statistics101/expectation.html#期待値の性質",
    "title": "4  期待値",
    "section": "",
    "text": "Def: 連続確率変数の期待値 \n\\(f\\) を確率変数 \\(X\\) の確率密度関数とする．\\(\\int_{\\mathbb R} \\vert x\\vert f(x) \\mathrm{d}x &lt; \\infty\\) のとき，\\(X\\) の期待値は以下のように定義する:\n\\[\n\\mathbb E[X] = \\int_{\\mathbb R} x f(x) \\mathrm{d}x\n\\]\nまた，\\(X\\) の関数 \\(g(X)\\) の期待値は \\(\\int_{\\mathbb R} \\vert g(x)\\vert f(x) \\mathrm{d}x &lt; \\infty\\) ならば\n\\[\n\\mathbb E[g(X)] = \\int_{\\mathbb R} g(x) f(x) \\mathrm{d}x\n\\]\n\n\n\nExample 4.1 指数分布の期待値 \nrate parameter \\(\\lambda\\) の指数分布に従う確率変数 \\(X\\) を考えます．\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\int^\\infty_0 x \\lambda \\exp(-\\lambda x)\\mathrm{d}x\\\\\n             &= \\bigg[-x\\exp(-\\lambda x)\\bigg]^\\infty_0 + \\int^\\infty_0 \\exp(-\\lambda x)\\mathrm{d}x\\\\\n             &= \\int^\\infty_0 \\exp(-\\lambda x)\\mathrm{d}x\\\\\n             &= -\\frac{1}{\\lambda}\\bigg[\\exp(-\\lambda x)\\bigg]^\\infty_0\\\\\n             &= \\frac{1}{\\lambda}\n\\end{align*}\n\\]\n指数分布は電球の寿命などに応用される分布ですが，rate parameter \\(\\lambda\\) が小さいほど期待値（= 電球の寿命）が大きくなることが分かります．\n\n\nExample 4.2 期待値が定義できない離散分布 \n確率変数 \\(X\\) のsupportを加算集合 \\(\\{2, 2^2, 2^3, \\cdots\\}\\) とする．確率関数を\n\\[\n\\Pr(X = 2^i) = \\frac{1}{2^i} \\quad (i = 1, 2, \\cdots)\n\\]\nこのとき，\n\\[\n\\sum_{i=1}^\\infty \\Pr(X=2^i) = \\sum_{i=1}^\\infty\\frac{1}{2^i} = 1\n\\]\nと確率の公理を満たしていることが分かる．一方，\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_{i=1}^\\infty 2^i \\frac{1}{2^i}\\\\\n    &= \\sum_{i=1}^\\infty 1 = \\infty\n\\end{align*}\n\\]\n従って，確率変数 \\(X\\) の分布は，期待値が定義できない分布であることがわかる．\n\n\nExample 4.3 期待値が定義できない連続分布 \n確率密度関数 \\[\nf(x) = \\begin{cases}\n0 & x &lt; 1\\\\\n\\frac{1}{x^2} & x\\geq 1\n\\end{cases}\n\\]\nという確率変数 \\(X\\) を考える．\n\\[\n\\begin{align*}\n\\int_1^\\infty f(x) \\mathrm{d}x\n    &= \\left[\\frac{1}{x}\\right]^1_\\infty = 1\n\\end{align*}\n\\]\n一方，\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\int_1^\\infty xf(x) \\mathrm{d}x\\\\\n    &= \\int_1^\\infty\\frac{1}{x}\\mathrm{d}x\\\\\n    &= \\left[\\log(x)\\right]_1^\\infty = \\infty\n\\end{align*}\n\\]\n従って，確率変数 \\(X\\) の分布は，期待値が定義できない分布であることがわかる．\n\n\n\n\n\n\n\n\n重さのない棒の中央を原点とする\n原点から右側をプラス，左側をマイナスとして，原券からの距離 \\(x_i\\) の場所に重さ \\(p_i\\) のおもりを吊り下げる\n\n\n\n\n\n\nTheorem 4.1 Tail probabilities \n\\([0, b]\\) の定義域をもつ非負確率変数 \\(X\\) を考える．\\(F\\) を累積分布関数とするとき\n\\[\n\\mathbb E[X] = \\int_0^b (1 - F(x))\\mathrm{d}x\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\bigg[xF(x)\\bigg]^b_0 = \\int^b_0xf(x) \\mathrm{d}x + \\int^b_0F(x) \\mathrm{d}x\n\\]\nを用いると\n\\[\n\\begin{align*}\n\\mathbb E[X] &= b - \\int^b_0F(x) \\mathrm{d}x\\\\\n             &= \\int^b_0 1 \\mathrm{d}x - \\int^b_0F(x) \\mathrm{d}x\\\\\n             &= \\int_0^b (1 - F(x))\\mathrm{d}x\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 4.2 Tail probabilities \n\\(\\mathbb E[\\vert X\\vert]&lt;\\infty\\) をもつ非負の確率変数 \\(X\\) について，\\(F(x)\\) を分布関数とすると\n\\[\n\\mathbb E[X] = \\int_0^\\infty (1 - F(x))\\mathrm{d}x\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\int_0^\\infty (1 - F(x))\\mathrm{d}x\n    &= \\int_0^\\infty \\Pr(X\\geq x)\\mathrm{d}x\\\\\n    &= \\int_0^\\infty \\mathbb E[\\mathbb 1(X\\geq x)]\\mathrm{d}x\\\\\n    &= \\int_0^\\infty \\int_0^\\infty \\mathbb 1(X\\geq x) \\mathrm{d}F(x)\\mathrm{d}x\\\\\n    &= \\int_0^\\infty \\left\\{\\int_0^\\infty \\mathbb 1(X\\geq x) \\mathrm{d}x\\right\\}\\mathrm{d}F(x) \\quad\\because\\text{積分の順序交換}\\\\\n    &= \\mathbb E\\left[\\int_0^\\infty \\mathbb 1(X\\geq x) \\mathrm{d}x\\right]\\\\\n    &= \\mathbb E\\left[\\int_0^X 1 \\mathrm{d}x\\right]\\\\\n    &= \\mathbb E[X]\n\\end{align*}\n\\]\n\n\n\n\n\n\nTheorem 4.3 \n\\([0, \\infty)\\) の定義域をもつ非負確率変数 \\(X\\) を考える．\\(\\mathbb E[\\vert X^{p+1} \\vert] &lt;\\infty\\) が定義可能及び， \\(F\\) を累積分布関数とするとき\n\\[\n\\mathbb E[X^p] = \\int_0^\\infty px^{p-1} (1 - F(x))\\mathrm{d}x \\quad \\text{where } p &gt; 0\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\bigg[x^p(1 - F(x))\\bigg]^\\infty_0 = \\int^\\infty_0 p x^{p-1}(1 -F(x))\\mathrm{d}x - \\int^\\infty_0 x^{p}f(x)\\mathrm{d}x\n\\end{align*}\n\\]\n\\(\\text{RHS} = 0\\) であるので\n\\[\n\\mathbb E[X^p] = \\int_0^\\infty px^{p-1} (1 - F(x))\\mathrm{d}x\n\\]\n\n\n\n\nExample 4.4 : Discrete Tail Probability \n同様の考えで定義域を \\(0,1,2,3,\\cdots\\) とする離散確率変数 \\(X\\) について\n\\[\n\\mathbb E[X] = \\sum_{k=0}^\\infty \\Pr(X &gt; k)\n\\]\nが成立します．\n\\[\n\\begin{align*}\n\\Pr(X &gt; k) &= \\Pr(X = k+1) + \\Pr(X = k+2) + \\cdots\\\\\n           &= \\sum_{l=k+1}^\\infty \\Pr(X=l)\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\sum_{k=0}^\\infty \\Pr(X &gt; k) &= \\sum_{k=0}^\\infty \\sum_{l=k+1}^\\infty \\Pr(X=l)\\\\\n                             &= \\sum_{l=1}^\\infty\\sum_{k=0}^{l-1}\\Pr(X=l) \\quad\\because \\Pr(X=l) &gt; 0 \\\\\n                             &= \\sum_{l=1}^\\infty l\\Pr(X=l)\\\\\n                             &= \\sum_{l=0}^\\infty l\\Pr(X=l)\\\\\n                             &= \\mathbb E[X]\n\\end{align*}\n\\] \\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\nExample 4.5 \n\\(0,1,2,3,\\cdots\\) とする離散確率変数 \\(X\\) について\n\\[\n\\mathbb E[X^2] = \\sum_{k=0}^\\infty \\Pr(X &gt; k)(2k+1)\n\\]\nも成立する．\n\\[\n\\begin{align*}\n\\sum_{k=0}^\\infty \\Pr(X &gt; k)(2k+1)\n    &= \\sum_{k=0}^\\infty \\sum_{l=k+1}^\\infty \\Pr(X=l)(2k+1)\\\\\n    &= \\sum_{l=1}^\\infty \\sum_{k=0}^{l-1}\\Pr(X=l)(2k+1)\\\\\n    &= \\sum_{l=1}^\\infty \\Pr(X=l)\\sum_{k=0}^{l-1}(2k+1)\\\\\n    &= \\sum_{l=1}^\\infty l^2\\Pr(X=l)\\\\\n    &= \\mathbb E[X^2]\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\n\nTheorem 4.4 期待値の線型性 \n\\(a, b\\) を実数，確率変数 \\(X, Y\\) について以下が成り立つ\n\\[\n\\mathbb E[aX + bY] = a\\mathbb E[X] + b\\mathbb E[Y]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n確率変数 \\(X, Y\\) が有限加算な標本空間で定義されているケースにて以下を示す．\n\n\\(\\mathbb E[X + Y] = \\mathbb E[X] + \\mathbb E[Y]\\)\n\\(\\mathbb E[cX] = c\\mathbb E[X]\\)\n\n ▶  1. \\(\\mathbb E[X + Y] = \\mathbb E[X] + \\mathbb E[Y]\\)\n確率変数 \\(X\\) は \\(\\{x_1, \\cdots, x_m\\}\\), 確率変数 \\(Y\\) は \\(\\{y_1, \\cdots, y_n\\}\\) の値をそれぞれ取りうるとする． このとき，\\(Z = X + Y\\) の標本空間 \\(\\{z_1, \\cdots, z_k\\}\\) について \\(k\\leq m + n\\) が成り立つ．\n\\(A_l = \\{(i,j): x_i + y_j = z_l\\}\\) としたとき，\n\\[\n\\begin{align*}\n\\mathbb E[X+Y]\n    &= \\sum_{l=1}^kz_l\\Pr(A_l)\\\\\n    &= \\sum_{l=1}^k\\sum_{(i,j)\\in Z_l}(x_i + y_j)\\Pr(x_i, y_j)\\\\\n    &= \\sum_{i=1}^m\\sum_{j=1}^n(x_i + y_j)\\Pr(x_i, y_j)\\\\\n    &= \\sum_{i=1}^m\\sum_{j=1}^nx_i\\Pr(x_i, y_j) + y_j\\Pr(x_i, y_j)\\\\\n    &= \\sum_{i=1}^m\\sum_{j=1}^n[x_i\\Pr(x_i, y_j) + y_j\\Pr(x_i, y_j)]\\\\\n    &= \\sum_{i=1}^mx_i\\sum_{j=1}^nPr(x_i, y_j) + \\sum_{j=1}^ny_j\\sum_{i=1}^m\\Pr(x_i, y_j)\\\\\n    &=\\sum_{i=1}^mx_i \\Pr(x_i) + \\sum_{j=1}^ny_j \\Pr(y_j)\\\\\n    &= \\mathbb E[X] + \\mathbb E[Y]\n\\end{align*}\n\\]\n ▶  2. \\(\\mathbb E[cX] = c\\mathbb E[X]\\)\n\\[\n\\begin{align*}\n\\mathbb E[cX]\n    &= \\sum_{i=1}^m cx_i = \\Pr(cX = cx_i)\\\\\n    &= c\\sum_{i=1}^m x_i = \\Pr(X = x_i)\\\\\n    &= c\\mathbb E[X]\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nExample 4.6 : 変数変換と分散 \nmean \\(\\mu\\) をもつ確率変数 \\(X\\) と実数 \\(a, b\\) について\n\\[\n\\operatorname{Var}(aX + b) = a^2\\operatorname{Var}(X)\n\\]\nが成立します．証明は以下，\n\\[\n\\begin{align*}\n\\operatorname{Var}(aX + b)\n    &= \\mathbb E[(aX + b) - (a\\mu +b)^2]\\\\\n    &= \\mathbb E[a^2(X - \\mu)^2]\\\\\n    &= a^2 \\mathbb E[(X - \\mu)^2]\\\\\n    &= a^2\\operatorname{Var}(X)\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\n\nTheorem 4.5 : 確率変数の標準化 \n確率変数 \\(X\\) について, \\(\\mathbb E[X] = \\mu, \\operatorname{Var}(X) =\\sigma^2\\) が存在するとき，\n\\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]\nとおくと，\\(\\mathbb E[Z] = 0, \\operatorname{Var}(Z) = 1\\) が成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[Z]\n    &= \\mathbb E\\left[\\frac{X - \\mu}{\\sigma}\\right]\\\\\n    &= \\frac{\\mathbb E[X] - \\mu}{\\sigma}\\\\\n    &= 0\n\\end{align*}\n\\]\n\\[\n\\begin{align*}\n\\operatorname{Var}(Z)\n    &= \\mathbb E[Z^2]\\\\\n    &= \\mathbb E\\left[\\frac{(X - \\mu)^2}{\\sigma^2}\\right]\\\\\n    &= \\frac{\\sigma^2}{\\sigma^2}\\\\\n    &= 1\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 4.6 positive operator \n確率変数 \\(X, Y\\) について，\\(X\\geq Y\\) が成り立つとき，\n\\[\n\\mathbb E[X] \\geq \\mathbb E[Y]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(X\\geq Y\\) より \\(X - Y \\geq 0\\). 期待値はpositive operatorなので\n\\[\n\\mathbb E[X - Y] \\geq 0\n\\]\n従って，期待値の線型性を用いると\n\\[\n\\begin{align*}\n\\mathbb E[X - Y] &= \\mathbb E[X] - \\mathbb E[Y] \\geq 0\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 4.7 \n確率変数 \\(X\\) について,\n\\[\n\\mathbb E[\\vert X \\vert] \\geq \\vert \\mathbb E[X] \\vert\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\vert X\\vert \\geq X\\) より\n\\[\n\\mathbb E[\\vert X \\vert] \\geq \\mathbb E[X]\n\\]\nまた, \\(\\vert X\\vert + X \\geq 0\\) より，\\(\\mathbb E[\\vert X\\vert + X] \\geq 0\\)， つまり，\n\\[\n\\mathbb E[\\vert X \\vert] \\geq -\\mathbb E[X]\n\\]\n以上より，\\(\\mathbb E[\\vert X \\vert] \\geq \\vert \\mathbb E[X] \\vert\\)\n\n\n\n\n\nTheorem 4.8 Schwarz inquality \n確率変数 \\(X, Y\\) についてシュワルツの不等式が成立することを示せ\n\\[\n\\left(\\mathbb E[XY]\\right)^2 \\leq \\mathbb E[X^2]\\mathbb E[Y^2]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nQuadratic functionを以下のように定義します\n\\[\n\\begin{align*}\ng(t)\n    &= \\mathbb E[(tX - Y)^2]\\\\\n    &= t^2\\mathbb E[X^2] - 2t\\mathbb E[XY] + E[Y^2]\\\\\n    &\\geq 0\n\\end{align*}\n\\]\nこのとき，\\(g(t)\\) はnon-negativeなので判別式について以下が成立する\n\\[\nD/4 = \\left(\\mathbb E[XY]\\right)^2 - \\mathbb E[X^2]\\mathbb E[Y^2]\\leq 0\n\\]\n従って，\\(\\left(\\mathbb E[XY]\\right)^2 \\leq \\mathbb E[X^2]\\mathbb E[Y^2]\\)\n\n\n\n\n\nTheorem 4.9 : Triangle inequality \n確率変数 \\(X, Y\\) について，以下のような三角不等式が成立することを示せ\n\\[\n\\sqrt{\\mathbb E[(X+Y)^2]} \\leq \\sqrt{\\mathbb E[X^2]} + \\sqrt{\\mathbb E[Y^2]}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nシュワルツの不等式を用いて以下のように示せる\n\\[\n\\begin{align*}\n\\mathbb E[(X+Y)^2]\n    &= \\mathbb E[X^2] + 2\\mathbb E[XY] + \\mathbb E[Y^2]\\\\\n    &= \\mathbb E[X^2] + 2\\sqrt{(\\mathbb E[XY])^2} + \\mathbb E[Y^2]\\\\\n    &\\leq \\mathbb E[X^2] + 2\\sqrt{\\mathbb E[X^2]\\mathbb E[Y^2]} + \\mathbb E[Y^2]\\\\\n    &= (\\sqrt{\\mathbb E[X^2]} + \\sqrt{\\mathbb E[Y^2]})^2\n\\end{align*}\n\\]\n両辺について，square rootをとると，\n\\[\n\\sqrt{\\mathbb E[(X+Y)^2]} \\leq \\sqrt{\\mathbb E[X^2]} + \\sqrt{\\mathbb E[Y^2]}\n\\]\n\n\n\n\n独立な確率変数と期待値\n\n\nTheorem 4.10 互いに独立な確率変数の積の期待値 \n\\(\\mathbb E[\\vert X\\vert ]&lt;\\infty, \\mathbb E[\\vert Y\\vert ]&lt;\\infty\\) を満たす, 確率空間 \\((\\Omega, \\mathscr{F},P)\\) 上で定義された確率変数 \\(X, Y\\) を考える． \\(X \\perp Y\\) であるとき，次が成立する\n\\[\n\\mathbb E[XY] = \\mathbb E[X]\\mathbb E[Y]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[XY] &= \\int\\int_\\Omega xy f(x, y)\\mathrm{d}x\\mathrm{d}y\\\\\n              &= \\int\\int_\\Omega xy f_X(x)f_Y(y)\\mathrm{d}x\\mathrm{d}y \\quad\\because{\\text{independence}}\\\\\n              &= \\left(\\int xf_X(x)\\mathrm{d}x\\right)\\left(\\int yf_Y(y)\\mathrm{d}y\\right)\\\\\n              &= \\mathbb E[X]\\mathbb E[Y]\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 4.11 互いに独立な確率変数についての関数の積の期待値 \n\\(\\mathbb E[\\vert X\\vert ]&lt;\\infty, \\mathbb E[\\vert Y\\vert ]&lt;\\infty\\) を満たす, 確率空間 \\((\\Omega, \\mathscr{F},P)\\) 上で定義された確率変数 \\(X, Y\\) を考える． また，関数 \\(g(X)\\), \\(h(Y)\\) は期待値が存在するとする．\\(X \\perp Y\\) であるとき，次が成立する\n\\[\n\\mathbb E[g(X)h(Y)] = \\mathbb E[g(X)]\\mathbb E[h(Y)]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(X \\perp Y\\) なので，確率密度関数について \\(f(x, y) = f_X(x)f_Y(y)\\) が成立します．\n\\[\n\\begin{align*}\n\\mathbb E [g(X)h(Y)]\n    &= \\int_{\\mathbb R}\\int_{\\mathbb R}g(x)h(x)f(x, y)\\,\\mathrm{d}x\\,\\mathrm{d}y\\\\\n    &= \\int_{\\mathbb R}\\int_{\\mathbb R}g(x)h(x)f(x, y)\\,\\mathrm{d}x\\,\\mathrm{d}y\\\\\n    &= \\left\\{\\int_{\\mathbb R}g(x)f_X(x)\\,\\mathrm{d}x\\right\\}\\left\\{\\int_{\\mathbb R}h(x)f_Y(y)\\,\\mathrm{d}y\\right\\}\\\\\n    &= \\mathbb E[g(X)]\\mathbb E[h(Y)]\n\\end{align*}\n\\]\n\n\n\n\n\n条件付き期待値\n\n\nTheorem 4.12 Law of Total Expectation \n\\[\n\\mathbb E[Y] = \\mathbb E[\\mathbb E[Y\\vert X]]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n ▶  連続確率変数の場合\n\\[\n\\begin{align*}\n\\mathbb E[\\mathbb E[Y\\vert X]]\n    &= \\int \\mathbb E[Y\\vert X=u]f_X(u)\\mathrm{d}u\\\\\n    &= \\int \\left[\\int t f_Y(t\\vert x=u)\\mathrm{d}t\\right]f_X(u)\\mathrm{d}u\\\\\n    &= \\int \\int t f_Y(t\\vert x=u)f_X(u)\\mathrm{d}u\\mathrm{d}t\\\\\n    &= \\int t\\left[\\int f_{X,Y}(u, t)\\mathrm{d}u\\right]\\mathrm{d}t\\\\\n    &= \\int t f_Y(t)\\mathrm{d}t\\\\\n    &= \\mathbb E[Y]\n\\end{align*}\n\\]\n ▶  離散確率変数の場合\n\\[\n\\begin{align*}\n\\mathbb E[\\mathbb E[Y\\vert X]]\n    &= \\sum_{x\\in \\mathcal{X}}\\mathbb E[Y\\vert X] f_X(x)\\\\\n    &= \\sum_{x\\in \\mathcal{X}}\\sum_{y\\in \\mathcal{Y}} y f_{Y\\vert X}(y \\vert x) f_X(x)\\\\\n    &= \\sum_{x\\in \\mathcal{X}}\\sum_{y\\in \\mathcal{Y}} y f(x, y)\\\\\n    &= \\sum_{y\\in \\mathcal{Y}}y f_Y(y)\\\\\n    &= \\mathbb E[Y]\n\\end{align*}\n\\]\n\n\n\nLTEの法則より\n\\[\n\\begin{align*}\n\\mathbb E[Y] &= \\mathbb E[\\mathbb E[Y\\vert X, Z]]\\\\\n\\mathbb E[Y\\vert X] &= \\mathbb E[\\mathbb E[Y\\vert X, Z]\\vert X]\n\\end{align*}\n\\]\nが成立します．\nなお，\\(\\mathbb E[Y\\vert X], \\mathbb E[Y\\vert X=x]\\) の違いには注意が必要です．前者は確率変数ですが， 後者は確率変数の実現値の取りうる値を表しています．\\(X\\) を \\(x_1, \\cdots, x_k\\) の値を取る離散確率変数とすると， \\(\\mathbb E[Y\\vert X]\\) は \\(k\\) 個の値を取る離散確率変数となります．\n\n\nTheorem 4.13 \n\\(g(\\cdot)\\) を \\(\\mathbb E[g(X)Y] &lt; \\infty\\) を満たす \\(X\\) の任意の関数とする．このとき，\n\\[\n\\mathbb E[\\mathbb E[g(X)Y\\vert X]] = \\mathbb E[g(X)Y]\n\\]\nが成り立つ．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\mu_Y(x) = \\mathbb E[Y\\vert X = x]\\) とする．このとき，\n\\[\n\\begin{align*}\n\\mathbb E[g(X)\\mathbb E[Y\\vert X]]\n    &= \\mathbb E[g(X)\\mu_Y(X))\\\\\n    &= \\sum_{x\\in\\mathcal{X}}g(x)\\mu_Y(x)f_X(x)\\\\\n    &= \\sum_{x\\in\\mathcal{X}}g(x)\\sum_{y\\in\\mathcal{Y}}y f_{Y\\vert X}(y\\vert x)f_X(x)\\\\\n    &= \\sum_{x\\in\\mathcal{X}}g(x)\\sum_{y\\in\\mathcal{Y}} y f(x, y)\\\\\n    &= \\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}g(x)y f(x, y)\\\\\n    &= \\mathbb E[g(X)Y]\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 4.14 : CEF Decomposition Property \n確率変数 \\(X, Y\\) について，\n\\[\nY = \\mathbb E[Y\\vert X] + \\epsilon\n\\]\nとしたとき，\n\n\\(\\epsilon\\) は \\(X\\) について mean-independent, つまり, \\[\\mathbb E[\\epsilon\\vert X] = 0\\]\n\\(\\epsilon\\) は \\(X\\) の任意の関数に対して無相関, つまり, \\[\\operatorname{Cov}(h(X), \\epsilon) = 0\\]\n\\(\\operatorname{Var}(\\epsilon) = \\mathbb E[\\operatorname{Var}(Y\\vert X)]\\)\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n ▶  (1)\n\\[\n\\begin{align*}\n\\mathbb E[\\epsilon\\vert X]\n    &= \\mathbb E[Y - \\mathbb E[Y\\vert X]\\vert X]\\\\\n    &= \\mathbb E[Y\\vert X] - \\mathbb E[Y\\vert X]\\\\\n    &= 0\n\\end{align*}\n\\]\n ▶  (2)\n\\[\n\\begin{align*}\n\\mathbb E[h(X)\\epsilon] &= E[\\mathbb E[h(X)\\epsilon\\vert X]]\\\\\n                        &= E[h(X)\\mathbb E[\\epsilon\\vert X]]\\\\\n                        &= 0 \\quad \\because{\\text{mean independence}}\n\\end{align*}\n\\]\n ▶  (3)\n条件付き期待値の公式 \\(\\mathbb E[g(X, Y)] = \\mathbb E_X[\\mathbb E_{Y\\vert X}[g(X, Y)\\vert X]]\\) より\n\\[\n\\begin{align*}\n\\operatorname{Var}(\\epsilon)\n    &= \\mathbb E[(Y - \\mathbb E[Y\\vert X] )^2]\\\\\n    &= \\mathbb E\\{\\mathbb E[(Y - \\mathbb E[Y\\vert X])^2 \\vert X]\\}\\\\\n    &= \\mathbb E[\\operatorname{Var}(Y\\vert X)]\n\\end{align*}\n\\]\n\n\n\nCEF Decomposition Propertyは，確率変数 \\(Y\\) は確率変数 \\(X\\) で説明できるパートと，\\(X\\) の任意の関数と直行（orthogonal） な誤差項のパートに分解できることを示しています．\n\n\nTheorem 4.15 : MSE minimizer \n\\(g(\\cdot)\\) を \\(X\\) の任意の関数とする．このとき，\n\\[\n\\mathbb E[(Y - g(X))^2] \\geq \\mathbb E[(Y - \\mathbb E[Y\\vert X])^2]\n\\]\nが成立する．つまり，\\(\\mathbb E[Y\\vert X]\\) は，\\(X\\) のすべての関数の中で，MSEの意味で \\(Y\\) の最良近似を与える．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\mu_Y(X) = \\mathbb E[Y\\vert X]\\) とする．\n\\[\n\\begin{align*}\n\\mathbb E[(Y - g(X))^2]\n    =& \\mathbb E[(Y - g(X) + \\mu_Y(X) - \\mu_Y(X))^2]\\\\[5pt]\n    =& \\mathbb E[(Y - \\mu_Y(X))^2 + (\\mu_Y(X)- g(X))^2\\\\\n     &+ 2(Y - \\mu_Y(X))(\\mu_Y(X)- g(X))]\\\\[5pt]\n    =& \\mathbb E[(Y - \\mu_Y(X))^2] + \\mathbb E[(\\mu_Y(X)- g(X))^2]\\\\\n     &+ 2\\mathbb E[(Y - \\mu_Y(X))(\\mu_Y(X)- g(X))]\\\\[5pt]\n    =& \\mathbb E[(Y - \\mu_Y(X))^2] + \\mathbb E[(\\mu_Y(X)- g(X))^2]\\\\[5pt]\n    \\geq& \\mathbb E[(Y - \\mu_Y(X))^2]\n\\end{align*}\n\\]\n\n\n\n\n\nMoments\n\nDef: モーメント \n\\(0 &lt; r &lt; \\infty\\) を満たすような正の整数 \\(r\\) に対して，確率変数 \\(X\\) のthe r-th moment（原点周りのモーメント）は以下のように定義される\n\\[\n\\mathbb E[X^r] = \\int_{\\mathbb R} x^r \\mathrm{d}F_X(x)\n\\]\nthe r-th central moment(平均周りのモーメント)は \\(\\mathbb E[(X - \\mathbb E[X])^r]\\) と定義される．\n\n原点周りのモーメントと平均周りのモーメントは以下のような関係で理解することができます．\\(\\mathbb E[X] = \\mu\\) とすると，\n\\[\n\\begin{align*}\n(X - \\mu + \\mu)^r\n    &= \\sum_{k=0}^k \\left(\\begin{array}{c}r\\\\ k\\end{array}\\right)(X - \\mu)^k\\mu^{r-k}\n\\end{align*}\n\\]\n両辺の期待値をとると\n\\[\n\\begin{align*}\n\\mathbb E[X^r] = \\sum_{k=0}^k \\left(\\begin{array}{c}r\\\\ k\\end{array}\\right)\\mathbb E[(X - \\mu)^k]\\mu^{r-k}\n\\end{align*}\n\\]\nまた，平均周りのモーメントを原点周りのモーメントで表すとなると，\n\\[\n\\begin{align*}\n(X - \\mu)^r = \\sum_{k=0}^r(-1)^{r-k}X^r\\mu^{r-k}\n\\end{align*}\n\\]\n同様に期待値をとると\n\\[\n\\begin{align*}\n\\mathbb E[(X - \\mu)^r] = \\sum_{k=0}^r(-1)^{r-k}\\mathbb E[X^r]\\mu^{r-k}\n\\end{align*}\n\\]\n\n\nTheorem 4.16 : higher moment and lower moment \n\\(\\mathbb E[\\vert X\\vert^r] &lt; \\infty\\) のとき，\\(0 &lt; q &lt; r\\) について，\\(\\mathbb E[\\vert X\\vert^q] &lt; \\infty\\) が成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n関数 \\(g:\\mathbb R\\to \\mathbb R\\) を\n\\[\ng(x) = \\vert x\\vert^r + 1\n\\]\n関数 \\(g:\\mathbb h\\to \\mathbb R\\) を \\(h(x) = \\vert x\\vert^q\\) と定義する．\\(0 &lt; q &lt; r\\) より，\n\\[\nh(x) &lt; g(x) \\quad \\forall x \\in \\operatorname{support}(X)\n\\]\nこのとき，\n\\[\n\\begin{align*}\n\\int_{\\mathbb{R}} |x|^r + 1 \\mathrm{d}F\n    &= \\int_{\\mathbb{R}} |x|^r\\mathrm{d}F + \\underbrace{\\int_{\\mathbb{R}} 1\\mathrm{d}F}_{=1}\\\\\n    &&gt; \\int_{\\mathbb{R}} |x|^s\\mathrm{d}F\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\infty &&gt; \\mathbb E[\\vert X\\vert^r] + 1\\\\\n       &&gt; \\int_{\\mathbb{R}} |x|^q \\mathrm{d}F \\\\\n       &= \\mathbb E[\\vert X\\vert^q]\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 4.17 : Mean minimizes squared error \n確率変数 \\(X\\) について，\\(\\mathbb E[\\vert X\\vert^2]&lt;\\infty\\) とする．このとき，\n\\[\n\\mathbb E[X] = \\arg\\min_b \\mathbb E[(X - b)^2]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\mu_X = \\mathbb E[X], \\sigma^2 = \\mathbb E[(X - \\mu_X)^2]\\) とおく．\n\\[\n\\begin{align*}\nE[(X - b)^2]\n    &= \\mathbb E[(X - \\mu_X + \\mu_X - b)^2]\\\\\\\n    &= \\mathbb E[(X - \\mu_X)^2] + \\mathbb E[(\\mu_X - b)^2] + \\mathbb E[(X - \\mu_X)(\\mu_x - b)]\\\\\n    &= \\sigma^2 + \\mathbb E[(\\mu_X - b)^2] + (\\mu_X - b)\\mathbb E[X - \\mu_X]\\\\\n    &= \\sigma^2 + \\mathbb E[(\\mu_X - b)^2]\n\\end{align*}\n\\]\n\\(\\mathbb E[(\\mu_X - b)^2] \\geq 0\\) が成立し，また, 等号成立条件は \\(\\mu_X = b\\)．従って，\n\\[\n\\mathbb E[X] = \\arg\\min_b \\mathbb E[(X - b)^2]\n\\]\n\n\n\n\n\nMarkov and Chebyshev Inequalities\n確率変数 \\(X\\) について，確率密度関数や分布関数がわかっている状況は少ないです．また，データが得られたとしても それらを計算することはかんたんではありません．その中で，\n\n\\(X\\) が mean \\(\\mu\\) からどれくらい離れる可能性があるのか\n\\(\\Pr(\\vert X \\leq a\\vert )\\) のupper boundはどれくらいか？\n\nという統計的推測をしたいときに使用されるMarkov and Chebyshev Inequalitiesを解説します．\n\n\nTheorem 4.18 Markov’s Inequality \nnon-negative 確率変数 \\(X \\geq 0\\)，constant \\(k &gt;0\\) について以下が成立する\n\\[\n\\Pr(X \\geq k) \\leq \\frac{\\mathbb E[X]}{k}\n\\]\nつまり，\n\\[\n\\Pr(X \\geq k\\mathbb E[X]) \\leq \\frac{1}{k}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\int_0^\\infty xf(x)\\mathrm{d}x\\\\\n             &= \\int_0^k xf(x)\\mathrm{d}x + \\int_k^\\infty xf(x)\\mathrm{d}x\\\\\n             &\\geq \\int_k^\\infty xf(x)\\mathrm{d}x\\\\\n             &\\geq \\int_k^\\infty kf(x)\\mathrm{d}x\\\\\n             &= k \\Pr(X \\geq k)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nProof: 変数変換\n\n\n\n\n\n\\[\nY =\n\\begin{cases}\n    0 & \\text{if} X &lt; k\\\\[5pt]\n    k & \\text{if} X \\geq k\n\\end{cases}\n\\]\nのように変数変換をすると常に \\(Y \\leq X\\) であるので \\(\\mathbb E[Y] \\leq \\mathbb E[X]\\).\n\\[\n\\begin{align*}\n&\\mathbb E[Y] = k\\Pr(X\\geq k)\\\\\n\\Rightarrow &\\Pr(X\\geq k)\\leq \\frac{\\mathbb E[X]}{k}\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 4.19 Markov’s Inequality with function \n確率変数 \\(X\\) の関数 \\(g(X)\\) が \\(g(X)\\geq 0\\) を満たすとする．任意の正の実数 \\(c\\in\\mathbb R_{++}\\) に対して\n\\[\n\\Pr(g(X)\\geq c) \\leq \\frac{\\mathbb E[g(X)]}{c}\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[g(X)]\n    &= \\mathbb E[g(X)(\\mathbb 1(g(X) \\geq c) + \\mathbb 1(g(X) &lt; c) )]\\\\\n    &\\geq \\mathbb E[g(X)\\mathbb 1(g(X) \\geq c)]\\\\\n    &\\geq c\\mathbb E[\\mathbb 1(g(X) \\geq c)]\\\\\n    &= c\\Pr(g(X)\\geq c)\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 4.20 \n確率変数 \\(X\\) の関数 \\(g(X)\\) が \\(g(X)\\geq 0\\) かつ \\(\\mathbb E[g(X)] = 0\\) を満たすとする． このとき，\n\\[\n\\Pr(g(X) = 0) = 1\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n背理法で示します．\\(\\Pr(g(X) = 0) &lt; 1\\) とすると，ある \\(c &gt;0\\) について \\(\\Pr(g(X) \\geq c) &gt; 0\\) となります．\nマルコフ不等式より，\n\\[\n\\Pr(g(X) \\geq c) \\leq \\frac{\\mathbb E[g(X)]}{c} = 0\n\\]\nとなるはずですが，これは \\(\\Pr(g(X) \\geq c) &gt; 0\\) に矛盾．従って，．\\(\\Pr(g(X) = 0) = 1\\) が成り立ちます．\n\n\n\n\n📘 REMARKS \n\nMarkov’s inequalityは 確率変数 \\(X\\) がnon-negative, population mean \\(\\mu\\) の知識のみで使用可能\n一方，bound幅は大きく，weakest inequalityである\n\n\n\nExample 4.7 \n点数範囲が \\(\\Omega_x=[0, 110]\\) の試験をついて，そのテストスコア確率変数 \\(X\\) を考える．分布の情報はわからないが population meanは 25 であることが知られている．このとき，\\(\\Pr(X \\geq 100)\\) のupper boundはMarkov’s inequalityを用いて 以下のように計算できます．\n\\(X\\) がnon-negativeなので\n\\[\n\\begin{align*}\n\\Pr(X\\geq 100) &\\leq \\frac{25}{100}\\\\\n               &= \\frac{1}{4}\n\\end{align*}\n\\]\n\n\nExample 4.8 : weak inequality \n\\(X_i \\overset{\\mathrm{iid}}{\\sim} \\operatorname{Bernoulli}(0.2)\\) を20回繰り返す試行を考える．この試行の結果のアウトカムを \\(Y\\) としたとき，\n\\[\n\\Pr(Y \\geq 16) = \\sum_{k=16}^{20} {}_{20}C_{k} 0.2^k 0.8^{20-k} \\approx 1.38\\cdot 10^{-8}\n\\]\n一方，Markov’s inequalityを用いると\n\\[\n\\begin{align*}\n\\Pr(Y \\geq 16) \\leq \\frac{4}{16} = \\frac{1}{4}\n\\end{align*}\n\\]\nこのように，bound幅は大きいことが分かる．\n\n\n\nTheorem 4.21 Chebyshev’s inequality \n\\(X \\sim D(\\mu, \\sigma^2)\\) とする．ただし，\\(D\\) の形状はわからない．実数 \\(\\alpha &gt;0\\) について，以下が成立する\n\\[\n\\Pr(\\vert X - \\mu \\vert \\geq \\alpha) \\leq \\frac{\\sigma^2}{\\alpha^2}\n\\]\nつまり，\n\\[\n\\Pr(\\vert X - \\mu \\vert \\geq \\alpha \\sigma) \\leq \\frac{1}{\\alpha^2}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(I = \\{x: \\vert x -\\mu \\vert \\geq k\\}\\) とする．\n\\[\n\\begin{align*}\n\\sigma^2 &= \\int_{\\mathbb R} (x - \\mu)^2f(x)\\mathrm{d}x\\\\\n         &\\geq  \\int_{I} (x - \\mu)^2f(x)\\mathrm{d}x\\\\\n         &\\geq  \\int_{I} k^2f(x)\\mathrm{d}x\\\\\n         &= k^2 \\Pr(\\vert x - \\mu\\vert \\geq k)\n\\end{align*}\n\\]\n以上より，\\(\\displaystyle\\Pr(\\vert X - \\mu \\vert \\geq k) \\leq \\frac{\\sigma^2}{k^2}\\) を得る．\n\n\n\n\n\n\n\n\n\nProof: using Markov’s inequality\n\n\n\n\n\n\\((x - \\mu)^2\\) を確率変数と考えると，non-negative確率変数になる，つまりMarkov’s inequalityを用いることができるので\n\\[\n\\begin{align*}\n\\Pr(\\vert x - \\mu\\vert \\geq k) &= \\Pr((x - \\mu)^2 \\geq k^2)\\\\\n                               &\\leq \\frac{\\mathbb E[(x - \\mu)^2]}{k^2} \\because{\\text{Markov's inequality}}\\\\\n                               &= \\frac{\\sigma^2}{k^2}\n\\end{align*}\n\\]\n\n\n\n\nExample 4.9 Markov’s inequality vs Chebyshev’s inequality \n\\(X \\sim \\operatorname{Binom}(n=20, p=0.2)\\) について，weak inequality で確認したように，Markov’s inequalityのより\n\\[\n\\Pr(X \\geq 16) = \\Pr(X \\geq 4\\mathbb E[X]) \\leq \\frac{1}{4}\n\\]\n一方，Chebyshev’s inequalityを用いると\n\\[\n\\begin{align*}\n\\Pr(X \\geq 16) &\\leq \\Pr(\\vert X - 4\\vert \\geq 12)\\\\\n               &\\leq \\frac{\\operatorname{Var}(X)}{12^2}\\\\\n               &\\leq \\frac{3.2}{12^2}\\\\\n               &= \\frac{1}{45}\n\\end{align*}\n\\]\n\n\n📘 REMARKS \n\nChebyshev’s inequalityはMarkov’s inqualityと異なり，確率変数 \\(X\\) がnon-negativeである必要はない\nmeanからの距離についての情報を得ることができる\n\n\n\n\nWeak Law of Large Numbers\n\n\nTheorem 4.22 Weak Law of Large Numbers \n平均 \\(\\mu\\), 分散 \\(\\sigma^2\\) の分布に独立に従う確率変数 \\(X_1, \\cdots, X_n\\) を考える．標本平均を \\(\\overline{X_n} = \\frac{1}{n}\\sum_{i=1}^nX_i\\) とする．\nこのとき，任意の実数 \\(\\epsilon &gt;0\\) に対して，\n\\[\n\\lim_{n\\to\\infty}\\Pr(\\vert \\overline{X_n} - \\mu \\vert &gt; \\epsilon) = 0\n\\]\nつまり，標本平均は母平均に確率収束する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nChebyshev’s inequalityを用いて以下のように示せる\n\\[\n\\begin{align*}\n\\lim_{n\\to\\infty}\\Pr(\\vert \\overline{X_n} - \\mu \\vert &gt; \\epsilon)\n                &\\leq \\lim_{n\\to\\infty} \\frac{\\operatorname{Var}(\\overline{X_n})}{\\epsilon^2}\\\\\n                &= \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n\\epsilon^2}\\\\\n                &=0\n\\end{align*}\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>期待値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/expectation.html#stein-identity",
    "href": "posts/statistics101/expectation.html#stein-identity",
    "title": "4  期待値",
    "section": "Stein identity",
    "text": "Stein identity\n\n\nTheorem 4.23 : スタインの等式 \n確率変数 \\(X\\sim N(\\mu, \\sigma^2)\\) とする．\\(g(\\cdot)\\) が微分可能で \\(\\mathbb E[\\vert g^\\prime(X)\\vert]&lt;\\infty\\) 及び \\(\\mathbb E[\\vert g(X)\\vert]&lt;\\infty\\) のとき，\n\\[\n\\mathbb E[(X-\\mu)g(X)] = \\sigma^2\\mathbb E[g^\\prime(X)]\n\\]\nが成り立つ．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\mathbb E[(X - \\mu)g(X)] = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int^\\infty_{-\\infty}g(x)(x-\\mu)\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,\\mathrm{d}x\n\\]\nここで，\n\\[\nh(x) = -\\sigma^2\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\nとおくと，\n\\[\nh^\\prime(x) =(x-\\mu)\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\n従って，\n\\[\n\\begin{align*}\n\\mathbb E[(X - \\mu)g(X)]\n    =& \\frac{1}{\\sqrt{2\\pi\\sigma^2}}[h(x)g(x)]^\\infty_{-\\infty} - \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int^\\infty_{-\\infty} g^\\prime(x)h(x)\\,\\mathrm{d}x\\\\\n    =& \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\left[-\\sigma^2\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)g(x)\\right]^\\infty_{-\\infty}\\\\\n     & + \\frac{\\sigma^2}{\\sqrt{2\\pi\\sigma^2}}\\int^\\infty_{-\\infty} g^\\prime(x)\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,\\mathrm{d}x\\\\\n    =& \\sigma^2\\mathbb E[g^\\prime(X)]\n\\end{align*}\n\\]\n\n\n\n ▶  スタインの等式とモーメント計算\n\\(m\\) 次のモーメントは\n\\[\n\\begin{align*}\n\\mathbb E[X^m]\n    &= \\mathbb E[(X - \\mu)X^{m-1} + \\mu X^{m-1}]\\\\\n    &= \\mathbb E[(X - \\mu)X^{m-1}] + \\mu \\mathbb E[X^{m-1}]\n\\end{align*}\n\\]\nここでスタインの等式を用いると\n\\[\n\\mathbb E[(X - \\mu)X^{m-1}] = \\sigma^2\\mathbb E[(m-1)X^{m-2}]\n\\]\n従って，\n\\[\n\\mathbb E[X^m] = \\sigma^2\\mathbb E[(m-1)X^{m-2}] + \\mu \\mathbb E[X^{m-1}]\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>期待値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/expectation.html#分散",
    "href": "posts/statistics101/expectation.html#分散",
    "title": "4  期待値",
    "section": "分散",
    "text": "分散\n\n\nTheorem 4.24 : Bienaymé Equality \n互いに独立な確率変数 \\(X, Y\\) について以下が成立する\n\\[\n\\operatorname{Var}(X+Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\operatorname{Var}(X+Y)\n    &= \\mathbb E[((X+Y) - (\\mu_X+\\mu_Y))^2]\\\\\n    &= \\mathbb E[((X- \\mu_X)+(Y - \\mu_Y))^2]\\\\\n    &= \\mathbb E[(X- \\mu_X)^2] + 2\\mathbb E[(X- \\mu_X)(Y- \\mu_Y)] + \\mathbb E[(Y- \\mu_Y)^2]\\\\\n    &= \\mathbb E[(X- \\mu_X)^2] + 2\\mathbb E[(X- \\mu_X)]\\mathbb E[(Y- \\mu_Y)] + \\mathbb E[(Y- \\mu_Y)^2] \\quad \\because{\\text{独立性}}\\\\\n    &= \\operatorname{Var}(X) + \\operatorname{Var}(Y)\n\\end{align*}\n\\]\n\n\n\nなお，確率変数 \\(X, Y\\) が独立ではない場合は\n\\[\n\\operatorname{Var}(X + Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) + 2\\operatorname{Cov}(X, Y)\n\\]\nが成立します．\\(\\mu_X = \\mathbb E[X], \\mu_Y = \\mathbb E[Y]\\) とすると，\n\\[\n\\begin{align*}\n\\operatorname{Var}(X+Y) &= \\mathbb E[(X+Y - \\mu_x - \\mu_y)^2]\\\\\n         &= \\mathbb E[(X-\\mu_x)^2] + 2\\mathbb E[(X-\\mu_x)(Y-\\mu_y)] + \\mathbb E[(Y-\\mu_y)^2]\\\\\n         &= \\operatorname{Var}(X) + \\operatorname{Var}(Y) + 2\\operatorname{Cov}(X,Y)\n\\end{align*}\n\\]\n\n\nTheorem 4.25 : n個の確率変数の和の分散 \n確率変数 \\(X_1, \\cdots, X_n\\), それぞれの期待値が存在し \\(\\mu_1, \\cdots, \\mu_n\\) と表せるとき，\n\\[\n\\operatorname{Var}(X_1+X_2 + \\cdots + X_n) = \\sum_i^n \\operatorname{Var}(X_i) + 2\\sum_{i&lt;j}\\operatorname{Cov}(X_i, X_j)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\operatorname{Var}(X_1+\\cdots+X_n)\n    &= \\mathbb E\\left[(X_1+\\cdots+X_n - \\mu_1 - \\cdots - \\mu_n)^2\\right]\\\\[5pt]\n    &= \\mathbb E\\left[\\{(X_1+\\mu_1)\\cdots + \\cdots + (X_n - \\mu_n)\\}^2\\right]\\\\[5pt]\n    &= \\mathbb E\\left[\\sum (X_i - \\mu_i)^2 + 2\\sum_{i&lt;j}(X_i - \\mu_i)(X_j-\\mu_j)\\right]\\\\[5pt]\n    &= \\sum \\mathbb E[(X_i - \\mu_i)^2] +2\\sum_{i&lt;j}\\mathbb E[(X_i - \\mu_i)(X_j-\\mu_j)]\\\\\n    &= \\sum_i^n \\operatorname{Var}(X_i) + 2\\sum_{i&lt;j}\\operatorname{Cov}(X_i, X_j)\n\\end{align*}\n\\]\n\n\n\n\n条件付き分散\n確率変数 \\(X, Y\\) についての条件付き分散は以下のような意味を持つ\n\n\\(\\operatorname{Var}(X\\vert Y=y)\\) は，\\(Y = y\\) と固定したときの \\(X\\) の分散\n\\(\\operatorname{Var}(X\\vert Y)\\) は，\\(Y\\) がランダムに選ばれた値に固定された場合の \\(X\\) の分散\n\n\\(\\operatorname{Var}(X\\vert Y)\\) は \\(Y\\) のランダムネスに依存した確率変数である一方， \\(\\operatorname{Var}(X\\vert Y=y)\\) は \\(y\\) の関数という違いがある\n\nDef: 条件付き分散 \n\\[\n\\operatorname{Var}(Y\\vert X) = \\mathbb E[(Y^2\\vert X)] - (\\mathbb E[(Y\\vert X)])^2 = \\mathbb E[(Y - \\mathbb E[Y\\vert X])^2\\vert X]\n\\]\n\n\n\n\nTheorem 4.26 Law of Total Variance \n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\mathbb E[Y\\vert X]) + \\mathbb E_X[\\operatorname{Var}(Y\\vert X)]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\epsilon = Y - \\mathbb E[Y\\vert X]\\) としたとき，\\(\\epsilon\\) と \\(\\mathbb E[Y\\vert X]\\) は無相関なので，\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\mathbb E[Y\\vert X]) + \\operatorname{Var}(\\epsilon)\n\\]\n\\(\\mathbb E[\\epsilon] = 0\\) より，\n\\[\n\\begin{align*}\n\\operatorname{Var}(\\epsilon)\n    &= \\mathbb E[\\epsilon^2] - (\\mathbb E[\\epsilon])^2\\\\\n    &= \\mathbb E[\\epsilon^2]\\\\\n    &= \\mathbb E_X(\\mathbb E[\\epsilon^2\\vert X])\\\\\n    &= \\mathbb E_X[\\operatorname{Var}(Y\\vert X)]\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\mathbb E[Y\\vert X]) + \\mathbb E_X[\\operatorname{Var}(Y\\vert X)]\n\\]\n\n\n\nLaw of Total Varianceより \\(Y\\) の分散は，CEFの分散 + 誤差項の分散に分解できることを示しています． 実務における分析において，賃金のバラツキを\n\n賃金を説明する各個人の特徴のバラツキ\n特徴で説明することのできない賃金のバラツキ(=誤差項)の期待値\n\nに分解して考察する際にLaw of Total Varianceを使用したりします．\n\n\n歪度と尖度\n\\(X_1\\sim N(1, 1)\\) と \\(X_2\\sim\\operatorname{Exponential}(1)\\) を２つの分布を考えます．どちらも平均，分散共に 1 で一致していますが以下のように分布の形状は大きく異なります．\n\n\nCode\nimport numpy as np\nimport plotly.express as px\nimport polars as pl\n\nnp.random.seed(42)\nn = 1000\n\nx1 = np.random.normal(loc=1, scale=1, size=n)\nx2 = np.random.exponential(scale=1, size=n)\n\ndf = pl.DataFrame({\"normal_dist\": x1, \"exp_dist\": x2})\n\npx.histogram(df, histnorm=\"probability density\",\n             opacity=0.8, barmode=\"overlay\",\n             title='Exp(1) vs Normal(1, 1): same mean and variance')\n\n\n                                                \n\n\n平均や分散(locationとscale)によって確率分布の様子はある程度わかりますが，locationとscaleが同じにもかかわらず上記の指数分布は \\(N(1, 1)\\) に対して，右の裾が長い分布になっています．分布の非対称性や尖りの程度を理解するにあたって尖度と歪度を用いることがあります．\n\nDef: 歪度(skewness)と尖度(kurtosis) \n確率変数 \\(X\\) について，歪度と尖度は以下のように定義される\n\\[\n\\begin{gather*}\n\\operatorname{skewness} = \\mathbb E\\left[\\left(\\frac{X - \\mathbb E[X]}{\\sqrt{\\operatorname{Var}(X)}}\\right)^3\\right]\\\\\n\\operatorname{kurtosis} = \\mathbb E\\left[\\left(\\frac{X - \\mathbb E[X]}{\\sqrt{\\operatorname{Var}(X)}}\\right)^4\\right]\n\\end{gather*}\n\\]\n\n\n📘 REMARKS \n\nskewnessはpositiveならば右の裾が長く，negativeならば左に裾が長い, 0ならば対称分布\nkurtosisは大きいほど，鋭いピークと長く太い裾をもった分布になる\nskewness,kurtosisともに標準化してから3rd-moment, 4th-momentを計算しているので，non-zeroの \\(a, b\\) を定数としたとき，\\(aX + b\\) と変数変換を行っても，計算結果は変わらない = 尺度の変換関して不変\n\n\n\nExample 4.10 : 一様分布の歪度と尖度 \n\\(X\\sim\\operatorname{Unif}(0, 1)\\) としたとき，一様分布はlocationから左右対称の分布なので計算することなく\n\\[\n\\operatorname{skewness} = 0\n\\]\nとわかる．一方，尖度は\n\\[\n\\begin{align*}\n\\operatorname{kurtosis}\n    &= \\frac{1}{\\sigma^4}\\int_0^1 \\left(x - \\frac{1}{2}\\right)^4\\mathrm{d}x\\\\\n    &= 144 \\times \\frac{1}{5}\\left[\\left(x - \\frac{1}{2}\\right)^5\\right]^1_0\\\\\n    &= \\frac{9}{5}\n\\end{align*}\n\\]\n標準正規分布の尖度を基準にして\n\\[\n\\operatorname{kurtosis} = \\frac{9}{5}-3=-\\frac{6}{5}\n\\]\nと表現する場合もある",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>期待値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/moment_generating_function.html",
    "href": "posts/statistics101/moment_generating_function.html",
    "title": "5  母関数",
    "section": "",
    "text": "確率母関数\n\\(p(x)\\geq 0, \\sum p(k)=1\\) より \\(\\vert s\\vert \\leq 1\\) の範囲で \\(\\sum_{k=0}^\\infty s^kp(k)\\) の収束が保証されるので， 「\\(\\vert s\\vert\\leq 1\\) となる \\(s\\) に対して」という条件が付きます．\n▶  PGFと離散確率関数\nPGFの定義より \\(p(0) = G_X(0), p(1) = G_X^\\prime(0), p(2) = \\frac{1}{2!}G_X^{\\prime\\prime}(0)\\) となります．一般に，\n\\[\np(k) = \\frac{1}{k!}\\frac{\\mathrm{d}^k}{\\mathrm{s}^k}G_X(s)\\bigg|_{s=0}\n\\]\nまた，\\(G_X(s) = \\sum_{k=0}^\\infty s^kp(k)\\) より，\n\\[\nG_X(1) = 1\n\\]\nになることも分かる．このように確率母関数が与えられれば，\\(G_X(s)\\) を \\(k\\) 回微分し，\\(s=0\\) と置くことで，\\(p(k)\\) を再現することができることから，非負整数上の確率分布とその確率母関数は１対１に対応していることがわかります．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>母関数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/moment_generating_function.html#確率母関数",
    "href": "posts/statistics101/moment_generating_function.html#確率母関数",
    "title": "5  母関数",
    "section": "",
    "text": "Def: 確率母関数(probability generating function) \n確率変数 \\(X\\) の標本空間を非負の整数全体 \\(\\mathcal{X} = \\{0, 1, 2, \\cdots\\}\\) とし， \\(p(k = \\Pr(X=k))\\) とする． \\(\\vert s\\vert\\leq 1\\) となる \\(s\\) に対して，\n\\[\nG_X(s) = \\mathbb E[s^x] = \\sum_{k=0}^\\infty s^kp(k)\n\\]\nを確率母関数という．\n\n\n\n\n\n\n\n\n\nExample 5.1 : PGFから確率関数を計算する \n確率変数 \\(X\\) について，PGFが \\(G_X(s) = \\frac{s}{5}(2 + 3s^2)\\) と与えられてるとします． このとき，\n\\[\n\\begin{align*}\nG_X(0) &= \\Pr(X=0) = 0\\\\\nG_X^\\prime(0) &= \\Pr(X=1) = \\frac{2}{5}\\\\\n\\frac{1}{2!}G_X^{\\prime\\prime}(0) &= \\Pr(X=2) = 0\\\\\n\\frac{1}{3!}G_X^{\\prime\\prime\\prime}(0) &= \\Pr(X=3) = \\frac{3}{5}\\\\\nG_X^{(r)}(0) &= 0 \\quad\\forall r\\geq 4\n\\end{align*}\n\\]\n従って，\n\\[\nX = \\bigg\\{\\begin{array}{c}\n    1 &\\text{with probability} \\frac{2}{5}\\\\\n    3 &\\text{with probability} \\frac{3}{5}\n\\end{array}\n\\]\nとなることがわかります\n\n\n\nTheorem 5.1 : PGFと階乗モーメント \n確率変数 \\(X\\) について，PGFが \\(G_X(s)\\) と与えられているとき，\n\n\\[\n\\begin{align*}\n&\\mathbb E[X] = G_X^\\prime(1)\\tag{a}\\\\\n&\\mathbb E[X(X-1)\\cdots(X-k)] = G_X^{(k)}(1)\\tag{b}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n ▶  (a)の証明\n\\[\n\\begin{align*}\nG^\\prime_X(s) = \\sum_{x=0}^\\infty x s^{x-1}p(x)\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\nG^\\prime_X(1)\n    &= \\sum_{x=0}^\\infty x p(x)\\\\\n    &= \\mathbb E[X]\n\\end{align*}\n\\]\n ▶  (b)の証明\n\\[\nG^{(k)}_X(s) = \\sum_{x=0}^\\infty x(x-1)\\cdots(x-k) s^{x-k}p(x)\n\\]\n従って，\n\\[\nG^{(k)}_X(1) = \\mathbb E[X(X-1)\\cdots(X-k)]\n\\]\n\n\n\n\n\nTheorem 5.2 : 独立な確率変数の和についてのPGF \n互いに独立な確率変数 \\(X_1, \\cdots, X_n\\) について，\\(Y = \\sum_{i=1}^nX_i\\) と定義する．このとき，\n\\[\nG_Y(s) = \\prod_{i=1}^nG_{X_i}(s)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nG_Y(s)\n    &= \\mathbb E[s^{(X_1+\\cdots+X_n)}]\\\\\n    &= \\mathbb E[s^{X_1}\\cdots s^{X_n}]\\\\\n    &= \\mathbb E[s^{X_1}]\\cdots \\mathbb E[s^{X_n}] \\because\\text{ 互いに独立}\\\\\n    &=\\prod_{i=1}^nG_{X_i}(s)\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 5.3 \n確率変数列 \\(X_1, X_2, \\cdots \\overset{\\mathrm{iid}}{\\sim}  F\\) とし，各確率変数 \\(X_i\\) の確率母関数を \\(G_X(s)\\) で表すとする． \\(\\{X_i\\}_{i=1}\\) と互いに独立な確率変数 \\(N\\) を考え，新たに以下の形で確率変数 \\(T_N\\) を定義する:\n\\[\nT_N = X_1 + \\cdots + X_N\n\\]\nこのとき，\\(T_N\\) についての確率母関数は\n\\[\nG_{T_N}(s) = G_N(G_X(s))\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nG_{T_N}(s)\n&= \\mathbb{E}(s^{T_N})\\\\\n&= \\mathbb{E}\\left( s^{X_1 + \\cdots + X_N} \\right)\\\\\n&= \\mathbb{E}_N \\left\\{ \\mathbb{E} \\left( s^{X_1 + \\cdots + X_N} \\mid N \\right) \\right\\} \\quad \\text{(conditional expectation)}\\\\\n&= \\mathbb{E}_N \\left\\{ \\mathbb{E} \\left( s^{X_1} \\cdots s^{X_N} \\mid N \\right) \\right\\}\\\\\n&= \\mathbb{E}_N \\left\\{ \\mathbb{E} \\left( s^{X_1} \\cdots s^{X_N} \\right) \\right\\} \\quad (X_i \\text{と} N \\text{は互いに独立})\\\\\n&= \\mathbb{E}_N \\left\\{ \\mathbb{E} \\left( s^{X_1} \\right) \\cdots \\mathbb{E} \\left( s^{X_N} \\right) \\right\\} \\quad (X_i \\text{は互いに独立})\\\\\n&= \\mathbb{E}_N \\left\\{ \\left( G_X(s) \\right)^N \\right\\}\\\\\n&= G_N\\left( G_X(s) \\right) \\quad (G_N\\text{の定義より})\n\\end{align*}\n\\]\n\n\n\n\nExample 5.2 : ATMの引き出し金額 \nとあるATMでお金を引き出す人の一日あたりの合計人数 \\(N\\) という確率変数を考える． ATMで各個人が引き出す金額 \\(X_i\\) は 互いに独立に 平均 \\(\\mu\\), 分散 \\(\\sigma^2\\), PGF \\(G_X(s)\\) の同一分布に従うとします．\nこのとき，一日あたりのATM引き出し金額 \\(T_N = X_1 + \\cdots + X_N\\) について，\n\\[\n\\begin{align*}\n\\mathbb E[T_n]\n    &= G_N^\\prime(G_X(1))\\cdot G_X^\\prime(1)\\\\\n    &= G_N^\\prime(1) \\cdot \\mathbb E[X_i] \\quad\\because G_X(1) = 1\\\\\n    &= \\mathbb E[N] \\cdot \\mathbb E[X_i]\n\\end{align*}\n\\]\nと期待値を表すことができる．\n\n\n📘 REMARKS \n積率母関数と確率母関数について以下の関係が知られています:\n\\[\n\\begin{align*}\nG_X(t) &= M_X(\\log(t))\\\\\nM_X(t) &= G_X(\\exp(t))\n\\end{align*}\n\\]\n以上より，一方が求まっていれば，それをもとにもう一方を求めることができることがわかります．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>母関数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/order_statistic.html",
    "href": "posts/statistics101/order_statistic.html",
    "title": "6  順序統計量",
    "section": "",
    "text": "順序統計量\n▶  \\(X_{(i)}\\) の累積密度関数と確率密度関数\n\\(F\\) を密度関数 \\(f\\) を持つ連続分布として，\\(X_{(i)} \\leq x\\) となる事象を考える．この事象は \\(X_1, \\cdots, X_n\\) のなかで \\(x\\) 以下となるものの個数が \\(i\\) 個以上であるという事象と同地なので\n\\[\nB_k = \\{X_1, \\cdots, X_n\\text{のうち}k\\text{個が}x\\text{以下}\\}\n\\]\nと事象 \\(B_k\\) を設定すると，\n\\[\n\\Pr(X_{(i)}\\leq x) = \\sum_{k=i}^n \\Pr(B_k)\n\\]\nそれぞれの \\(X_j\\) について，独立に成功確率 \\(p = F(x)\\) のベルヌーイ試行と考えることができるので\n\\[\nF_{X_{(i)}}(X) = \\sum_{k=i}^n {}_nC_{k} p^k (1 - p)^{n-k}, \\ \\ p=F(x)\n\\]\nとなることがわかります．また，この式を \\(x\\) で微分することで \\(X_{(i)}\\) の確率密度関数がわかるので, \\(p(k, m)\\) を \\(\\operatorname{Binom}(m, p)\\) の確率関数とすると\n\\[\n\\begin{align*}\n&\\frac{\\mathrm{d}}{\\mathrm{d}p} {}_nC_{k} p^k (1 - p)^{n-k}\\\\\n&= \\frac{n!}{(k-1)!(n-k)!}p^{k-1}(1-p)^{n-k} - \\frac{n!}{k!(n-k-1)!}p^{k-1}(1-p)^{n-k-1}\\\\\n&= n (p(k-1, n-1) - p(k. n-1))\n\\end{align*}\n\\]\n従って，\\(p(n, n-1) =0\\) とすると，\n\\[\n\\begin{align*}\nf_{X_{(i)}}(x) &= nf(x)\\sum_{k=i}^n(p(k-1, n-1) - p(k. n-1))\\\\\n               &= nf(x) p(i-1, n-1)\\\\\n               &= \\frac{n!}{(i-1)!(n-i)!}f(x)F(x)^{i-1}(1 - F(x))^{n-i}\n\\end{align*}\n\\]\n▶  最大値の分布関数\n最大値の分布関数 \\(\\Pr(X_{(n)} \\leq x)\\) は\n\\[\n\\max_i X_i \\leq x \\Leftrightarrow X_i \\leq x, \\  \\ i = 1, \\cdots, n\n\\]\nなので，\n\\[\n\\begin{align*}\n&\\Pr(\\max_i X_i \\leq x )= F(x)^n\\\\\n&f_{X_{(n)}}(x) = nf(x)F(x)^{n-1}\n\\end{align*}\n\\]\n▶  最小値の分布関数\n最小値の分布関数 \\(\\Pr(X_{(1)} \\leq x) = \\Pr(\\min(X_i)\\leq x)\\) は\n\\[\n\\min_i X_i &gt; x \\Leftrightarrow X_i &gt; x, \\  \\ i = 1, \\cdots, n\n\\]\nより\n\\[\n\\begin{align*}\n&\\Pr(\\min_i X_i \\leq x )= 1 - (1 - F(x))^n\\\\\n&f_{X_{(1)}}(x) = nf(x)(1 - F(x))^{n-1}\n\\end{align*}\n\\]\nCode\nimport numpy as np\nfrom scipy.stats import beta\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nN = 10\nITER = 10000\nI = 9\nNBINS = 100\na, b = 10, 1\nnp.random.seed(42)\n\n\ndef random_sampling_from_unif(index, size):\n    return sorted(np.random.uniform(0, 1, size))[index]\n\n\nx = np.array(list(map(lambda x: random_sampling_from_unif(I, N), range(ITER))))\nbeta_x = np.linspace(beta.ppf(0.01, a, b), beta.ppf(0.99, a, b), NBINS)\n\n# plot\nnewnames = {\"0\": \"sample maximum\"}\nfig = px.histogram(x, histnorm=\"probability density\", nbins=NBINS, title=\"maximum value distribution of 10 rvs from Unif(0, 1)\")\nfig.for_each_trace(\n    lambda t: t.update(\n        name=newnames[t.name],\n        hovertemplate=t.hovertemplate.replace(t.name, newnames[t.name]),\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=beta_x, y=beta.pdf(beta_x, a, b), mode=\"lines\", name=\"beta(10, 1) pdf\"\n    ),\n    row=1,\n    col=1,\n)\nfig.show()",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>順序統計量</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/order_statistic.html#順序統計量",
    "href": "posts/statistics101/order_statistic.html#順序統計量",
    "title": "6  順序統計量",
    "section": "",
    "text": "Def: 順序統計量 \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} F\\) とするとき，これkらの確率変数の値を小さい順に並び替えたものを\n\\[\nX_{(1)}\\leq\\cdots\\leq X_{(i)}\\leq\\cdots \\leq X_{(n)}\n\\]\nと表し，順序統計量(order statistic)という．\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.1 一様分布に従う確率変数列の順序統計量とベータ分布 \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} \\operatorname{Unif}(0, 1)\\) とするとき，第 \\(i\\) 順序統計量の確率密度関数と分布関数はそれぞれ以下のようになる\n\\[\n\\begin{align*}\nf_{X_{(i)}} &= \\frac{n!}{(i-1)!(n-i)!} x^{i-1}(1 - x)^{n-i}\\\\\n            &= \\frac{x^{i-1}(1 - x)^{n-i}}{\\operatorname{B}(i, n-i+1)}\\\\\nF_{X_{(i)}} &= \\sum_{k=i}^n \\frac{n!}{(n-k)!k!} x^k (1 - x)^{n-k}\\\\\n            &= \\frac{n!}{(i-1)!(n-i)!}\\int^x_0t^{i-1}(1 - t)^{n-i}\\mathrm{d}t\n\\end{align*}\n\\]\n従って，\\(X_{(i)}\\) はベータ分布 \\(\\operatorname{Beta}(i, n-i+1)\\) に従うことがわかる．ここから \\(\\operatorname{Beta}(1, 1)\\) が \\(\\operatorname{Unif}(0, 1)\\) に等しくなることもわかる．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>順序統計量</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/order_statistic.html#最大値と最小値の同時密度関数",
    "href": "posts/statistics101/order_statistic.html#最大値と最小値の同時密度関数",
    "title": "6  順序統計量",
    "section": "最大値と最小値の同時密度関数",
    "text": "最大値と最小値の同時密度関数\n\n\nTheorem 6.1 最大値と最小値の同時密度関数 \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} F\\) とするとき，\\(x&lt;y\\) とすると\n\\[\n\\begin{align*}\nF_{X_{(1)},X_{(n)}}(x, y) &= \\Pr(X_{(1)} \\leq x,X_{(n)}\\leq y)\\\\\n                          &= [F(y)]^n - [F(y) - F(x)]^n\n\\end{align*}\n\\]\n同時確率密度関数は\n\\[\nf_{X_{(1)},X_{(n)}}(x, y) = n(n-1)[F(y) - F(x)]^{n-2}f(x)f(y)\n\\]\n\n\n\\(x &lt; y\\) の条件のもとで，\n\\[\nF_{X_{(1)},X_{(n)}}(x, y) = \\Pr(X_{(1)} \\leq x,X_{(n)}\\leq y)\n\\]\nについてまず考える．確率事象の排他性より\n\\[\n\\Pr(X_{(n)} \\leq y) = \\Pr(X_{(1)}\\leq x, X_{(n)}\\leq y) + \\Pr(X_{(1)}&gt; x, X_{(n)}\\leq y)\n\\]\nここから，\\(\\Pr(X_{(1)}\\leq x, X_{(n)}\\leq y) = \\Pr(X_{(n)} \\leq y) - \\Pr(X_{(1)}&gt; x, X_{(n)}\\leq y)\\) を得る．\n\\[\n\\begin{align*}\n&\\Pr(X_{(1)}&gt; x, X_{(n)}\\leq y)\\\\\n&= \\Pr(x &lt; X_{(1)} \\leq y, \\cdots, x &lt; X_{(i)} \\leq y, \\cdots, x&lt; X_{(n)}\\leq y)\\\\\n&= [F(y) - F(x)]^n \\because{\\operatorname{i.i.d}}\n\\end{align*}\n\\]\n\\(\\Pr(X_{(n)}\\leq y) = [F(y)]^n\\) であるのは上で確認したので，従って，\n\\[\n\\begin{align*}\nF_{X_{(1)},X_{(n)}}(x, y) &= \\Pr(X_{(1)} \\leq x,X_{(n)}\\leq y)\\\\\n                          &= [F(y)]^n - [F(y) - F(x)]^n\n\\end{align*}\n\\]\n同時確率密度関数は\n\\[\n\\begin{align*}\nf_{X_{(1)},X_{(n)}}(x, y) &= \\frac{\\mathrm{d}}{\\mathrm{d}x}\\frac{\\mathrm{d}}{\\mathrm{d}x}\\{[F(y)]^n - [F(y) - F(x)]^n\\}\\\\\n&= \\frac{\\mathrm{d}}{\\mathrm{d}x}\\{n[F_y]^{n-1}f(y) - n[F(y) - F(x)]^{n-1}f(y)\\}\\\\\n&= n(n-1)[F(y) - F(x)]^{n-2}f(y)f(x)\n\\end{align*}\n\\]\n ▶  A heuristic approach for calculating the joint pdf\n\\(\\min X_i = x, \\max X_i =y\\), と決まっており, それ以外の値は \\((x, y)\\) 区間に収まっていれば何でも良いので， index通りに順番があるならば\n\\[\nf(x)[F(y) - F(x)]^{n-2}f(y)\n\\]\nまた，この並べ方は \\(n!\\) 通り存在するが，\\(X_{(1)}, X_{(n)}\\) 以外の並び方には区別はいらないので\n\\[\n\\frac{n!}{(n-2)!}f(x)[F(y) - F(x)]^{n-2}f(y) = n(n-1)f(x)[F(y) - F(x)]^{n-2}f(y)\n\\]\n\n📘 REMARKS \n並び替えの考えで同様に \\((X_{(1)}, \\cdots, X_{(n)})\\) の同時確率密度関数は\n\\[\nf_{X_{(1)}, \\cdots, X_{(n)}}(x_1, \\cdots, x_n) = n!f(x_1)f(x_2)\\cdots f(x_n)\n\\]\nであることがわかる．\n\\(X_{(i)}, X_{(j)}\\) の同時確率密度関数は\n\\[\nf_{X_{(i)}, X_{(j)}}(x, y)=\\frac{n!}{(i-1)!(j-i-1)!}(n-j)!f(x)f(y)F(x)^{i-1}[F(y)-F(x)]^{j-i-1}[1 - F(y)]^{n-j}\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>順序統計量</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/delta-method.html",
    "href": "posts/statistics101/delta-method.html",
    "title": "7  デルタ法",
    "section": "",
    "text": "Theorem 7.1 : スラツキーの定理 \n\\(U_n \\overset{\\mathrm{d}}{\\to} U, V_n\\overset{\\mathrm{p}}{\\to} a\\) とする．このとき，次の性質が成立する：\n\\[\n\\begin{align*}\nU_n + V_n &\\overset{\\mathrm{d}}{\\to}U + a\\\\\nU_nV_n&\\overset{\\mathrm{d}}{\\to}aU\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n確率収束は分布収束を意味するので\n\\[\n[U_n\\quad V_n] \\overset{\\mathrm{d}}{\\to} [U\\quad  a]\n\\]\n\\(g(x , y) = x +y, h(x, y) = xy\\) と見たとき，どちらも連続関数であるので連続写像定理により\n\\[\n\\begin{align*}\ng(U_n, V_n) \\overset{\\mathrm{d}}{\\to} g(U, a)\\\\\nh(U_n, V_n) \\overset{\\mathrm{d}}{\\to} h(U, a)\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\nU_n + V_n &\\overset{\\mathrm{d}}{\\to}U + a\\\\\nU_nV_n&\\overset{\\mathrm{d}}{\\to}aU\n\\end{align*}\n\\]\n\n\n\n ▶  スラツキー定理の確認\n\\(X\\sim\\operatorname{Po}(3)\\) のサンプルサイズを \\(N \\in \\{10, 20, 500\\}\\) に設定し\n\\[\n\\begin{align*}\nX_n &= \\sqrt{N}\\frac{\\overline X -\\lambda}{\\sqrt{\\lambda}}\\\\\nA_n &= 2 + \\exp(-N)\n\\end{align*}\n\\]\nとしたとき，\\(X_n\\) はCLTにより \\(X_n \\overset{\\mathrm{d}}{\\to} N(0, 1)\\), \\(A_n \\to 2\\) が成り立つので\n\\[\n\\begin{align*}\nA_NX_n \\overset{\\mathrm{d}}{\\to} 2\\cdot N(0, 1)\n\\end{align*}\n\\]\nとなるはずです．各レベルの \\(X_n\\) について 1000回ずつシミュレーションを行い，density plotを以下確認してみました． 期待することとしては，\\(N\\) が大きくなるにつれて，\\(N(0, 4)\\) の分布に近づいていくことです．\n\n\nCode\nimport numpy as np\nfrom scipy.stats import poisson, norm\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\n\nnp.random.seed(42)\n\n# ------------------------\n# simulations\n# ------------------------\nN = np.array([10, 20, 500])\nmu = 3\n\nX_n = list(map(lambda x: np.mean(poisson.rvs(mu=mu, size=[x, 1000]), axis=0), N))\nA_n = 2 + np.exp(-N / 20)\n\nfor i in range(0, 3):\n    tmp = A_n[i] * np.sqrt(N[i]) * (X_n[i] - mu) / np.sqrt(mu)\n    exec(f\"b{i} = tmp\")\n\nx_range = np.linspace(-8, 8, 1000)\nnormal_density = norm(0, 2).pdf(x_range)\n\n# ------------------------\n# visualization\n# ------------------------\nhist_data = [b0, b1, b2]\n\ngroup_labels = [\"N=10\", \"N=20\", \"N=500\"]\ncolors = [\"#6BAED6\", \"#2171B5\", \"#08306b\"]\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot(\n    hist_data, group_labels, show_hist=False, colors=colors, show_rug=False\n)\nfig.add_traces(\n    go.Scatter(\n        x=x_range,\n        y=normal_density,\n        mode=\"lines\",\n        name=\"N(0,4)\",\n        line=dict(color=\"gray\", width=2, dash=\"dashdot\"),\n    )\n)\n# Add title\nfig.update_layout(\n    title_text=\"Check Slutsky’s Theorem\",\n    xaxis_title=dict(text=\"A_nX_n\"),\n    yaxis_title=dict(text=\"density\"),\n)\nfig.show()\n\n\n                                                \n\n\n\nExample 7.1 : t統計量の正規性 \n次のようにt統計量を定義します\n\\[\n\\begin{gather}\nt_n = \\frac{\\sqrt{n}(\\bar{X}_n-\\mu)}{\\sqrt{\\hat{\\sigma^2}}}\\\\\n\\text{where } \\hat{\\sigma^2} = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X_n})^2.\n\\end{gather}\n\\]\nCLTにより\n\\[\n\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0,\\sigma^2)\n\\]\nまた，\\(\\hat\\sigma^2\\xrightarrow{p}\\sigma^2\\) であり，\\(\\sigma &gt; 0\\)より連続写像定理より\n\\[\n\\frac{1}{\\hat{\\sigma^2}} \\xrightarrow{p} \\frac{1}{\\sigma^2}\n\\]\nここでスラツキー定理より\n\\[\n\\begin{align}\n\\sqrt{n}(\\bar{X}_n-\\mu) \\times \\frac{1}{\\sqrt{\\hat\\sigma^2}} &\\xrightarrow{d} N(0,\\sigma^2) \\times \\frac{1}{\\sqrt{\\sigma^2}} \\\\\n&= \\sigma N(0,1) \\times \\frac{1}{\\sigma}\\\\\n&= N(0,1)\n\\end{align}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\nDef: Delta Method \n確率変数 \\(X_n\\) について\n\\[\n\\frac{\\sqrt{n}(X_n-\\mu)}{\\sigma}\\overset{\\mathrm{d}}{\\sim} N(0, 1)\n\\]\nが成立しているとします．関数 \\(g(\\cdot)\\) について，その微分導関数 \\(g^\\prime(\\cdot)\\) が連続で \\(g^\\prime(\\mu)\\neq 0\\) であるとき， 次の分布収束が成り立つ\n\\[\n\\frac{\\sqrt{n}(g(X_n)-g(\\mu))}{\\sigma}\\overset{\\mathrm{d}}{\\sim} N(0, [g\\prime(\\mu)]^2)\n\\]\n\n\n\n\n\n\n\nProof: テイラー展開\n\n\n\n\n\n\\(g(X_n)\\) の \\(X_n=\\mu\\) 周りでのテイラー展開から\n\\[\ng(X_n) \\approx g(\\mu) + g^\\prime(\\mu^*)(X_n - \\mu)\n\\]\n\\(\\mu^*\\) は \\(\\vert \\mu^* - \\mu\\vert &lt; \\vert X_n - \\mu \\vert\\) を満たす点となります． スラツキーの定理より\n\\[\nX_n - \\mu = \\frac{1}{\\sqrt{n}}[\\sqrt{n}(X_n-\\mu)]\\overset{\\mathrm{d}}{\\to} 0\n\\]\n従って，\n\\[\n\\begin{align*}\nX_n &\\overset{\\mathrm{p}}{\\to}  \\mu\\\\\n\\mu^* &\\overset{\\mathrm{p}}{\\to}  \\mu\n\\end{align*}\n\\]\n以上より，\\(g^\\prime(\\cdot)\\) の連続性より \\(g^\\prime(\\mu^*)\\overset{\\mathrm{p}}{\\to} g^\\prime(\\mu)\\). 従って，スラツキーの定理より\n\\[\n\\sqrt{n}\\{g(X_n) - g(\\mu)\\} = \\sqrt{n}[g^\\prime(\\mu^*)(X_n - \\mu)]\\overset{\\mathrm{d}}{\\to} g^\\prime(\\mu)N(0, \\sigma^2)\n\\]\n\n\n\n\nExample 7.2 : logit \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} \\operatorname{Bernoulli}(p)\\) のとき，\n\\[\n\\begin{align*}\nX_n &= \\frac{1}{n}\\sum_[i=1]^n X_i\\\\\n\\sqrt{n}(X_n - p)&\\overset{\\mathrm{d}}{\\to}N(0, p(1-p))\n\\end{align*}\n\\]\nが成立します．このとき，ロジット関数\n\\[\nh(x) = \\log\\left(\\frac{x}{1 - x}\\right)\n\\]\nを考えます．\\(h^\\prime(x) = \\frac{1}{x(1-x)}\\) と \\(x\\in (0,1)\\) で連続関数であるので，デルタ法より\n\\[\n\\sqrt{n}\\left\\{\\log\\left(\\frac{X_n}{1 - X_n}\\right) - \\log\\left(\\frac{p}{1 - p}\\right) \\right\\}\\overset{\\mathrm{d}}{\\to} N\\left(0, \\frac{p}{1-p}\\right)\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\nExample 7.3 : 指数分布の分散パラメーターの漸近分布 \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} \\operatorname{Exp}(\\lambda)\\) とします． ただし，確率密度関数を\n\\[\nf(x) = \\lambda\\exp(-x\\lambda)\\quad (x &gt;0)\n\\]\nと表す．\nこのとき，\\(\\operatorname{Var}(X_i) = \\theta\\) の推定はMLEより\n\\[\n\\begin{gather}\n\\sum \\frac{1}{\\hat\\lambda} - \\sum X_i = 0\\\\\n\\Rightarrow\\hat\\lambda = \\left(\\frac{1}{n}\\sum X_i\\right)\\\\\n\\Rightarrow\\hat\\theta = \\frac{1}{\\hat\\lambda^2} = \\left(\\frac{1}{n}\\sum X_i\\right)^2\\\\\n\\end{gather}\n\\]\nここで \\(\\hat\\theta\\) の漸近分布は, CLTより\n\\[\n\\sqrt{n}\\left(\\frac{1}{n}\\sum X_i - \\frac{1}{\\lambda}\\right)\\overset{\\mathrm{d}}{\\to} N(0, \\lambda^{-2})\n\\]\nより，\\(g(x) = x^2\\) とおくと \\(g^\\prime(x) = 2x\\) は連続かつ \\(\\lambda \\in (0, \\infty)\\) であるので\n\\[\n\\begin{align*}\n\\sqrt{n}(\\hat\\theta - \\theta)\\overset{\\mathrm{d}}{\\to} N(0, 4\\lambda^{-4})\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>デルタ法</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/discrete_uniform_dist.html",
    "href": "posts/probability_distribution/discrete_uniform_dist.html",
    "title": "8  離散一様分布",
    "section": "",
    "text": "離散一様分布の性質\nフェアなサイコロをふったときに出る目を確率変数 \\(X\\) とみなしたとき，\\(X\\sim \\operatorname{DU}(6)\\) となります．\n離散一様分布のQuantile functionは，\n\\[\nQ_X(u) = \\inf\\{x\\in\\mathbb R: F_X(x) \\geq u\\}\n\\]\nなので，\\(u\\in(0, 1]\\) について\n\\[\nQ_X(u) = \\lceil np\\rceil\n\\]\nと定義されます．\n▶  確率母関数を用いたモーメントの計算\n\\[\n(1-s^n) = (1 - s)(1 + s + s^2 + \\cdots + s^{n-1})\n\\]\nより\n\\[\nG_X(s) = \\frac{s + s^2 + \\cdots + s^n}{n}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\mathbb E[X] &= G_X^\\prime(s)\\bigg\\vert_{s=0} \\\\\n             &= \\frac{n+1}{2}\\\\\n\\mathbb E[X(X-1)]\n    &= G_X^{\\prime\\prime}(s)\\bigg\\vert_{s=0}\\\\\n    &= \\frac{1}{n}\\sum_{k=1}^nk(k-1)\\\\\n    &= \\frac{(n+1)(2n+1)}{6} - \\frac{n+1}{2}\n\\end{align*}\n\\]\n▶  特性関数\n特性関数は\n\\[\n\\varphi_X(t) = \\frac{\\exp(it) - \\exp(it(n+1))}{n(1 - \\exp(it))}\n\\]\n▶  歪度\n\\[\n\\mathbb E[(X - \\mathbb E[X])^3] = (\\mathbb E[X^3] – 3\\mathbb E[X^2]E[X] + 2\\mathbb E[X]^3)\n\\]\nなので，これをまず計算します．\n\\[\n\\mathbb E[X^3] = M_X^{\\prime\\prime\\prime}(t) \\bigg\\vert_{t=0} = n\\left(\\frac{n+1}{2}\\right)^2\n\\]\nこれを用いて計算すると，\\(n - 2n-1 + n+1 = 0\\) より \\(\\operatorname{skewness} = 0\\)． 平均を中心に対称な分布なので，計算しなくても歪度は0であると判断することもできます．\n▶  エントロピー\n離散確率変数 \\(X\\) の確率関数を \\(p(x)\\) とします．このとき，エントロピーを以下のように定義します．\n\\[\n\\mathrm{H}(X) = -\\sum_x p(x) \\log(p(x)) \\qquad (\\lim_{p\\to 0}p\\log(p) = 0 \\text{とする})\n\\]\n\\(X\\sim \\operatorname{DU}(n)\\) としたとき，エントロピーは\n\\[\n\\begin{align*}\n\\mathrm{H}(X) = -\\frac{1}{n}\\sum_x \\log(\\frac{1}{n}) = \\log(n)\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>離散一様分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/discrete_uniform_dist.html#離散一様分布の性質",
    "href": "posts/probability_distribution/discrete_uniform_dist.html#離散一様分布の性質",
    "title": "8  離散一様分布",
    "section": "",
    "text": "Def: 離散一様分布 \n\\(n\\in\\mathbb N\\) に対して，確率変数 \\(X\\) が \\(S = \\{1, 2, \\cdots, n\\}\\) を標本空間し，\n\\[\n\\Pr(X = k) = \\frac{1}{n}\\quad k\\in S\n\\]\nを満たすとき，\\(X\\) は離散一様分布に従うという．(\\(X\\sim \\operatorname{DU}(n)\\))\n\n\n\n\n\n\n\n\nProperty: 累積分布関数\n\n\n\n離散一様分布の累積分布関数(分布関数)は\n\\[\nF(x) = \\Pr(X\\leq x) = \\left\\{\\begin{array}{c}\n0 & x &lt; 1\\\\\n\\frac{\\lfloor x\\rfloor}{n} & 1\\leq 1 \\leq n\\\\\n1 & x \\geq n\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\n\n\n\nTheorem 8.1 : 期待値と分散 \n離散一様分布に従う確率変数 \\(X\\sim \\operatorname{DU}(n)\\) について，\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\frac{n+1}{2}\\\\\n\\operatorname{Var}(X) &= \\frac{n^2-1}{2}\\\\\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\sum_{k=1}^nk\\frac{1}{n} = \\frac{n(n+1)}{2n}=\\frac{n+1}{2}\\\\\n\\end{align*}\n\\]\n分散については\n\\[\n\\begin{align*}\n\\mathbb E[X^2] &= \\sum_{k=1}^nk^2\\frac{1}{n} = \\frac{(n+1)(2n+1)}{6}\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(X) = \\frac{n^2-1}{12}\n\\]\n\n\n\n\n\nTheorem 8.2 : 確率母関数 \n\\(X\\sim \\operatorname{DU}(n)\\) について，確率母関数 \\(G_X(s)\\) は\n\\[\nG_X(s) = \\frac{s(1 - s^n)}{n(1-s)}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nG_X(s) &= \\mathbb E[s^x]\\\\\n       &= \\sum_{k=1}^n \\frac{s^x}{n}\\\\\n       &= \\frac{s(1-s^n)}{n(1-s)}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 8.3 \n\\(X\\sim \\operatorname{DU}(n)\\) について，MGF \\(M_X(t)\\) は\n\\[\nM_X(t) = \\frac{\\exp(t) - \\exp(t(n+1))}{n(1 - \\exp(t))}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\mathbb E[\\exp(tX)] = \\frac{1}{n}\\sum_{k=1}^n \\exp(tk) = \\frac{\\exp(t) - \\exp(t(n+1))}{n(1 - \\exp(t))}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>離散一様分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/discrete_uniform_dist.html#k-個のサイコロの目の合計",
    "href": "posts/probability_distribution/discrete_uniform_dist.html#k-個のサイコロの目の合計",
    "title": "8  離散一様分布",
    "section": "\\(k\\) 個のサイコロの目の合計",
    "text": "\\(k\\) 個のサイコロの目の合計\n\\(k\\) 個のフェアなサイコロの目の合計の確率分布はモーメント母関数を用いて導出することができるのでこのセクションで紹介します．\n\nDef: 離散確率変数のMGF \n確率変数 \\(X\\) が離散確率変数 \\(\\Pr(X = x_i) = p_i, i = 1, \\cdots, k\\) の場合，MGFは以下のように表現される：\n\\[\n\\begin{align*}\nM_X(t)\n    &= \\mathbb E[\\exp(tX)]\\\\\n    &= p_1\\exp(tx_1) + p_2\\exp(tx_2) + \\cdots + p_k\\exp(tx_k)\n\\end{align*}\n\\]\n\n上記の定義より，フェアなサイコロのMGFは以下のようになります\n\\[\nM_X(t) = \\frac{1}{6}\\left(\\exp(t) + \\exp(2t) + \\cdots + \\exp(6t)\\right)\n\\]\n\n\nTheorem 8.4 : 互いに独立な確率変数の和とMGF \n互いに独立な確率変数 \\(X, Y\\) について，\\(Z = X +Y\\) と確率変数を定義したとき，\n\\[\nM_Z(t) = M_X(t)M_Y(t)\n\\]\nが成立する・\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nM_Z(t)\n    &= \\mathbb E[\\exp(tZ)]\\\\\n    &= \\mathbb E[\\exp(tX + tY)]\\\\\n    &= \\mathbb E[\\exp(tX)\\exp(tY)]\\\\\n    &= \\mathbb E[\\exp(tX)]\\mathbb E[\\exp(tY)]\\\\\n    &= M_X(t)M_Y(t)\n\\end{align*}\n\\]\n\n\n\n\n\\(k\\) 個のサイコロの目の合計の確率分布\n\\(k\\) 個のフェアなサイコロを独立に投げ，それぞれの目を \\(X_1, \\cdots, X_k\\) としたとき，サイコロの目の合計 \\(Y\\) は\n\\[\nY = X_1 + \\cdots + X_k\n\\]\nこのとき，上で確認したMGFの性質より\n\\[\nM_Y(t) = \\frac{1}{6}(\\exp(t)+\\exp(2t)+\\cdots + \\exp(6t))^k\n\\]\n ▶  \\(k=2\\) の場合\n\\[\n\\begin{align*}\nM_Y(t)\n    =& \\frac{1}{6}(\\exp(2t) + 2\\exp(3t)+3\\exp(4t) + 4\\exp(5t)\\\\[4pt]\n    &+ 5\\exp(6t)+ 6\\exp(7t)+ 5\\exp(8t)+ 4\\exp(9t)+ 3\\exp(10t)\\\\[4pt]\n    &+ + 2\\exp(11t)+ \\exp(12t))\n\\end{align*}\n\\]\n各係数が確率関数に対応していることがわかります．\n ▶  \\(k=4\\) の場合\n基本的には，\\(Y = \\lfloor\\mathbb E[X_i] \\times 4\\rfloor\\) が最頻値となることを留意すると\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(k=1\\)\n\\(k=2\\)\n\\(k=3\\)\n\\(k=4\\)\n\n\nvalue\nfreq\nvalue\nfreq\nvalue\nfreq\nvalue\nfreq\n\n\n\\(1\\)\n\\(1\\)\n\\(2\\)\n\\(1\\)\n\\(3\\)\n\\(1\\)\n\\(4\\)\n\\(1\\)\n\n\n\\(2\\)\n\\(1\\)\n\\(3\\)\n\\(2\\)\n\\(4\\)\n\\(3\\)\n\\(5\\)\n\\(4\\)\n\n\n\\(3\\)\n\\(1\\)\n\\(4\\)\n\\(3\\)\n\\(5\\)\n\\(6\\)\n\\(6\\)\n\\(10\\)\n\n\n\\(4\\)\n\\(1\\)\n\\(5\\)\n\\(4\\)\n\\(6\\)\n\\(10\\)\n\\(7\\)\n\\(20\\)\n\n\n\\(5\\)\n\\(1\\)\n\\(6\\)\n\\(5\\)\n\\(7\\)\n\\(15\\)\n\\(8\\)\n\\(35\\)\n\n\n\\(6\\)\n\\(1\\)\n\\(7\\)\n\\(6\\)\n\\(8\\)\n\\(21\\)\n\\(9\\)\n\\(56\\)\n\n\n\n\n\\(8\\)\n\\(5\\)\n\\(9\\)\n\\(25\\)\n\\(10\\)\n\\(80\\)\n\n\n\n\n\\(9\\)\n\\(4\\)\n\\(10\\)\n\\(27\\)\n\\(11\\)\n\\(104\\)\n\n\n\n\n\\(10\\)\n\\(3\\)\n\\(11\\)\n\\(27\\)\n\\(12\\)\n\\(125\\)\n\n\n\n\n\\(11\\)\n\\(2\\)\n\\(12\\)\n\\(25\\)\n\\(13\\)\n\\(140\\)\n\n\n\n\n\\(12\\)\n\\(1\\)\n\\(13\\)\n\\(21\\)\n\\(14\\)\n\\(146\\)\n\n\n\n\n\n\n\\(14\\)\n\\(15\\)\n\\(15\\)\n\\(140\\)\n\n\n\n\n\n\n\\(15\\)\n\\(10\\)\n\\(16\\)\n\\(125\\)\n\n\n\n\n\n\n\\(16\\)\n\\(6\\)\n\\(17\\)\n\\(104\\)\n\n\n\n\n\n\n\\(17\\)\n\\(3\\)\n\\(18\\)\n\\(80\\)\n\n\n\n\n\n\n\\(18\\)\n\\(1\\)\n\\(19\\)\n\\(56\\)\n\n\n\n\n\n\n\n\n\\(20\\)\n\\(35\\)\n\n\n\n\n\n\n\n\n\\(21\\)\n\\(20\\)\n\n\n\n\n\n\n\n\n\\(22\\)\n\\(10\\)\n\n\n\n\n\n\n\n\n\\(23\\)\n\\(4\\)\n\n\n\n\n\n\n\n\n\\(24\\)\n\\(1\\)\n\n\n\n\n📘 REMARKS \n\n最小値と最大値の頻度は \\(1\\) となる\n最頻値はちょうど真ん中の値となる(\\(Y = \\lfloor\\mathbb E[X_i] \\times 4\\rfloor\\))\n頻度 \\(f\\) は最頻値に達するまで，以下のようにrecursiveに計算できる\n\n\\[\nf_k(y) = f_k(y-1) + f_{k-1}(y-1) - f_{k-1}(y-7)\n\\]\n上記において，\\(f_{k}(y)\\) が存在しない場合は \\(0\\) と扱う\n\n ▶  Pythonを用いた計算\nフェアなダイスを４つ投げたときの目の合計を \\(X_4\\) としたとき，\\(X_4 = 17\\) となる確率は上の表より\n\\[\n\\Pr(X_4 = 17) = \\frac{104}{1296} = \\frac{13}{162}\n\\]\nとなるはずです．これは sympy を用いて以下のように計算することができます．\n\nimport sympy as sy\nfrom sympy import exp\n\n## variables\nnumber_of_dice = 4\ntarget = 17\n\n## set up sympy variable\nt = sy.Symbol(\"t\", real=True)\n\nexpr = sy.expand(\n    ((exp(t) + exp(t) ** 2 + exp(t) ** 3 + exp(t) ** 4 + exp(t) ** 5 + exp(t) ** 6) / 6)\n    ** number_of_dice\n)\n\n## compute Pr(X = 17)\nprint(\n    expr.coeff(exp(t), target),\n)\n\n13/162\n\n\n上側確率の計算は\n\n## compute Pr(X &gt;= 17)\nprint(sum(map(lambda x: expr.coeff(exp(t), x), range(target, number_of_dice * 6 + 1))))\n\n155/648\n\n\n期待値の計算は，\\(t\\) で微分して \\(t=0\\) で評価すれば良いので\n\nmean_expr = sy.diff(expr, t)\nprint(mean_expr.subs(t, 0))\n\n14",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>離散一様分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/binomial_dist.html",
    "href": "posts/probability_distribution/binomial_dist.html",
    "title": "9  二項分布",
    "section": "",
    "text": "ベルヌーイ分布の性質\n標本空間 \\(\\mathcal{X} = \\{0, 1\\}\\) となるような確率変数をベルヌーイ確率変数と呼んだりします． ベルヌーイ確率変数は，車を購入するか否か，現在の与党を支持するか否か，といったYes/Noクエッションの場面などで登場したりします．\nより一般的にいうと，事象 \\(A\\) に対して指示関数を\n\\[\nI_A(\\omega) = \\bigg\\{\\begin{array}{c}1 & (\\omega \\in A)\\\\ 0 & (\\omega\\not\\in A)\\end{array}\n\\]\nと定義すると，\\(\\Pr(A) = p\\) をパラメーターとするベルヌーイ確率変数になります．\n▶  期待値，分散，MGF\n確率変数 \\(X \\sim \\operatorname{Bernoulli}(p)\\) とするとき，\n\\[\n\\begin{align*}\n\\mathbb E[X] &= p\\\\\n\\operatorname{Var}(X) &= p(1-p)\\\\\nG_X(s) &= (1-p) + ps\\\\\nM_X(t) &= (1-p) + p\\exp(t), \\quad (-\\infty &lt; t &lt; \\infty)\\\\\n\\varphi(t) &= (1-p) + p\\exp(it)\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>二項分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/binomial_dist.html#ベルヌーイ分布の性質",
    "href": "posts/probability_distribution/binomial_dist.html#ベルヌーイ分布の性質",
    "title": "9  二項分布",
    "section": "",
    "text": "Def: ベルヌーイ分布 \n確率変数 \\(X\\) の確率関数 \\(f_X(x)\\) が次の条件を満たすとき，ベルヌーイ分布に従うという:\n\\[\nf_X(x) = \\bigg\\{\\begin{array}{c}p^x(1-p)^{1-x} & x = 0, 1\\\\ 0 & \\text{otherwise}\\end{array}\n\\]\nただし，\\(0 &lt; p &lt; 1\\) とする．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>二項分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/binomial_dist.html#二項分布の性質",
    "href": "posts/probability_distribution/binomial_dist.html#二項分布の性質",
    "title": "9  二項分布",
    "section": "二項分布の性質",
    "text": "二項分布の性質\n成功確率が \\(p\\) のベルヌーイ試行を独立に \\(n\\) 回繰り返したときの成功の回数を \\(X\\) で表したとき，この確率変数 \\(X\\) は二項分布に従います．\n\nDef: 二項分布(Binomial distribution) \n確率変数 \\(X\\) が試行回数 \\(n\\in\\mathbb N\\), 成功確率 \\(0&lt;p&lt;1\\) の二項分布に従うとき，標本空間は \\(\\mathcal{X} = \\{0, 1, \\cdots, n\\}\\)，確率関数 \\(f_X(x)\\) は\n\\[\nf_X(x) = \\bigg\\{\\begin{array}{c}{}_nC_x p^x(1-p)^{n-x} & x \\in \\mathcal{X}\\\\0 & \\text{otherwise}\\end{array}\n\\]\nこのとき，\\(X\\sim \\operatorname{Bin}(n,p)\\) と表す．\n\n二項分布は単一の分布ではなく，特定の \\(n, p\\) に対応する分布の集合です．このような意味で二項分布は分布族(family of distribution)を なしているといいます．分布族において，個々の分布を指定するものを分布族のパラメーターと言います．\n ▶  期待値の導出\n成功確率が \\(p\\) のベルヌーイ試行を独立に \\(n\\) 回繰り返したときの合計が二項分布に従う確率関数なので， 各独立のベルヌーイ確率変数を \\(X_i\\) と表すと\n\\[\nX = \\sum_{i=1}^n X_i\n\\]\n従って，期待値の線型性より\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\mathbb E[\\sum_{i=1}^n X_i]\\\\\n             &= \\sum_{i=1}^n \\mathbb E[X_i]\\\\\n             &= np\n\\end{align*}\n\\]\nまたは，二項分布の定義通りに，\\(q = 1- p\\) とすると\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_{k=0}^n k \\frac{n!}{k!(n-k)!}p^kq^{n-k}\\\\\n    &= np\\sum_{k=1}^n \\frac{(n-1)!}{(k-1)!((n-1)-(k-1))!}p^{k-1}q^{n-1-(k-1)}\\\\\n    &= np\\sum_{l=1}^{n-1} \\frac{(n-1)!}{l!((n-1)-l)!}p^{l}q^{n-1-l}\\\\\n    &= np(p+q)^{n-1}\\\\[5pt]\n    &= np\n\\end{align*}\n\\]\n ▶  二項定理を用いた期待値の導出\n\\(q = 1 - p\\) としたとき，\n\\[\n\\sum_{k=0}^n{}_nC_k p^kq^{n-k} = (p + q)^n\n\\]\n両辺を \\(p\\) について微分すると，\n\\[\n\\sum_{k=0}^n{}_nC_k kp^{k-1}q^{n-k} = n(p + q)^{n-1}\n\\]\nRHSは \\(p=q = 1\\) より, \\(\\operatorname{RHS} = n\\)．LHSについて\n\\[\n\\begin{align*}\n\\operatorname{LHS}\n    &= \\frac{1}{p}\\sum_{k=0}^n{}_nC_k kp^{k}q^{n-k} \\\\\n    &= \\frac{1}{p}\\mathbb E[X]\n\\end{align*}\n\\]\n従って，\\(\\mathbb E[X] = np\\) を得ます．\n ▶  分散の導出\n互いに独立なベルヌーイ確率変数の合計という観点から\n\\[\n\\begin{align*}\n\\operatorname{Var}(X)\n    &= \\operatorname{Var}(\\sum_{i=1}^n X_i)\\\\\n    &= \\sum_{i=1}^n \\operatorname{Var}(X_i)\\\\\n    &= np(1-p)\n\\end{align*}\n\\]\nまたは，二項分布の定義通りに，\\(q = 1- p\\) とすると\n\\[\n\\begin{align*}\n&\\mathbb E[X(X-1)]\\\\\n    &= \\sum_{k=0}^n k(k-1) \\frac{n!}{k!(n-k)!}p^kq^{n-k}\\\\\n    &= n(n-1)p^2 \\sum_{k=2}^n \\frac{(n-2)!}{(k-2)!((n-2)-(k-2))!}p^{k-2}q^{n-2-(k-2)}\\\\\n    &= n(n-1)p^2\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\operatorname{Var}(X)\n    &= \\mathbb E[X(X-1)] + \\mathbb E[X](1 - \\mathbb E[X])\\\\\n    &= n(n-1)p^2 + np(1 - np)\\\\\n    &= np[np-p + 1 - np]\\\\\n    &= np(1-p)\n\\end{align*}\n\\]\n ▶  確率母関数の計算\n確率変数 \\(X\\sim\\operatorname{Bin}(n, p)\\) としたとき，\n\\[\n\\begin{align*}\nG_X(s)\n    &= \\sum_{x=0}^ns^x \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)p^x(1-p)^{n-x}\\\\\n    &= \\sum_{x=0}^n \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)(sp)^x(1-p)^{n-x}\\\\\n    &= (sp + 1-p)^n\\\\[5pt]\n    &= [(s-1)p + 1]^n\n\\end{align*}\n\\]\nまたは，ベルヌーイ確率変数の合計という観点から\n\\[\n\\begin{align*}\nG_X(s)\n    &= \\prod_{i=1}^n G_{X_{i}}(s) \\\\\n    &= (sp + 1-p)^n\\\\[5pt]\n    &= [(s-1)p + 1]^n\n\\end{align*}\n\\]\n確率母関数から期待値を計算してみると\n\\[\nG_X^\\prime(s) = np [(s-1)p + 1]^{n-1}\n\\]\n従って，\n\\[\nG_X^\\prime(s)\\bigg\\vert_{s=1} = np\n\\]\n ▶  MGFの計算\n\\[\n\\begin{align*}\nM_X(t)\n    &= \\sum_{x=0}^n e^{tx} \\times {}_nC_xp^x(1-p)^{n-x}\\\\\n    &= \\sum_{x=0}^n \\times {}_nC_x(pe^t)^{x}(1-p)^{n-x}\\\\\n    &= [pe^t + (1 - p)]^n\n\\end{align*}\n\\]\n\nポワソン分布との関係\n二項分布において，\n\n\\(n\\) が大きい: 大量の観察が可能\n\\(p\\) が小さい: レアな事象\n\nである場合，両方の傾向が釣り合って，それほど大きくないけれどもある程度の \\(x\\) が現実に観察されます． 例えば，不動産申込みからの成約率が \\(p = 0.003\\) で月に \\(n=1000\\) の申込みがあるとします．\nこのとき，modeであるような\n\\[\n\\Pr(X = 3) = {}_{1000}C_3(0.003)^3(0.997)^{1000-3}\n\\]\nは数値的に計算することが難しいです．このようなとき，ポワソン近似を用いたりします．\n\n\nTheorem 9.1 : ポワソンの少数の法則 \n\\(np=\\lambda\\) が一定のもとで \\(p\\to 0, n\\to\\infty\\) とすると， 二項分布 \\(\\operatorname{Bin}(n, p)\\) はポアソン分布 \\(\\operatorname{Po}(\\lambda)\\) に収束する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(p = \\lambda/n\\) であるので，二項分布の確率関数は\n\\[\n\\begin{align*}\np(x)\n    &= \\frac{n!}{x!(n-x)!}\\left(\\frac{\\lambda}{n}\\right)^x\\left(1-\\frac{\\lambda}{n}\\right)^{n-x}\\\\\n    &= \\frac{\\lambda^x}{x!}\\frac{n!}{(n-x)!n^x}\\left(1-\\frac{\\lambda}{n}\\right)^n\\left(1-\\frac{\\lambda}{n}\\right)^{-x}\n\\end{align*}\n\\]\n\\(n\\to\\infty\\) のとき，\n\\[\n\\begin{align*}\n\\frac{n!}{(n-x)!n^x} &\\to 1\\\\\n\\left(1-\\frac{\\lambda}{n}\\right)^n &\\to \\exp(-\\lambda)\\\\\n\\left(1-\\frac{\\lambda}{n}\\right)^{-x}&\\to 1\n\\end{align*}\n\\]\n従って，\n\\[\np(x) \\to \\frac{\\lambda^x}{x!}\\exp(-\\lambda)\n\\]\nとポワソン分布の確率関数に収束することがわかる．\n ▶  pythonでの確認\n\nimport numpy as np\nfrom scipy.stats import binom, poisson\nimport polars as pl\nfrom plotly import express as px\n\nmu = 3\nN = [8, 10, 100]\n\nx = np.arange(0, 9)\ndf = pl.DataFrame(\n    {\n        \"normalized_domain\": x,\n        **{f\"Binom-N={n}\": binom(n=n, p=mu / n).pmf(x) for n in N},\n        \"Poisson-λ=3\": poisson(mu).pmf(x),\n    }\n)\n\nfig = px.line(\n    df,\n    x=\"normalized_domain\",\n    y=df.columns[1:],\n    title='Binomial dist converges to Poisson dist with λ = 3',\n    markers='x'\n)\nfig.update_layout(yaxis=dict(title=\"probability\"))\nfig.show()\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\nProof: 積率母関数を用いた証明\n\n\n\n\n\n\\(p = \\lambda/n\\) を用いて二項分布の積率母関数を表すと\n\\[\n\\begin{align*}\nM_X(t)\n    &= [pe^t + (1-p)]^n\\\\\n    &= \\left[\\frac{\\lambda}{n}e^t + 1 - \\frac{\\lambda}{n}\\right]^n\\\\\n    &= \\left[1 + \\frac{\\lambda}{n}(e^t - 1)\\right]^n\\\\\n    &\\to \\exp[\\lambda (e^t - 1)]\\quad (\\text{as } n\\to\\infty)\n\\end{align*}\n\\]\n以上より，ポワソン分布の積率母関数に収束することがわかります．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>二項分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/poisson_dist.html",
    "href": "posts/probability_distribution/poisson_dist.html",
    "title": "10  ポワソン分布",
    "section": "",
    "text": "ポワソン分布の性質\n稀な現象の大量観測（二項分布に置き換えるならば \\(n\\) がおおきく，\\(p\\) が十分小さい）において発生する個数の分布を表すのにポワソン分布が 用いられます．例として，\nポワソン分布の \\(\\lambda &gt; 0\\) は強度もしくは生起率と呼ばれるパラメーターです．\n▶  確率関数の和\n\\[\n\\begin{align*}\n\\sum_{x=0}^\\infty f_X(x)\n    &= \\sum_{x=0}^\\infty\\frac{\\lambda^x}{x!}\\exp(-\\lambda)\\\\\n    &= \\exp(-\\lambda)\\sum_{x=0}^\\infty\\frac{\\lambda^x}{x!}\\\\\n    &= \\exp(-\\lambda)\\left[1 + \\frac{\\lambda}{1!} + \\frac{\\lambda^2}{2!} + \\cdots\\right]\\\\\n    &= \\exp(-\\lambda)\\exp(\\lambda) \\quad\\because\\text{マクローリン展開より}\\\\\n    &= 1\n\\end{align*}\n\\]\n▶  期待値の導出\n確率変数 \\(X\\sim\\operatorname{Po}(\\lambda)\\) について，\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_{x=0}^\\infty x \\frac{\\lambda^x}{x!}\\exp(-\\lambda)\\\\\n    &= \\lambda\\exp(-\\lambda)\\sum_{x=1}^\\infty\\frac{\\lambda^{x-1}}{(x-1)!}\\\\\n    &= \\lambda\\exp(-\\lambda)\\sum_{k=0}^\\infty\\frac{\\lambda^k}{k!}\\\\\n    &= \\lambda\\exp(-\\lambda)\\exp(\\lambda)\\\\\n    &= \\lambda\n\\end{align*}\n\\]\n▶  分散の導出\n\\[\n\\begin{align*}\n\\mathbb E[X(X-1)]\n    &= \\sum_{x=0}^\\infty x(x-1) \\frac{\\lambda^x}{x!}\\exp(-\\lambda)\\\\\n    &= \\lambda^2\\exp(-\\lambda)\\sum_{x=2}^\\infty\\frac{\\lambda^{x-2}}{(x-2)!}\\\\\n    &= \\lambda^2\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\operatorname{Var}(X)\n    &= \\mathbb E[X(X- 1)] + \\mathbb E[X](1 - \\mathbb E[X])\\\\\n    &= \\lambda^2 + \\lambda(1 - \\lambda)\\\\\n    &= \\lambda\n\\end{align*}\n\\]\n▶  確率母関数の計算\n確率変数 \\(X\\sim\\operatorname{Po}(\\lambda)\\) としたとき， 確率母関数 \\(G_X(s)\\) は\n\\[\n\\begin{align*}\nG_X(s)\n    &= \\sum_{x=0}^\\infty s^x\\frac{\\lambda^x}{x!}\\exp(-\\lambda)\\\\\n    &= \\exp(-\\lambda)\\sum_{x=0}^\\infty \\frac{(s\\lambda)^x}{x!}\\\\\n    &= \\exp(-\\lambda)\\exp(s\\lambda)\\\\\n    &= \\exp((s-1)\\lambda)\n\\end{align*}\n\\]\n期待値は\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= G_X^\\prime(1)\\\\\n    &= \\lambda \\exp((1-1)\\lambda)\\\\\n    &= \\lambda\n\\end{align*}\n\\]\n分散は \\(\\operatorname{Var}(X) = \\mathbb E[X(X-1)] + \\mathbb E[X](1 - \\mathbb E[X])\\) なので\n\\[\n\\begin{align*}\n\\mathbb E[X(X-1)]\n    &= G_X^{\\prime\\prime}(1)\\\\\n    &= \\lambda \\lambda\\exp((1-1)\\lambda)\\\\\n    &= \\lambda^2\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\operatorname{Var}(X)\n    &= \\lambda^2 + \\lambda(1-\\lambda)\\\\\n    &= \\lambda\n\\end{align*}\n\\]\n▶  積率母関数の計算\n確率変数 \\(X\\sim\\operatorname{Po}(\\lambda)\\) としたとき， 積率母関数 \\(M_X(t)\\) は\n\\[\n\\begin{align*}\nM_X(t)\n    &= \\mathbb E[\\exp(tX)]\\\\\n    &= \\sum_{x=0}\\exp(-\\lambda)\\exp(tX)\\frac{\\lambda^x}{x!}\\\\\n    &= \\exp(-\\lambda)\\sum_{x=0}\\frac{(\\exp(t)\\lambda)^x}{x!}\\\\\n    &= \\exp(-\\lambda)\\exp(e^t\\lambda)\\\\[5pt]\n    &= \\exp[\\lambda(e^t - 1)]\n\\end{align*}\n\\]x",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ポワソン分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/poisson_dist.html#ポワソン分布の性質",
    "href": "posts/probability_distribution/poisson_dist.html#ポワソン分布の性質",
    "title": "10  ポワソン分布",
    "section": "",
    "text": "ある都市の１日に起こる交通事故の件数の分布\nある都市で１年間に肺がんによりなくなる人数の分布\n\n\nDef: ポワソン分布(Poisson distribution) \n確率変数 \\(X\\) がパラメーター \\(\\lambda &gt; 0\\) のポワソン分布に従うとき，標本空間は \\(\\mathcal{X} = \\{0, 1, \\cdots, n\\}\\)，確率関数 \\(f_X(x)\\) は\n\\[\nf_X(x) = \\bigg\\{\\begin{array}{c} \\frac{\\lambda^x}{x!}\\exp(-\\lambda) & x \\in \\mathcal{X}\\\\0 & \\text{otherwise}\\end{array}\n\\]\nこのとき，\\(X\\sim \\operatorname{Po}(\\lambda)\\) と表す．\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 10.1 : 19-20シーズンのMan Utdのゴール数とポワソン分布 \nMan Utd English Premier League 2019/20 fixture and resultsというサイトではプレミアリーグの各シーズン及び各チームの試合結果が保存されています. 上記サイトのURLの構造は\nhttps://fixturedownload.com/results/epl-&lt;シーズン&gt;/&lt;team名&gt;\n19-20シーズンのMan Utdのデータを抽出したい場合は次のようにパラメーターを設定します．\n\n\n\nparameter\nvalue\n意味\n\n\n\n\nteam名\nman-utd\nMan Utd, すべて小文字、スペースはハイフンとなる\n\n\nシーズン\n2019\n2019-2020シーズン\n\n\n\n抽出結果は以下のようになります\n\n\nCode\nimport pandas as pd\ntarget_team = 'man-utd'\nTARGET_TEAMNAME = target_team.replace('-', ' ').title()\nURL_PATH = 'https://fixturedownload.com/results/epl-2019/'+target_team\n\ndf = pd.read_html(URL_PATH, flavor=\"bs4\")[0]\ndf.head()\n\n\n\n\n\n\n\n\n\nRound Number\nDate\nLocation\nHome Team\nAway Team\nResult\n\n\n\n\n0\n1\n11/08/2019 16:30\nOld Trafford\nMan Utd\nChelsea\n4 - 0\n\n\n1\n2\n19/08/2019 20:00\nMolineux Stadium\nWolves\nMan Utd\n1 - 1\n\n\n2\n3\n24/08/2019 15:00\nOld Trafford\nMan Utd\nCrystal Palace\n1 - 2\n\n\n3\n4\n31/08/2019 12:30\nSt. Mary's Stadium\nSouthampton\nMan Utd\n1 - 1\n\n\n4\n5\n14/09/2019 15:00\nOld Trafford\nMan Utd\nLeicester\n1 - 0\n\n\n\n\n\n\n\nここからMan Utdのゲームごとのゴール数配列を抽出し，ポワソン分布比較してみます．\n\n\nCode\nimport numpy as np\nfrom plotly import express as px\nimport plotly.graph_objects as go\nfrom scipy.stats import poisson\n\n\ndef extract_goal(data, target_team):\n    score_list = data[\"Result\"].str.split(r\"\\s-\\s\", expand=True)\n    score_list.columns = [\"home_score\", \"away_score\"]\n    score_list = score_list.astype({\"home_score\": \"int64\", \"away_score\": \"int64\"})\n\n    data = pd.concat([data, score_list], axis=1)\n    return np.where(\n        df[\"Home Team\"] == target_team, data[\"home_score\"], data[\"away_score\"]\n    )\n\n\ngoal_array = extract_goal(df.copy(), target_team=TARGET_TEAMNAME)\n\n## 可視化\nx, y = np.unique(goal_array, return_counts=True)\n\nfig = go.Figure()\nfig.add_trace(\n    go.Bar(\n        x=x,\n        y=y / sum(y),  # Normalizing the frequency\n        name=\"Goals per games\", \n        width=0.3\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=x,\n        y=poisson(np.mean(goal_array)).pmf(x),\n        mode=\"lines+markers\",\n        name=\"Poisson\",\n        line=dict(color=\"gray\", dash=\"dot\"),\n    )\n)\n\nfig.update_layout(\n    title=\"19-20シーズン Man Utd Goal Distribution\",\n    xaxis_title=\"Goals\",  # X-axis label\n    yaxis_title=\"Frequency\",  # Y-axis label\n    legend_title=\"Legend\",  # Legend title\n    showlegend=True,  # Ensure legend is displayed\n)\n\nfig.show()    \n\n\n                                                \n\n\n\nFittingはそんなに悪くないが，scoreがゼロのところが理論頻度より実現値のほうが頻度が多い傾向がわかります\n0が過剰に含まれたデータに対してはゼロ過剰ポアソン分布(Zero-inflated Poisson Model, ZIP)を用いて対処することが考えられます\n\n\n\n\nTheorem 10.1 : ポワソン分布の再生性 \n互いに独立な確率変数 \\(X\\sim\\operatorname{Po}(\\lambda_x), Y\\sim\\operatorname{Po}(\\lambda_y)\\) について, \\(X + Y\\) もパラメーター \\(\\lambda_x + \\lambda_y\\) のポワソン分布に従う．つまり，\n\\[\nX + Y \\sim \\operatorname{Po}(\\lambda_x + \\lambda_y)\n\\]\n\n\n\n\n\n\n\n\nProof: PGFを用いた証明\n\n\n\n\n\n\\[\n\\begin{align*}\nG_{X+Y}(s)\n    &= G_{X}(s)G_{Y}(s)\\\\\n    &= \\exp(-\\lambda_x)\\exp(s\\lambda_x)\\exp(-\\lambda_y)\\exp(s\\lambda_y)\\\\\n    &= \\exp(-(\\lambda_x+\\lambda_y))\\exp(s(\\lambda_x+\\lambda_y))\n\\end{align*}\n\\]\nこれは，パラメーター \\(\\lambda_x + \\lambda_y\\) のポワソン分布のPGFに一致しています．\n\n\n\n\n\n\n\n\n\nProof: MGFを用いた証明\n\n\n\n\n\n\\[\n\\begin{align*}\nM_{X+Y}(t)\n    &= M_{X}(t)M_{X}(t)\\\\\n    &= \\exp[\\lambda_x(e^t - 1)]\\exp[\\lambda_y(e^t - 1)]\\\\\n    &= \\exp[\\lambda_x(e^t - 1) + \\lambda_y(e^t - 1)]\\\\\n    &= \\exp[(\\lambda_x + \\lambda_y)(e^t - 1)]\n\\end{align*}\n\\]\nこれは，パラメーター \\(\\lambda_x + \\lambda_y\\) のポワソン分布のMGFに一致しています．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ポワソン分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/poisson_dist.html#estimation",
    "href": "posts/probability_distribution/poisson_dist.html#estimation",
    "title": "10  ポワソン分布",
    "section": "Estimation",
    "text": "Estimation\n\nOmmited vriable and overdispersion\n\nDef: overdispersion \n観察データについて，期待される分散よりも大きい分散(variation)が確認される状があるとき，overdispersionであるという．\n\nポワソン分布において，上で確認したように分散と平均は一致するはずですが，ommitted variable biasを原因として overdispersionが発生するケースが多くあります．\n\\[\n\\begin{align*}\ny &\\sim \\operatorname{Po}(\\lambda)\\\\\n\\log(\\lambda) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\end{align*}\n\\]\nという確率変数を考えます．このとき次のような条件を想定します：\n\n観察データにおいて, \\(x_2\\) は観察不能(=missing variable)\n\\(x_1 \\perp x_2\\)\n\nこのとき，\\(y\\) について \\(x_1\\) を条件づけたときの期待値を計算すると\n\\[\n\\begin{align*}\n\\mathbb E[y\\vert x_1]\n    &= \\mathbb E[\\mathbb E[y\\vert x_1, x_2]\\vert x_1]\\\\\n    &= \\mathbb E[\\exp(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)\\vert x_1]\\\\\n    &= \\exp(\\beta_0 + \\beta_1 x_1)\\mathbb E[\\exp(\\beta_2 x_2)\\vert x_1]\\\\\n    &= \\exp(\\beta_0 + \\beta_1 x_1)\\mathbb E[\\exp(\\beta_2 x_2)] \\because\\text{独立性}\\\\\n    &= \\exp(\\tilde\\beta_0+ \\beta_1 x_1)\n\\end{align*}\n\\]\nと，切片には影響を与えますが，\\(\\beta_1\\) についてはunbiasedに推定できることがわかります．一方，条件付き分散を見てみると\n\\[\n\\begin{align*}\n\\operatorname{Var}(y\\vert x_1)\n    =& \\mathbb E[\\operatorname{Var}(y\\vert x_1, x_2)\\vert x_1] + \\operatorname{Var}(\\mathbb E[y\\vert x_1, x_2]\\vert x_1)\\\\[3pt]\n    =& \\mathbb E[\\exp(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)\\vert x_1] \\\\\n     &+ \\operatorname{Var}(\\exp(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)\\vert x_1)\\\\\n    =& \\exp(\\tilde\\beta_0+ \\beta_1 x_1) \\\\\n     & + [\\exp(\\beta_0 + \\beta_1x_1)]^2\\operatorname{Var}(\\exp(\\beta_2 x_2))\\\\\n    &gt;& \\exp(\\tilde\\beta_0+ \\beta_1 x_1) = \\mathbb E[y\\vert x_1]\n\\end{align*}\n\\]\n以上より，OVBのとき，ovberdispersionが発生してしまうことがわかります．\n\n📘 REMARKS \nポワソン回帰においてover-dispersionが発生した場合は以下のケースが考えられます：\n\n重要な特徴量が欠落変数(ommited variable)になってしまっている\n説明変数，被説明変数(response variable)についてmeasurement errorが発生してしまっている\n\\(\\log(\\lambda)\\) と特徴量ベクトル \\(\\mathbf x\\) のモデル特定に誤りがある\nOutlierの存在\nresponse variableが複数の確率分布の混合(mixture)に基づいている場合",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ポワソン分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/geometric_dist.html",
    "href": "posts/probability_distribution/geometric_dist.html",
    "title": "11  幾何分布",
    "section": "",
    "text": "幾何分布の性質\n幾何分布は，時間を \\(0, 1, 2, \\cdots\\) と離散的に考えるとき，初めてイベント \\(S\\) が起こるまでの時間の長さを表す確率分布 = 離散的な待ち時間分布とみなすことができます．一回あたりのイベント \\(S\\) の生起確率が \\(p\\) であるとすると，ちょうど１回の \\(S\\) を得るまで平均的に\n\\[\n\\frac{1}{p}\n\\]\nの時間がかかるので，生起までの待ち時間として \\(\\frac{1}{p}-1 = \\frac{1-p}{p}\\) と直感的に理解することができます．\n▶  累積分布関数\n幾何分布の累積分布関数は以下のように表すことができます:\n\\(X\\sim\\operatorname{geo}(p)\\) について，\\(x \\geq 0\\) のもとで, \\(q = 1- p\\) とすると\n\\[\n\\begin{align*}\n\\Pr(X\\leq x)\n    &= \\sum_{k=0}^x \\Pr(X=k)\\\\\n    &= \\sum_{k=0}^{\\lfloor x\\rfloor} pq^k\\\\\n    &= p\\frac{1-q^{\\lfloor x\\rfloor+1}}{1- q}\\\\\n    &= 1-q^{\\lfloor x\\rfloor+1}\n\\end{align*}\n\\]\nCode\nimport numpy as np\nfrom scipy.stats import geom\nfrom plotly import express as px\n\np = 0.4\nx = np.arange(geom.ppf(0.01, p), geom.ppf(0.99995, p))\n\n# note that scipy follows p(x) = (1-p)^{x-1}p \nrv = geom(p)\n\nfig = px.line(\n    x=x-1,\n    y=rv.cdf(x),\n    title=\"geometric distribution with p=0.4\",\n    labels={\"x\": \"x\", \"y\": \"probability\"},\n    markers='x'\n)\nfig.show()\n▶  Tail probability\n\\(X\\sim\\operatorname{geo}(p)\\) について，\\(x\\geq 0\\) のもとで，, \\(q = 1- p\\) とすると\n\\[\n\\begin{align*}\n\\Pr(X\\geq x)\n    &= \\sum_{k=\\lceil x \\rceil}^\\infty pq^k\\\\\n    &= p \\frac{q^{\\lceil x \\rceil}}{1-q}\\\\\n    &= q^{\\lceil x \\rceil}\\\\\n    &= (1-p)^{\\lceil x \\rceil}\n\\end{align*}\n\\]\n▶  幾何分布の期待値\n\\(q=1-p\\)とおくと\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_{x=0}^\\infty xq^xp\\\\\n    &= p\\sum_{x=1}^\\infty xq^x\n\\end{align*}\n\\]\nここで，\\(S = \\sum_{x=1}^\\infty xq^x\\) について考えると， \\[\n\\begin{align*}\nS &= q + 2q^2 + 3q^3 + \\cdots\\\\\nqS &= q^2 + 2q^3 + \\cdots\\\\\n(1-q)S& = q + q^2 + q^3 + \\cdots\n\\end{align*}\n\\]\n従って，\\(S = \\frac{q}{(1-q)^2}\\) を得る．以上より\n\\[\n\\mathbb E[X] = p \\frac{q}{(1-q)^2} = \\frac{1-p}{p}\n\\]\n▶  分布関数を用いた幾何分布の期待値の導出\n離散確率変数 \\(X\\) について，\n\\[\n\\mathbb E[X] = \\sum_{x=0}^\\infty \\Pr(X&gt; x)\n\\]\nが知られている(参考:期待値 &gt; Discrete Tail Probability)ので\n\\[\n\\begin{align*}\n\\Pr(X&gt; x)\n    &= 1 - \\Pr(X\\leq x)\\\\\n    &= 1 - (1 - q^{x+1})\\\\\n    &= q^{x+1}\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_{x=0}^\\infty q^{x+1}\\\\\n    &= \\frac{q}{1-q}\\\\\n    &= \\frac{1-p}{p}\n\\end{align*}\n\\]\n▶  確率の公理を用いた幾何分布の期待値の導出\n\\[\n\\sum_{x=0}^\\infty q^xp = 1\n\\]\nに留意すると\n\\[\n\\begin{align*}\n\\sum_{x=0}^\\infty xq^xp - q\\sum_{x=0}^\\infty xq^xp\n    &= \\sum_{x=0}^\\infty q^xp - p\\\\\n    &= 1-p\n\\end{align*}\n\\]\n従って，\\(\\mathbb E[X] - q\\mathbb E[X] = 1-p\\) を得る．これを整理すると\n\\[\n\\mathbb E[X] = \\frac{1-p}{p}\n\\]\n▶  幾何分布の分散\n\\[\n\\begin{align*}\n\\mathbb E[X^2]\n    &= \\sum_{x=0}^\\infty x^2q^xp\\\\\n    &= p\\sum_{x=1}^\\infty x^2q^x\n\\end{align*}\n\\]\nここで，\\(S = \\sum_{x=1}^\\infty x^2q^x\\) について考えると， \\[\n\\begin{align*}\nS &= 1^2q + 2^2q^2 + 3^2q^3 + \\cdots\\\\\nqS &= 1^2q^2 + 2^2q^3 + \\cdots\\\\\n(1-q)S& = 1^2q + (2^2-1^2)q^2 + (3^2-2^2)q^3 + \\cdots\\\\\n      &= \\sum_{x=1}^\\infty [x^2 - (x-1)^2]q^x\\\\\n      &= \\sum_{x=1}^\\infty [2x-1]q^x\\\\\n      &= \\frac{2-2p}{p^2} - \\frac{1-p}{p}\\\\\n      &= \\frac{(1-p)(2-p)}{p^2}\n\\end{align*}\n\\]\n\\(\\mathbb E[X^2] = pS = (1-q)S\\) より，\n\\[\n\\operatorname{Var}(X) = \\frac{(1-p)(2-p)}{p^2} - \\frac{(1-p)^2}{p^2} = \\frac{(1-p)}{p^2}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/geometric_dist.html#幾何分布の性質",
    "href": "posts/probability_distribution/geometric_dist.html#幾何分布の性質",
    "title": "11  幾何分布",
    "section": "",
    "text": "Def: 幾何分布 \n標本空間 \\(\\mathcal{X} = \\{0, 1, 2, \\cdots\\}\\) をもつ確率変数 \\(X\\) の確率関数が\n\\[\n\\Pr(X=k) = (1-p)^{k}p \\qquad (0&lt;p&lt;1)\n\\]\nのとき，\\(X\\) はパラメータ \\(p\\) の幾何分布に従うという．つまり，\n\\[\nX\\sim\\operatorname{Geo}(p)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n幾何分布の母関数\n\n\nTheorem 11.1 : 幾何分布の確率母関数 \n\\(X\\sim\\operatorname{Geo}(p)\\) のとき，確率母関数は\n\\[\nG_X(s) = \\frac{p}{1-sq}\n\\]\nと表される\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n確率変数 \\(X\\sim\\operatorname{Geo}(p)\\) としたとき，確率関数は \\(x = 0, 1, 2, \\cdots\\) について\n\\[\n\\begin{align*}\n\\Pr(X=x)\n    &= p(1-p)^x\\\\\n    &=pq^x \\qquad \\text{where } q = 1-p\n\\end{align*}\n\\]\nと表されるので\n\\[\n\\begin{align*}\nG_X(s)\n    &= \\sum_{x=0}^\\infty s^xpq^x\\\\\n    &= p\\sum_{x=0}^\\infty (sq)^x\\\\\n    &= \\frac{p}{1-sq} \\quad \\text{for all} s \\text{ such that } \\vert qs\\vert &lt; 1\n\\end{align*}\n\\]\n従って，\n\\[\nG_X(s) = \\frac{p}{1-sq} \\text{ for } \\vert s\\vert &lt; \\frac{1}{q}\n\\]\nとなります．PGFより期待値は\n\\[\n\\begin{align*}\nG_X^\\prime(s)\n    &= q\\frac{p}{(1-sq)^2}\\\\\n\\Rightarrow \\mathbb E[X] &= \\frac{q}{p} = \\frac{1-p}{p}   \n\\end{align*}\n\\]\n分散は\n\\[\n\\begin{align*}\nG_X^{\\prime\\prime}(s)\n        &= 2q^2\\frac{p}{(1-sq)^3}\\\\\n\\Rightarrow \\operatorname{Var}(X) &= \\frac{2q^2}{p^2} + \\frac{q}{p}(1-\\frac{q}{p})\\\\\n        &= \\frac{q}{p}\\left(\\frac{q}{p}+1\\right)\\\\\n        &=\\frac{1-p}{p^2}\n\\end{align*}\n\\]\n\n\n\n\nExample 11.1 \n初めての成功までに要した回数という形で幾何分布を変更した場合を考えます．つまり， \\(\\mathcal{X} = \\{1, 2, \\cdots\\}\\) と標本空間が表され，確率関数は\n\\[\n\\Pr(X = x) = pq^{x-1}\n\\]\nこの場合の確率母関数は\n\\[\n\\begin{align*}\nG_X(s)\n    &= \\sum_{x=1}^\\infty s^xpq^{x-1}\\\\\n    &= q^{-1}p\\sum_{x=1}^\\infty (sq)^x\\\\\n    &= \\frac{p}{q}\\frac{sq}{1-sq} \\quad \\text{for all} s \\text{ such that } \\vert qs\\vert &lt; 1\n\\end{align*}\n\\]\n期待値を求めてみると\n\\[\n\\begin{align*}\nG_X^\\prime(s)\n    &= \\frac{p}{q}\\frac{q}{1-sq} + p\\frac{sq}{(1-sq)^2}\n\\end{align*}\n\\]\n従って，\n\\[\n\\mathbb E[X] = G_X^\\prime(0) = \\frac{1}{p}\n\\]\n\n\n\nTheorem 11.2 : 積率母関数 \n\\(X\\sim\\operatorname{Geo}(p)\\) のとき，積率母関数は\n\\[\nM_X(t) = \\frac{p}{1 - q\\exp(t)}, \\qquad t &lt; -\\log q\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[\\exp(tX)]\n    &= \\sum_{x=0}^\\infty pq^x\\exp(tx)\\\\\n    &= p\\sum_{x=0}^\\infty q^x\\exp(tx)\\\\\n    &= p \\frac{1}{1 - q\\exp(t)}\n\\end{align*}\n\\]\nただし，収束するためには \\(q\\exp(t) &lt; 1\\)，すなわち \\(t &lt; -\\log(q)\\) が条件となります．\n\n\n\n\n\nTheorem 11.3 : 特性関数 \n\\(X\\sim\\operatorname{Geo}(p)\\) のとき，特性関数は\n\\[\n\\varphi(t) = \\frac{p}{1 - q\\exp(it)}, \\qquad t \\in \\mathbb R\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[\\exp(itX)]\n    &= \\sum_{x=0}^\\infty pq^x\\exp(itx)\\\\\n    &= p\\sum_{x=0}^\\infty q^x\\exp(itx)\\\\\n    &= p \\frac{1}{1 - q\\exp(it)}\n\\end{align*}\n\\]\n\\(\\vert q\\exp(it)\\vert &lt; 1\\) であるので，\\(t\\in \\mathbb R\\) で上記の式は収束します．\n\n\n\n\n\n無記憶性\n\n\nTheorem 11.4 : 幾何分布の無記憶性 \n\\(m, n\\) を非負の整数とし，確率変数 \\(X\\sim\\operatorname{Geo}(p)\\) とする．このとき，\n\\[\n\\Pr(X \\geq m+n \\vert X \\geq m) = \\Pr(X\\geq n)\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(q = 1 - p\\) としたとき\n\\[\n\\Pr(X\\geq m) = q^m\n\\]\nとなるので，条件付き確率は\n\\[\n\\begin{align*}\n\\Pr(X \\geq m+n \\vert X \\geq m)\n    &= \\frac{q^{m+n}}{q^m}\\\\[5pt]\n    &= q^n\\\\[5pt]\n    &= \\Pr(X\\geq n)\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/geometric_dist.html#他の確率分布との関係性",
    "href": "posts/probability_distribution/geometric_dist.html#他の確率分布との関係性",
    "title": "11  幾何分布",
    "section": "他の確率分布との関係性",
    "text": "他の確率分布との関係性\n\n\nTheorem 11.5 : 指数分布との関係性 \n\\(n\\in\\mathbb N\\) に対して，\\(U_n\\) が成功確率 \\(p_n\\in(0,1)\\) をもつ幾何分布に従うとします．ここで， \\(np_n\\to r &gt;0 \\text{ as } n\\to\\infty\\) が成立するとき，\n\\[\nU_n/n \\overset{\\mathrm{d}}{\\to} \\operatorname{Exp}(r)\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(F_n\\) を \\(U_n/n\\) の累積分布関数とすると，\n\\[\n\\begin{align*}\nF_n(x)\n    &= \\Pr(U_n/n \\leq x)\\\\[5pt]\n    &= \\Pr(U_n \\leq nx)\\\\[5pt]\n    &= \\Pr(U_n \\leq \\lfloor nx\\rfloor)\\\\[5pt]\n    &= 1 - (1 - p_n)^{\\lfloor nx\\rfloor + 1}\\\\[5pt]\n    &= 1 - (1 - p_n)^{\\lfloor nx\\rfloor}(1 - p_n)\n\\end{align*}\n\\]\nここで，\n\\[\n\\begin{align*}\n\\lim_{n\\to\\infty} (1 -p_n)^n\n    &= \\lim_{n\\to\\infty} \\left(1 -\\frac{np_n}{n}\\right)^n\\\\\n    &= \\exp(-p_n)\\\\\n\\lim_{n\\to\\infty} (1 -p_n)\n    &= \\lim_{n\\to\\infty} \\left(1 -\\frac{np_n}{n}\\right)\\\\\n    &= \\lim_{n\\to\\infty} \\left(1 - \\frac{r}{n}\\right)\\\\\n    &= 1\n\\end{align*}\n\\]\nであるので，\n\\[\nF_n(x) \\to 1 - \\exp(-rx) \\quad \\text{as } n\\to \\infty\n\\]\nこれは指数分布の累積分布関数とするので，\n\\[\nU_n/n \\overset{\\mathrm{d}}{\\to} \\operatorname{Exp}(r)\n\\]\nが成立する．\n ▶  収束の確認\n\nimport numpy as np\nfrom scipy.stats import expon, geom\nimport polars as pl\nfrom plotly import express as px\n\nr = 1\nN = [2, 5, 10, 50]\n\nx = np.linspace(0, 4, 1000)\ndf = pl.DataFrame(\n    {\n        \"normalized_domain\": x,\n        **{f\"Geom-N={n}\": geom.cdf(x * n, r / n) for n in N},\n        \"Expon\": expon.cdf(x),\n    }\n)\n\nfig = px.line(\n    df, x=\"normalized_domain\", y=df.columns[1:], title=\"Geometric dist converges to exponential dist\"\n)\nfig.update_layout(yaxis=dict(title=\"cumulative probability\"))\nfig.show()\n\n                                                \n\n\n\n\n\n\nExample 11.2 : 離散一様分布との関係性 \n確率変数列 \\(X = (X_1, X_2, \\cdots)\\) を互いに独立なパラメータ \\(p\\in(0,1)\\) のベルヌーイ確率変数列とします． \\(n\\in\\mathbb N\\) について，\n\\[\nY_n = \\sum_{i=1}^nX_i\n\\]\nと定義します．このとき，\\(Y_n\\sim\\operatorname{Binom}(n, p)\\) となります．\n次に，ベルヌーイ試行列で初めて成功を引くまでに要した回数を確率変数 \\(N\\) とし，\\(Y_n = 1\\) の条件の下での確率分布を計算すると， 以下のように標本空間 \\(\\mathcal{X}= \\{1, 2, \\cdots, n\\}\\) とする離散一様分布と一致することがわかります．\n\\(j\\in\\{1, 2, \\cdots, n\\}\\) に対して，\n\\[\n\\begin{align*}\n\\Pr(N = j\\vert Y_n = 1)\n    &= \\frac{\\Pr(N=j, Y_n=1)}{\\Pr(Y_n=1)}\\\\\n    &= \\frac{\\Pr(Y_{j-1} = 0,X_j=1, Y_n=1)}{\\Pr(Y_n=1)}\\\\\n    &= \\frac{(1-p)^{j-1}p(1-p)^{n-j}}{np(1-p)^{n-1}}\\\\\n    &= \\frac{1}{n}\n\\end{align*}\n\\]\n上記より，条件付き分布はパラメータ \\(p\\) に依存しないことがわかります．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/negative_binom.html",
    "href": "posts/probability_distribution/negative_binom.html",
    "title": "12  負の二項分布",
    "section": "",
    "text": "負の二項分布の性質\n▶  確率関数の和\n\\(1/(1-q)\\) のマクローリン展開より，\n\\[\n\\frac{1}{1-q}=1 + q + q^2 +\\cdots = \\sum_{k=0}^\\infty q^k\n\\]\n両辺を \\(q\\) について微分すると\n\\[\n\\frac{1}{(1-q)^2} =  \\sum_{k=1}^\\infty kq^{k-1} =\\sum_{k=0}^\\infty (k+1)q^{k}\n\\]\nこれを \\(r-1\\) 回繰り返すと\n\\[\n(r-1)!\\frac{1}{(1-q)^r} =  \\sum_{k=0}^\\infty \\frac{(k+r-1)!}{k!}q^{k}\n\\]\n従って，\nこれを確率関数の和に代入すると\n\\[\n\\begin{align*}\n\\sum_{x=0}^\\infty f_X(x)\n    &= \\sum_{x=0}^\\infty {}_{r-1 +x}C_x p^r(1-p)^x\\\\\n    &= p^r\\sum_{x=0}^\\infty {}_{r-1 + x}C_x (1-p)^x\\\\\n    &= p^r\\sum_{x=0}^\\infty \\frac{(r-1 + x)!}{(r-1)!x!}q^{x}\\\\\n    &= p^r\\frac{1}{(1-q)^r}\\\\\n    &= 1\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n▶  期待値の導出\n\\[\n\\begin{align*}\n\\sum_{x=0}^\\infty xf_X(x)\n    &= p^r\\sum_{x=0}^\\infty {}_{r+x-1}C_x x(1-p)^x\\\\\n    &= rp^rq\\sum_{x=1}^\\infty \\frac{(x+r-1)!}{(r)!(x-1)!}q^{x-1}\\\\\n    &= rp^rq\\sum_{k=0}^\\infty \\frac{(r+k)!}{(r)!k!}q^{k}\\\\\n    &= rp^rq\\frac{1}{(1-q)^{r+1}}\\\\\n    &= r\\frac{1-p}{p}\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n▶  微分を用いた期待値の導出\nEquation 12.1 の両辺を \\(q\\) について微分すると\nこれを整理すると\n\\[\nr\\frac{qp^r}{(1-q)^{r+1}} = \\sum_{k=0}^\\infty k\\frac{(k+r-1)!}{(r-1)!k!}q^{k}p^r\n\\]\nRHSは期待値と一致するので\n\\[\n\\mathbb E[X] = \\frac{r(1-p)}{p}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n▶  分散の導出\n\\[\n\\begin{align*}\n\\mathbb E[X(X-1)]\n    &= \\sum_{x=0}^\\infty x(x-1) \\frac{(r-1 + x)!}{(r-1)!x!}q^{x}p^r\\\\\n    &= p^rq^2 \\sum_{x=2}^\\infty\\frac{(r-1 + x)!}{(r-1)!(x-2)!}q^{x-2}\\\\\n    &= r(r+1)p^rq^2 \\sum_{k=0}^\\infty\\frac{(r+1 + k)!}{(r+1)!k!}q^{k}\\\\\n    &= r(r+1)q^2\\frac{p^r}{p^{r+2}}\\\\\n    &= r(r+1)\\frac{q^2}{p^2}\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\operatorname{Var}(X)\n    &= r(r+1)\\frac{q^2}{p^2} + r\\frac{1-p}{p}\\left[1 - r\\frac{1-p}{p}\\right]\\\\\n    &= r\\frac{(1-p)}{p^2}\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>負の二項分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/negative_binom.html#負の二項分布の性質",
    "href": "posts/probability_distribution/negative_binom.html#負の二項分布の性質",
    "title": "12  負の二項分布",
    "section": "",
    "text": "Def: 負の二項分布 \n確率変数 \\(X\\) が成功回数 \\(r\\in\\mathbb N_+\\), 成功確率 \\(0&lt;p&lt;1\\) の負の二項分布に従うとき，標本空間は \\(\\mathcal{X} = \\{0, 1, \\cdots, n\\}\\)，確率関数 \\(f_X(x)\\) は\n\\[\nf_X(x) = \\bigg\\{\\begin{array}{c}{}_{r+x-1}C_x p^r(1-p)^x & x \\in \\mathcal{X}\\\\0 & \\text{otherwise}\\end{array}\n\\]\nこのとき，\\(X\\sim \\operatorname{NBin}(r,p)\\) と表す．\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{1}{(1-q)^r} = \\sum_{k=0}^\\infty \\frac{(k+r-1)!}{(r-1)!k!}q^{k}\n\\tag{12.1}\\]\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align*}\nr\\frac{1}{(1-q)^{r+1}} = \\sum_{x=1}^\\infty x\\frac{(x+r-1)!}{(r-1)!x!}q^{x-1}\n\\end{align*}\n\\tag{12.2}\\]\n\n\n\n\n\n\n\n\n\n\n\n📘 REMARKS \n\n負の二項分布 \\(\\operatorname{NBin}(r,p)\\) の期待値と分散は，幾何分布 \\(\\operatorname{Geo}\\) の \\(r\\) 倍と思えば理解しやすいです\n\n\n\n\nTheorem 12.1 : 積率母関数 \n\\(X\\sim\\operatorname{NBin}(r, p)\\) の積率母関数は次のように表せる，\n\\[\nM_X(t) = p^r[1 - (1-p)e^t]^{-r} \\quad \\operatorname{s.t.} \\, t &lt; \\log\\frac{1}{1-p}\n\\]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{align*}\nM_X(t)\n    &= \\mathbb E[\\exp(tX)]\\\\\n    &= \\sum \\exp(tx)\\frac{(r-1+x)!}{(r-1)!x!}q^xp^r\n\\end{align*}\n\\]\nこのとき，\\(0 &lt; e^tq &lt; 1\\), つまり，\\(t &lt; \\log(\\frac{1}{q})\\) であるならば\n\\[\nM_X(t) = p^r [1 - e^tq]^{-r}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>負の二項分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/hypergeometric.html",
    "href": "posts/probability_distribution/hypergeometric.html",
    "title": "13  超幾何分布",
    "section": "",
    "text": "超幾何分布の性質\nツボに \\(K\\) 個の赤玉と \\(N-K\\) 個の白玉，つまり合計 \\(N\\) 個の玉が入っている中から，\\(n\\) 個の玉をランダムに 非復元(without replacement)で抽出するとする．このとき取り出した赤玉の個数を \\(X\\) としたとき，この \\(X\\) は超幾何分布 \\(\\operatorname{Hypergeometric}(N, K, n)\\) に従います．\n▶  確率関数の合計が1になることの証明\n恒等式\n\\[\n(1 + t)^N = (1 + t)^{K}(1 + t)^{N-K}\n\\]\nを考える．RHSを展開し，\\(t^n\\) の係数 \\(\\beta_n\\) を見てみると\n\\[\n\\begin{align*}\n\\beta_n = \\sum_{\\max(n+K-N, 0)}^{\\min(K, n)} {}_KC_{x} \\times {}_{N-K}C_{n-x}\n\\end{align*}\n\\]\n一方，LHSでみると\n\\[\n\\beta_n = {}_NC_n\n\\]\n従って，\n\\[\n\\begin{gather*}\n\\sum_{\\max(n+K-N, 0)}^{\\min(K, n)} {}_KC_{x} \\times {}_{N-K}C_{n-x} = {}_NC_n\\\\\n\\Rightarrow \\sum_{\\max(n+K-N, 0)}^{\\min(K, n)}\\Pr(X=k) = 1\n\\end{gather*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>超幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/hypergeometric.html#超幾何分布の性質",
    "href": "posts/probability_distribution/hypergeometric.html#超幾何分布の性質",
    "title": "13  超幾何分布",
    "section": "",
    "text": "Def: 超幾何分布 \nParameters \\((N, K, n)\\) の超幾何分布に従う確率変数 \\(X\\) について，その確率関数は\n\\[\n\\begin{gather*}\n\\Pr(X = x) = \\frac{{}_KC_x \\cdot {}_{N-K}C_{n-x}}{{}_NC_n}\\\\\n\\text{where} \\max\\{0, n+K-N\\} \\leq x \\leq \\min\\{n, K\\}\n\\end{gather*}\n\\]\nまた \\(\\max\\{0, n+K-N\\} \\leq x \\leq \\min\\{n, K\\}\\) の範囲外の \\(x\\) については \\(\\Pr(X = x) = 0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 13.1 期待値 \n確率変数 \\(X \\sim \\operatorname{Hypergeometric}(N, K, n)\\) について\n\\[\n\\mathbb E[X] = n\\frac{K}{N}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\left(\\begin{array}{c}n\\\\k\\end{array}\\right)= \\frac{n}{k} \\left(\\begin{array}{c}n-1\\\\ k-1\\end{array}\\right)\n\\]\nという関係式をもちいると\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_x\\frac{x \\left(\\begin{array}{c}K\\\\ x\\end{array}\\right)\\left(\\begin{array}{c}N-K\\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N\\\\ n\\end{array}\\right)}\\\\\n    &= \\frac{nK}{N}\\sum_x\\frac{ \\left(\\begin{array}{c}K-1\\\\ x-1\\end{array}\\right)\\left(\\begin{array}{c}N-K\\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N-1\\\\ n-1\\end{array}\\right)}\\\\\n    &= \\frac{nK}{N}\\sum_x\\frac{ \\left(\\begin{array}{c}K-1\\\\ x-1\\end{array}\\right)\\left(\\begin{array}{c}N-1-(K-1)\\\\ n-1 - (x-1)\\end{array}\\right)}{\\left(\\begin{array}{c}N-1\\\\ n-1\\end{array}\\right)}\n\\end{align*}\n\\]\n最後の式変形は，ツボに \\(K-1\\) 個の赤玉と \\(N-1 - (K-1)\\) 個の白玉，つまり合計 \\(N-1\\) 個の玉が入っている中から \\(n-1\\) 個のボールを選ぶ場合の確率関数と同じなので\n\\[\n\\begin{align*}\n\\sum_x\\frac{ \\left(\\begin{array}{c}K-1\\\\ x-1\\end{array}\\right)\\left(\\begin{array}{c}N-1-(K-1)\\\\ n-1 - (x-1)\\end{array}\\right)}{\\left(\\begin{array}{c}N-1\\\\ n-1\\end{array}\\right)} = 1\n\\end{align*}\n\\]\n従って，\\(\\displaystyle\\mathbb E[X] = \\frac{nK}{N}\\) を得る．\n\n\n\n\n\nTheorem 13.2 分散 \n確率変数 \\(X \\sim \\operatorname{Hypergeometric}(N, K, n)\\) について\n\\[\n\\operatorname{Var}(X) = n\\frac{K}{N}\\frac{N-K}{N}\\frac{N-n}{(N-1)}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\operatorname{Var}(X) = \\mathbb E[X(X-1)] + E[X](1 - E[X])\n\\]\nなので，\\(\\mathbb E[X(X-1)]\\) がわかれば良い．\n\\[\n\\begin{align*}\n\\mathbb E[X(X-1)]\n    &= \\sum_{x}\\frac{x(x-1)\\left(\\begin{array}{c}K \\\\ x\\end{array}\\right)\\left(\\begin{array}{c}N-K \\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N \\\\ n\\end{array}\\right)}\\\\\n    &= \\frac{n(n-1)K(K-1)}{N(N-1)}\\sum_{x}\\frac{\\left(\\begin{array}{c}K-2 \\\\ x-2\\end{array}\\right)\\left(\\begin{array}{c}(N-2) - (K-2) \\\\ (n-2)-(x-2)\\end{array}\\right)}{\\left(\\begin{array}{c}N -2\\\\ n - 2\\end{array}\\right)}\\\\\n    &= \\frac{n(n-1)K(K-1)}{N(N-1)}\\sum_{l=x-2}\\frac{\\left(\\begin{array}{c}K-2 \\\\ l\\end{array}\\right)\\left(\\begin{array}{c}(N-2) - (K-2) \\\\ (n-2)-l\\end{array}\\right)}{\\left(\\begin{array}{c}N -2\\\\ n - 2\\end{array}\\right)}\\\\\n    &= \\frac{n(n-1)K(K-1)}{N(N-1)}\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(X) = n\\frac{K}{N}\\frac{N-K}{N}\\frac{N-n}{N-1}\n\\]\n\n\n\n\n📘 REMARKS \n\n\\(\\frac{N-n}{N-1}\\) は有限母集団修正と呼ばれる\n\n\n\n超幾何分布の極限と二項分布\n確率変数 \\(X \\sim \\operatorname{Hypergeometric}(N, K, n)\\) について, \\(\\frac{K}{N} = p \\text{ as } N, K \\to\\infty\\) が極限において 成立するとします．\n\\[\n\\begin{align*}\n&\\Pr(X = x)\\\\\n    &= \\frac{\\left(\\begin{array}{c}K\\\\ x\\end{array}\\right)\\left(\\begin{array}{c}N-K\\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N\\\\n\\end{array}\\right)}\\\\\n    &= \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)\\frac{K!}{(K-x)!}\\frac{(N-k)!}{(N-K-n+x)!}\\frac{(N-n)!}{N}\\\\\n    &= \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)\\frac{K(K-1)\\cdots(K-x+1)}{N(N-1)\\cdots(N-x+1)}\\frac{(N-K)\\cdots(N-K-(n-x)+1)}{(N-x)\\cdots(N-x+1)}\n\\end{align*}\n\\]\nこのとき，\\(p = K/N\\) とすると，極限において\n\\[\n\\begin{gather*}\n\\frac{K(K-1)\\cdots(K-x+1)}{N(N-1)\\cdots(N-x+1)} \\approx p^x\\\\\n\\frac{(N-K)\\cdots(N-K-(n-x)+1)}{(N-x)\\cdots(N-x+1)}\\approx (1-p)^{n-x}\n\\end{gather*}\n\\]\n以上より\n\\[\n\\lim_{N,K\\to\\infty}\\Pr(X=x) = \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)p^x(1-p)^{n-x}\n\\]\n\n\n有限母集団からの非復元抽出と有限母集団修正\n有限母集団からの復元抽出は，i.i.d.確率変数が観測されるが，非復元抽出の場合は i.i.d.となりません． 大きさ \\(N\\) の有限母集団を考え，\\(X_i\\) を標本として抽出された観測値とします．なお，有限母集団に属する各個体の個体値を，\n\\[\na_1, a_2, \\cdots, a_N\n\\]\nとします．サイズ \\(n\\) の非復元抽出は，任意の互いに異なる \\(i_1, \\cdots, i_n\\) について，以下のように確率が定義される標本抽出方法です：\n\\[\n\\begin{align*}\n\\Pr(X_1 = a_{i_1}, \\cdots, X_1 = a_{i_n}) = \\frac{1}{N(N-1)\\cdots(N-n+1)}\n\\end{align*}\n\\]\n ▶  有限母集団の平均と分散\n有限母集団の平均と分散は\n\\[\n\\begin{gather*}\n\\mu = \\frac{1}{N}\\sum_{i=1}^Na_i\\\\\n\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N(a_i - \\mu)^2\n\\end{gather*}\n\\]\nと定義できます．\n ▶  標本平均の期待値と分散\n\\[\n\\begin{align*}\n\\mathbb E[\\overline{{X}}]\n    &= \\frac{1}{n}\\sum_{i=1}^n\\mathbb E[X_i]\\\\\n    &= \\mu\\\\\n\\\\\n\\operatorname{Var}(\\overline{{X}})\n    &= \\frac{1}{n^2}\\operatorname{Var}(\\sum_{i=1}^nX_i)\\\\\n    &=\\frac{1}{n^2}\\left[\\sum_{i=1}^n\\operatorname{Var}(X_i) + \\sum_{i\\neq j}\\operatorname{Cov}(X_i, X_j)\\right]\\\\\n    &=\\frac{1}{n^2}\\left[n\\operatorname{Var}(X_1) + n(n-1)\\operatorname{Cov}(X_1, X_2)\\right]\n\\end{align*}\n\\]\nここで，\n\\[\n\\begin{align*}\n&\\operatorname{Cov}(X_1, X_2)\\\\\n    &= \\mathbb E[X_1X_2] - \\mathbb E[X_1]\\mathbb E[X_2]\\\\\n    &= \\frac{1}{N(N-1)}\\sum_{i\\neq j}a_ia_j - \\left(\\frac{1}{N}\\sum_{i=1}^Na_i\\right)^2\\\\\n    &= \\frac{1}{N(N-1)}\\left[(\\sum_{i=1}^Na_i)^2 - \\sum_{i=1}^Na_i^2\\right]- \\left(\\frac{1}{N}\\sum_{i=1}^Na_i\\right)^2\\\\\n    &= \\frac{(\\sum_{i=1}^Na_i)^2}{N^2(N-1)} - \\frac{\\sum_{i=1}^Na_i^2}{N(N-1)}\\\\\n    &= -\\frac{1}{N(N-1)}\\left(\\sum_{i=1}^Na_i^2 - \\frac{1}{N}(\\sum_{i=1}^Na_i)^2\\right)\\\\\n    &= -\\frac{1}{N(N-1)}\\sum_{i=1}^N(a_i - \\overline(a))^2\\\\\n    &= -\\frac{\\sigma^2}{N-1}\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(\\overline{{X}}) = \\frac{\\sigma^2}{n}\\frac{N-n}{N-1}\n\\]\n\n📘 REMARKS \nサイズ \\(N\\) の有限母集団からサイズ \\(n\\) の非復元無作為抽出を実施する場合，\n\\[\n\\{a_{i_1}, \\cdots, a_{i_n}\\}\n\\]\nという要素の重複を許した多重集合を一度抽出し，そこから改めて１個ずつ順に無作為に抜き出すという方法でも同じ抽出方法となります． ここから，\\(X_i\\) の周辺分布は \\(X_1\\) の周辺分布と同じになることが分かるし，また，\\((X_i, X_j)\\) の２次元同時分布は \\((X_1, X_2)\\) の２次元同時分布と同じであることがわかります．\n\n ▶  別解: 非復元抽出における \\(\\operatorname{Cov}(X_1, X_2)\\) の求め方\n\\[\n\\sum_{i=1}^N X_i = \\sum_{i=1}^N a_i\n\\]\nと定数であるので，\\(\\operatorname{Var}(\\sum_{i=1}^N X_i) = 0\\) となる．\n\\[\n\\begin{align*}\n\\operatorname{Var}(\\sum_{i=1}^N X_i)\n    &= \\sum_{i=1}^N \\operatorname{Var}(X_1) + \\sum_{i\\neq j}\\operatorname{Cov}(X_1, X_2)\\\\\n    &= N\\sigma^2 + N(N-1)\\operatorname{Cov}(X_1, X_2) = 0\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Cov}(X_1, X_2) = -\\frac{\\sigma^2}{N-1}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\nExample 13.1 : 有限母集団修正 \n\n\n\nID\n身長\n\n\n\n\n1\n171.0\n\n\n2\n167.3\n\n\n3\n170.6\n\n\n4\n178.7\n\n\n5\n162.3\n\n\n\n上記のような５観測単位からなる有限母集団を考える．このとき，３人を非復元抽出でサンプリングしたとき， その標本平均の平均と分散は以下のようになる\n\nimport numpy as np\nfrom itertools import combinations as comb\n\nfinite_population = np.array([171.0, 167.3, 170.6, 178.7, 162.3])\nmeans_of_comb = list(\n    map(lambda x: np.mean(finite_population[np.array(x)]), comb(range(0, 5), 3))\n)\n\nprint(\"mean: {:.2f}, var: {:.2f}\".format(np.mean(means_of_comb), np.var(means_of_comb)))\n\nmean: 169.98, var: 4.79\n\n\n有限母集団修正を考えずに計算してみると\n\nbiased_var = np.var(finite_population) / 3\ncorrection = (5 - 3) / (5 - 1)\n\nprint(\n    \"biased-var: {:.2f}, corrected-var:{:.2f}\".format(\n        biased_var, correction * biased_var\n    )\n)\n\nbiased-var: 9.58, corrected-var:4.79\n\n\n以上のように，標本平均の分散について，有限母集団修正により正しい値が得られることが分かる．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>超幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/hypergeometric.html#多変量超幾何分布",
    "href": "posts/probability_distribution/hypergeometric.html#多変量超幾何分布",
    "title": "13  超幾何分布",
    "section": "多変量超幾何分布",
    "text": "多変量超幾何分布\n\nDef: 多変量超幾何分布 \n各個体が \\(c_1, c_2, \\cdots, c_k\\) のいずれかに所属するようなクラスサイズ \\(N\\) 有限母集団を考える(各 \\(c_i\\) のサイズは \\(C_i\\) とする)．つまり，\n\\[\n\\sum_{j=1}^kC_j = N\n\\]\nこの有限母集団から，サイズ \\(n\\) の非復元無作為抽出をする場合，その同時確率関数は\n\\[\n\\begin{gather*}\n\\Pr(X_1=x_1, \\cdots, X_k=x_k) = \\frac{\\left(\\begin{array}{c}C_1\\\\x_1 \\end{array}\\right)\\left(\\begin{array}{c}C_2\\\\x_2 \\end{array}\\right)\\cdots\\left(\\begin{array}{c}C_k\\\\ x_k \\end{array}\\right)}\n{\\left(\\begin{array}{c}N\\\\n \\end{array}\\right)}\\\\\n\\text{where } \\sum_{i=1}^kx_i = n\n\\end{gather*}\n\\]\nとなる．このとき，\\(k\\) 次元確率変数ベクトル \\(X\\) は \\(\\operatorname{Multi-hypergeometric}(N, (C_1, \\cdots, C_k), n)\\) に従う．\n\n ▶  周辺確率分布\n多変量超幾何分布の \\(X_i\\) についての周辺確率分布は，\\(X_i\\) 以外のグループをまとめてシンプルな超幾何分布とみなして考えることができるので\n\\[\n\\begin{gather*}\n\\Pr(X_i=x) = \\frac{\\left(\\begin{array}{c}C_i\\\\ x\\end{array}\\right)\\left(\\begin{array}{c}N-C_i\\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N\\\\ n\\end{array}\\right)}\\\\\n\\text{where } \\max\\{0, n+C_i-N\\} \\leq x \\leq \\min\\{n, C_i\\}\n\\end{gather*}\n\\]\n ▶  期待値と分散\n\\[\n\\begin{gather*}\n\\mathbb E[X_i] = n\\frac{C_i}{N}\\\\\n\\operatorname{Var}(X_i) = n\\frac{C_i}{N}\\frac{N-C_i}{N}\\frac{N-n}{N-1}\\\\\n\\operatorname{Cov}(X_i, X_j) = -n\\frac{N-n}{N-1}\\frac{C_i}{N}\\frac{C_j}{N}\n\\end{gather*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>超幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/hypergeometric.html#references",
    "href": "posts/probability_distribution/hypergeometric.html#references",
    "title": "13  超幾何分布",
    "section": "References",
    "text": "References\n\nLibreTexts Statistics &gt; The Multivariate Hypergeometric Distribution",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>超幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/continuous_uniform_dis.html",
    "href": "posts/probability_distribution/continuous_uniform_dis.html",
    "title": "14  連続一様分布",
    "section": "",
    "text": "一様分布の性質\n▶  期待値の計算\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\int^\\infty_{-\\infty}xf_X(x)\\,\\mathrm{d} x\\\\\n    &= \\int^b_a \\frac{1}{b-a}x\\mathrm{d} x\\\\\n    &= \\frac{a+b}{2}\n\\end{align*}\n\\]\n▶  分散の計算\n\\[\n\\begin{align*}\n\\mathbb E[X^2]\n    &= \\int^\\infty_{-\\infty}x^2f_X(x)\\,\\mathrm{d} x\\\\\n    &= \\int^b_a \\frac{1}{b-a}x^2\\mathrm{d} x\\\\\n    &= \\frac{1}{b-a}\\left[\\frac{x^3}{3}\\right]^b_a\\\\\n    &= \\frac{a^2+ab+b^2}{3}\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(X) = \\frac{(b-a)^2}{12}\n\\]\n▶  積率母関数の導出\n積率母関数については，\\(t\\neq 0\\) であれば\n\\[\n\\begin{align*}\nM_X(t)\n    &= \\int^b_a \\frac{1}{b-a}\\exp(tx)\\,\\mathrm{d} x\\\\\n    &= \\frac{1}{b-a}\\left[\\frac{1}{t}\\exp(tx)\\right]^b_a\\\\\n    &=\\frac{e^{tb} - e^{ta}}{(b-a)t}\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>連続一様分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/continuous_uniform_dis.html#一様分布の性質",
    "href": "posts/probability_distribution/continuous_uniform_dis.html#一様分布の性質",
    "title": "14  連続一様分布",
    "section": "",
    "text": "Def: 一様分布 \n確率変数 \\(X\\) がパラメータ \\(a, b\\) の一様分布に従うとき，\\(X\\sim\\operatorname{U}[a,b]\\) と表す．\\(X\\) の値域は\n\\[\nX(\\Omega)= [a, b]\n\\]\nであり，確率密度関数 \\(f_X(x)\\) は\n\\[\nf_X(x) = \\bigg\\{\\begin{array}{c}\\frac{1}{b-a} & x\\in [a, b] \\\\ 0 & \\text{otherwise}\\end{array}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 14.1 : 一様分布の累積分布関数 \n確率変数 \\(X \\operatorname{U}[a, b]\\) の累積分布関数 \\(F_X(x)\\) は\n\\[\nF_X(x)\n    = \\left\\{\\begin{array}{c}\n    0 & \\text{if} x &lt; a\\\\\n    \\frac{x-a}{b-a} & \\text{if} x \\in [a, b]\\\\\n    1& \\text{if} x &gt; b\n    \\end{array}\\right.\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(x&lt;a\\) のとき，\\(X\\) の値域は \\([a, b]\\) であるので\n\\[\nF_X(a) = \\Pr(X \\leq a) = 0\n\\]\n\\(x\\in [a,b]\\) において，\n\\[\n\\begin{align*}\nF_X(x)\n    &= \\int^x_a f_X(t)\\,\\mathrm{d}t\\\\\n    &= \\frac{x-a}{b-a}\n\\end{align*}\n\\]\n\\(x&gt;b\\) のとき，上記より \\(F_X(b) = 1\\).\n\n\n\n\n\nTheorem 14.2 : 一様分布の微分エントロピー \n確率変数 \\(X \\operatorname{U}[a, b]\\) の微分エントロピーは\n\\[\n\\operatorname{H}(x) = \\ln (b-a)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\operatorname{H}(x)\n    &= -\\int f_X(x)\\ln(f_X(x))\\,\\mathrm{d} x\\\\\n    &= \\frac{\\ln(b-a)}{b-a} \\int^b_a\\,\\mathrm{d} x\\\\\n    &= \\ln (b-a)\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>連続一様分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/continuous_uniform_dis.html#他の確率分布との関係性",
    "href": "posts/probability_distribution/continuous_uniform_dis.html#他の確率分布との関係性",
    "title": "14  連続一様分布",
    "section": "他の確率分布との関係性",
    "text": "他の確率分布との関係性\n\n\nTheorem 14.3 : ベータ分布の特殊ケースとしての一様分布 \nパラメータ \\((\\alpha, \\beta)=(1,1)\\) のベータ分布は標準一様分布と等しい．\n\\[\n\\operatorname{Be}(1, 1) = \\operatorname{U}[0, 1]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n確率変数 \\(X\\sim\\operatorname{Be}(\\alpha, \\beta)\\) の確率密度関数は,\n\\[\nf_X(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\quad (0 &lt; x &lt; 1)\n\\]\n\\(\\alpha=\\beta=1\\) を代入すると，\n\\[\n\\begin{align*}\nf_X(x)\n    &= \\frac{\\Gamma(2)}{\\Gamma(1)\\Gamma(1)}x^{1-1}(1-x)^{1-1}\\\\\n    &= 1\n\\end{align*}\n\\]\n従って，\\(\\operatorname{U}[0, 1]\\) の確率密度関数と一致することがわかります．\n\n\n\n\n\n\n\n\n\nProof: MGFを用いた証明\n\n\n\n\n\n確率変数 \\(X\\sim\\operatorname{Be}(\\alpha, \\beta)\\) のMGFは\n\\[\n\\label{eq:beta-mgf-s1}\n\\begin{split}\nM_X(t) &= \\int_{0}^{1} \\exp[tx] \\cdot \\frac{1}{\\mathrm{B}(\\alpha, \\beta)} \\, x^{\\alpha-1} \\, (1-x)^{\\beta-1} \\, \\mathrm{d}x \\\\\n&= \\frac{1}{\\mathrm{B}(\\alpha, \\beta)} \\int_{0}^{1} e^{tx} \\, x^{\\alpha-1} \\, (1-x)^{\\beta-1} \\, \\mathrm{d}x\n\\end{split}\n\\]\n\\(\\alpha=\\beta=1\\) を代入すると，\n\\[\n\\begin{align*}\nM_X(t)\n    &= \\frac{1}{\\mathrm{B}(1, 1)} \\int_{0}^{1} e^{tx} \\, x^{1-1} \\, (1-x)^{1-1} \\, \\mathrm{d}x \\\\\n    &= \\frac{1}{t}[e-1]\n\\end{align*}\n\\]\nこれは　\\(\\operatorname{U}[0, 1]\\) のMGFと一致するので，パラメータ \\((\\alpha, \\beta)=(1,1)\\) のベータ分布は標準一様分布と等しいことがわかります．\n\n\n\n\n\nTheorem 14.4 : 一様分布の対数変換と指数分布 \n確率変数 \\(X \\sim \\operatorname{U}(0, 1)\\) について，\n\\[\nY = -\\log(X)\n\\]\nという変数変換を考える．このとき，\\(Y\\sim\\operatorname{Exp}(1)\\) が成り立つ．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(Y = -\\log(X)\\) とするとその定義域は \\([0, \\infty)\\) となります．対数変換は単調増加関数なので，\\(y\\in[0, \\infty)\\) の範囲で\n\\[\nf_Y(y) = f_X(\\exp(-y))\\times \\left\\vert\\frac{\\,\\mathrm{d}\\exp(-y)}{\\,\\mathrm{d}y}\\right\\vert = \\exp(-y)\n\\]\nこれは \\(\\operatorname{Exp}(1)\\) の確率密度関数と一致するので，\\(Y\\sim\\operatorname{Exp}(1)\\) が成り立つ．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>連続一様分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/normal_dist.html",
    "href": "posts/probability_distribution/normal_dist.html",
    "title": "15  正規分布",
    "section": "",
    "text": "正規分布の性質\n\\(X\\sim N(\\mu, \\sigma^2)\\) について，標準化変換(standardization)\n\\[\nZ = \\frac{X-\\mu}{\\sigma}\n\\]\nを行うと，変数変換の公式より\n\\[\nf_Z(z) = \\sigma f_X(\\sigma z + \\mu)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\n\\]\nとなります．\\(N(0, 1)\\) のことを特に標準正規分布とよび，そのpdfを \\(\\phi(z)\\), CDFを \\(\\Phi(z)\\) で表します．\n\\(f_X(x)\\) の形状から，location parameter \\(\\mu\\) を中心に対称であることが分かる．つまり, \\(\\phi(z)\\) は \\(z=0\\) で対称であり\n\\[\n\\begin{gather*}\n\\Phi(0) = \\frac{1}{2}\\\\\n\\Phi(-z) = 1 - \\Phi(z)\n\\end{gather*}\n\\]\nがわかる．\n▶  \\(\\sigma\\) 範囲\n\\(X\\sim N(\\mu, \\sigma^2)\\) という確率分布を考えたとき，シグマ範囲の目安として以下が知られてます\n\\[\n\\begin{gather*}\n\\Pr(\\vert X - \\mu\\vert &gt; \\sigma) \\approx \\frac{1}{3}\\\\\n\\Pr(\\vert X - \\mu\\vert &gt; 2\\sigma) \\approx \\frac{1}{20}\\\\\n\\Pr(\\vert X - \\mu\\vert &gt; 3\\sigma) \\approx \\frac{3}{1000}\\\\\n\\Pr(\\vert X - \\mu\\vert &gt; 4\\sigma) \\approx \\frac{1}{10000}\n\\end{gather*}\n\\]\n大体の目安として, \\(3\\sigma\\) 範囲はいわゆる「千三つ」であることは覚えといて損はないと思います．\n▶  標準正規分布のn次モーメントについて\n標準正規分布について，\\(\\phi(z)\\) が偶関数であることから，\\(n\\) が奇数のときは\n\\[\n\\mathbb E[X^n] = 0\n\\]\nであることはすぐに分かります．一方，\\(l\\) を自然数として, \\(n=2l\\) と表せるときは\n\\[\n\\begin{align*}\n\\mathbb E[X^n] &= \\frac{1}{\\sqrt{2\\pi}}\\int^{\\infty}_{-\\infty}x^n\\exp\\bigg(-\\frac{x^2}{2}\\bigg)\\mathrm{d}x\\\\[3pt]\n               &= \\frac{2}{\\sqrt{2\\pi}}\\int^{\\infty}_{0}x^{2l}\\exp\\bigg(-\\frac{x^2}{2}\\bigg)\\mathrm{d}x\\\\[3pt]\n\\end{align*}\n\\]\n\\(x^2/2 = u\\) と変数変換を行うと，\\(\\frac{\\mathrm{d}x}{\\mathrm{d}u}=\\frac{1}{\\sqrt{2u}}\\) より\n\\[\n\\begin{align*}\n\\mathbb E[X^n] &= \\frac{2^{l+1}}{\\sqrt{2\\pi}}\\int^{\\infty}_{0}u^{l}\\exp(-u)\\frac{1}{\\sqrt{2u}}\\mathrm{d}u\\\\[3pt]\n               &= \\frac{2^l}{\\sqrt{\\pi}}\\int^{\\infty}_{0}u^{l-\\frac{1}{2}}\\exp(-u)\\mathrm{d}u\\\\[3pt]\n               &= \\frac{2^l}{\\sqrt{\\pi}}\\int^{\\infty}_{0}u^{l+\\frac{1}{2}-1}\\exp(-u)\\mathrm{d}u\\\\[3pt]\n               &= \\frac{2^l}{\\sqrt{\\pi}}\\Gamma\\bigg(l+\\frac{1}{2}\\bigg)\n\\end{align*}\n\\]\n\\(\\Gamma(1/2) = \\sqrt{\\pi}\\) であることに留意すると\n\\[\n\\begin{align*}\n\\Gamma\\left(\\frac{3}{2}\\right) &= \\frac{1}{2}\\Gamma\\left(\\frac{1}{2}\\right)\\\\\n\\Gamma\\left(\\frac{5}{2}\\right) &= \\frac{3}{2}\\Gamma\\left(\\frac{3}{2}\\right)\\\\\n                               &= \\frac{1 \\times 3}{2^2}\\sqrt{\\pi}\n\\end{align*}\n\\]\nになるので\n\\[\n\\begin{align*}\n\\mathbb E[X^n]\n    &= 1\\times 3\\times 5\\times\\cdots\\times (2l-1)\\\\\n    &= \\prod_{i=1}^l(2i-1)           \n\\end{align*}\n\\]\nまたはこれを変形して，\n\\[\n\\frac{1}{2l!}\\mathbb E[X^{2l}] = \\prod^l_{k=1}\\frac{1}{2k}\n\\]\nと表すこともできます．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>正規分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/normal_dist.html#正規分布の性質",
    "href": "posts/probability_distribution/normal_dist.html#正規分布の性質",
    "title": "15  正規分布",
    "section": "",
    "text": "Def: 正規分布 \n確率変数 \\(X\\) が平均と分散 \\(\\mu, \\sigma^2\\) をもつ正規分布に従う，つまり \\(X \\sim N(\\mu, \\sigma^2)\\) のとき，\\(X\\) の確率密度関数 \\(f_X(x)\\) は\n\\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}, \\quad -\\infty&lt;x&lt;\\infty\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 15.1 \n\\[\n\\int^\\infty_{-\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\mathrm{d}x = 1\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(z = \\frac{x-\\mu}{\\sigma}\\) と変数変換をすると\n\\[\n\\int^\\infty_{-\\infty}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\\mathrm{d}z = 1\n\\]\nが示せれば良い．\n\\[\nI = \\int^\\infty_{-\\infty}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\\mathrm{d}z\n\\]\nとおくと，\n\\[\n\\begin{align*}\nI^2 &= \\left(\\int^\\infty_{-\\infty}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\\mathrm{d}z\\right)^2\\\\\n    &= \\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}\\exp\\left\\{-\\frac{a^2 + b^2}{2}\\right\\}\\mathrm{d}a\\mathrm{d}b\n\\end{align*}\n\\]\nここで，\\(a=r\\cos\\theta, b= r\\sin\\theta\\) と極座標変換を行う. ヤコビアン \\(J\\) は\n\\[\n\\begin{align*}\n\\vert J\\vert\n    &= \\bigg\\vert \\frac{\\partial a}{\\partial r}\\frac{\\partial b}{\\partial \\theta} - \\frac{\\partial a}{\\partial \\theta}\\frac{\\partial b}{\\partial r}\\bigg\\vert\\\\\n    &= r\n\\end{align*}\n\\]\nより \\[\n\\begin{align*}\nI^2 &= \\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}\\exp\\left\\{-\\frac{a^2 + b^2}{2}\\right\\}\\mathrm{d}a\\mathrm{d}b\\\\\n    &= \\int^\\infty_{0}\\int^{2\\pi}_{0}\\exp\\left\\{-\\frac{r^2}{2}\\right\\}r \\mathrm{d}\\theta\\mathrm{d}r\\\\\n    &= 2\\pi \\int^\\infty_{0}\\exp\\left\\{-\\frac{r^2}{2}\\right\\}r \\mathrm{d}r\\\\\n    &= 2\\pi \\left[\\exp\\left\\{-\\frac{r^2}{2}\\right\\}\\right]^0_\\infty\\\\\n    &= 2\\pi\n\\end{align*}\n\\]\n以上より, \\(I = \\sqrt{2\\pi}\\) を得る．\n\\[\n\\begin{align*}\n\\int^\\infty_{-\\infty}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\\mathrm{d}z\n    &= \\frac{1}{\\sqrt{2\\pi}}I\\\\\n    &= 1\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 15.2 \n標準正規分布の確率密度関数を \\(\\phi(x)\\) とするとき，\n\\[\n\\begin{gather*}\n\\int_\\mathbb R x\\phi(x) \\mathrm{d}x = 0\\\\\n\\int_\\mathbb R x^2\\phi(x) \\mathrm{d}x = 1\\\\\n\\end{gather*}\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\phi(x)\\) が偶関数，\\(x\\) が奇関数より, \\(x\\phi(x)\\) は奇関数になる．従って，\n\\[\n\\int_\\mathbb R x\\phi(x) \\mathrm{d}x = 0\n\\]\n2次モーメントについては\n\\[\n\\begin{align*}\n\\int_\\mathbb R x^2\\phi(x) \\mathrm{d}x\n    &=  2\\int_0^\\infty x^2\\phi(x) \\mathrm{d}x\\\\\n    &= 2[-x\\exp(-x^2/2)]^\\infty_0 + 2\\int^\\infty_0 \\phi(x) \\mathrm{d}x\\\\\n    &= 0 + 2 \\times \\frac{1}{2}\\\\\n    &= 1\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 15.3 \n確率変数 \\(X\\sim N(\\mu, \\sigma^2)\\) のとし，\\(f(x)\\) をその確率密度関数とする，このとき，\n\\[\n\\begin{gather*}\n\\int_\\mathbb R xf(x;\\mu,\\sigma^2) \\mathrm{d}x = \\mu\\\\\n\\int_\\mathbb R (x-\\mu)^2f(x;\\mu,\\sigma^2) \\mathrm{d}x = \\sigma^2\\\\\n\\end{gather*}\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n ▶  location parameterについて\n\\(z = x - \\mu\\) と変数変換をすると，\\(\\frac{\\mathrm{d}}{\\mathrm{d}z}x = 1\\) より\n\\[\n\\begin{align*}\n\\int_\\mathbb R xf(x;\\mu,\\sigma^2) \\mathrm{d}x\n    =& \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb R \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)x\\,\\mathrm{d}x\\\\\n    =& \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb R \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)(z +\\mu)\\,\\mathrm{d}z\\\\\n    =& \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb R \\underbrace{\\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)z\\mathrm{d}z}_{\\text{偶関数}\\times\\text{奇関数}}\\\\\n     &+ \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb R \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)\\mu\\mathrm{d}z\\\\\n    =& \\mu\n\\end{align*}\n\\]\n ▶  scale parameterについて\n確率密度関数より\n\\[\n\\int_\\mathbb R \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,\\mathrm{d}x = \\sqrt{2\\pi\\sigma^2}\n\\]\n両辺について \\(\\sigma^2\\) で微分すると\n\\[\n\\begin{align*}\n\\text{RHS}\n    &= \\frac{1}{2}\\sqrt{2\\pi}(\\sigma^2)^{-\\frac{1}{2}}\\\\\n\\text{LHS}\n    &= \\int_\\mathbb R \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\frac{(x-\\mu)^2}{2}(\\sigma^2)^{-2}\\,\\mathrm{d}x\n\\end{align*}\n\\]\nこれを整理すると\n\\[\n\\begin{align*}\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb R \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)(x-\\mu)^2\\,\\mathrm{d}x = \\sigma^2\n\\end{align*}\n\\]\nこれは\n\\[\n\\mathbb E[(X -\\mu)^2] = \\sigma^2\n\\]\nに相当する．\n\n\n\n\nExample 15.1 : 標準正規分布の4次モーメントの導出 \n\\(X\\sim N(0, 1)\\) の4次モーメントについて，ガンマ関数 \\(\\Gamma(1/2) = \\sqrt{\\pi}\\) を用いて以下のように計算できます．\n\\[\n\\begin{align*}\n\\mathbb E[X^4]\n    &= \\frac{1}{\\sqrt{2\\pi}}\\int^\\infty_{-\\infty}x^4\\exp\\left(-\\frac{x^2}{2}\\right)\\mathrm{d}x\\\\\n    &= \\frac{2}{\\sqrt{2\\pi}}\\int^\\infty_{0}x^4\\exp\\left(-\\frac{x^2}{2}\\right)\\mathrm{d}x \\quad\\because \\text{偶関数より}\n\\end{align*}\n\\]\nここで, \\(x^2 = u\\) という変数変換を行う．\n\\[\n\\begin{align*}\n\\frac{2}{\\sqrt{2\\pi}}\\int^\\infty_{0}x^4\\exp\\left(-\\frac{x^2}{2}\\right)\\mathrm{d}x\n    &= \\frac{2}{\\sqrt{2\\pi}}\\int^\\infty_{0}u^2 \\exp\\left(-\\frac{u}{2}\\right)\\frac{u^{-1/2}}{2}\\mathrm{d}u\\\\\n    &= \\frac{1}{\\sqrt{2\\pi}}\\int^\\infty_{0}u^{\\frac{5}{2}-1} \\exp\\left(-\\frac{u}{2}\\right)\\mathrm{d}u\\\\\n    &= \\frac{1}{\\sqrt{2\\pi}} \\Gamma\\left(\\frac{5}{2}\\right)\\left(\\frac{1}{2}\\right)^{-5/2}\\\\\n    &= \\frac{\\sqrt{\\pi} \\frac{1}{2}\\frac{3}{2}}{\\sqrt{2\\pi}}2^{5/2}\\\\\\\\\n    &= 3\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMGFと特性関数\n\n\nTheorem 15.4 : 標準正規分布の積率母関数と特性関数 \n\\(X\\sim N(0, 1)\\) としたとき，積率母関数 \\(M_Z(t)\\) 及び，特性関数 \\(\\varphi_Z(t)\\) は以下のようになる\n\\[\n\\begin{align*}\nM_Z(t) &= \\exp(t^2/2)\\\\\n\\varphi_Z(t)&= \\exp(-t^2/2)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nM_Z(t)\n    &= \\mathbb E[\\exp(tZ)]\\\\\n    &= \\frac{1}{\\sqrt{2\\pi}}\\int^\\infty_{-\\infty}\\exp(tz-z^2/2)\\mathrm{d}z\\\\\n    &= \\exp(t^2/2)\\frac{1}{\\sqrt{2\\pi}}\\int^\\infty_{-\\infty}\\exp(-(t-z)^2/2)\\mathrm{d}z\\\\\n    &= \\exp(t^2/2)\n\\end{align*}\n\\]\n特性関数は \\(\\varphi(t) = M_Z(it)\\) より \\(\\varphi(t) = \\exp(-t^2/2)\\) とわかるが，以下のように計算することもできる．\n\\[\n\\begin{align*}\n\\varphi_Z(t)\n    &= \\mathbb E[\\exp(itZ)]\\\\[5pt]\n    &= \\mathbb E[\\cos(tZ) + i\\sin(tZ)]\\\\[5pt]\n    &= \\mathbb E[\\cos(tZ)] + i\\mathbb E[\\sin(tZ)]\\\\[5pt]\n    &= \\int^\\infty_{-\\infty}\\cos(tz)\\phi(z)\\mathrm{d}z + i\\int^\\infty_{-\\infty}\\sin(tz)\\phi(z)\\mathrm{d}z\\\\[5pt]\n    &= \\int^\\infty_{-\\infty}\\cos(tz)\\phi(z)\\mathrm{d}z \\quad \\because\\text{奇関数}\n\\end{align*}\n\\]\n次に，\\(t\\) について \\(\\varphi_Z(t)\\) を微分すると\n\\[\n\\begin{align*}\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\varphi_Z(t)\n    &= \\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb E[\\exp(itZ)]\\\\[5pt]\n    &= \\mathbb E\\left[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\exp(itZ)\\right] \\because\\text{期待値と微分の順序交換性}\\\\[5pt]\n    &= i\\mathbb E\\left[Z\\exp(itZ)\\right]\\\\\n    &= -\\int^\\infty_{-\\infty}z\\sin(tz)\\phi(z)\\mathrm{d}z\n\\end{align*}\n\\]\nこのとき，\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{z}}\\phi(z) = -z\\phi(z)\n\\]\nであるので\n\\[\n\\begin{align*}\n-\\int^\\infty_{-\\infty}z\\sin(tz)\\phi(z)\\mathrm{d}z\n    &= [\\sin(tz)\\phi(z)]^\\infty_{-\\infty} - \\int^\\infty_{-\\infty}t\\cos(tz)\\phi(z)\\mathrm{d}z\\\\\n    &= - \\int^\\infty_{-\\infty}t\\cos(tz)\\phi(z)\\mathrm{d}z\\\\\n    &= -t\\varphi_Z(t)\n\\end{align*}\n\\]\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\varphi_Z(t)=-t\\varphi_Z(t)\n\\]\n\\(\\varphi_Z(0) = 1\\) より，\n\\[\n\\varphi_Z(t) = \\exp(-t^2/2)\n\\]\n\n\n\n\n\nTheorem 15.5 : MGF of non-standard normal distribution \n\\(X \\sim N(\\mu, \\sigma^2)\\) の積率母関数および特性関数は\n\\[\n\\begin{align*}\nM_X(t) &= \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2t^2\\right)\\\\\n\\varphi_X(t) &= \\exp\\left(i\\mu t-\\frac{t^2\\sigma^2}{2}\\right)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(Z\\sim N(0, 1)\\) とすると，\\(Z = \\sigma Z + \\mu\\) と表せるので，\n\\[\n\\begin{align*}\nM_X(t) &= \\mathbb E[\\exp(t(\\sigma Z + \\mu))]\\\\[5pt]\n       &= \\exp(t\\mu) \\mathbb E[\\exp(t\\sigma Z)]\\\\[5pt]\n       &= \\exp(t\\mu)\\exp\\left(\\frac{1}{2}t^2\\sigma^2\\right)\\\\\n       &=  \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2t^2\\right)\n\\end{align*}\n\\]\n同様に\n\\[\n\\begin{align*}\n\\varphi_X(t)\n    &= \\mathbb E[\\exp(it(\\sigma z + \\mu))]\\\\[5pt]\n    &= \\exp(it\\mu) \\mathbb E[\\exp(it\\sigma z)]\\\\[5pt]\n    &= \\exp\\left(i\\mu t-\\frac{t^2\\sigma^2}{2}\\right)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nProof: 直接計算\n\n\n\n\n\n\\[\n\\begin{align*}\nM_X(t)\n    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{\\mathbb R}\\exp(tx)\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,\\mathrm{d}x\\\\\n    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{\\mathbb R}\\exp\\left(-\\frac{(x-(t\\sigma^2+\\mu))^2}{2\\sigma^2}\\right)\\exp\\left(t\\mu + \\frac{t^2\\sigma^2}{2}\\right)\\,\\mathrm{d}x\\\\\n    &= \\exp\\left(t\\mu + \\frac{t^2\\sigma^2}{2}\\right)\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 15.6 \n\\(X\\sim N(\\mu, \\sigma^2)\\) とする．定数 \\(a, b\\) に対して\n\\[\nY = aX + b\n\\]\nとしたとき，\\(Y\\sim N(a\\mu + b, a^2\\sigma^2)\\) となる．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(N(a\\mu + b, a^2\\sigma^2)\\) に従う確率変数の特性関数は\n\\[\n\\varphi(t) = \\exp\\left[(a\\mu + b)it - \\frac{a^2\\sigma^2t^2 }{2}\\right]\n\\]\nなので，\\(Y\\) の特性関数がこれと一致することを示せば良い．\n\\[\n\\begin{align*}\n\\varphi_Y(t)\n    &= \\mathbb E\\left[\\exp(itY)\\right]\\\\[5pt]\n    &= \\mathbb E\\left[\\exp(it(aX + b))\\right]\\\\[5pt]\n    &= \\exp(itb)\\exp\\left(i a\\mu t - \\frac{a^2\\sigma^2t^2}{2}\\right)\\\\\n    &=  \\exp\\left[(a\\mu + b)it - \\frac{a^2\\sigma^2t^2 }{2}\\right]\n\\end{align*}\n\\]\n\n\n\n\n\n正規分布の再生性\n\nDef: 確率分布の再生性 \n確率分布 \\(F\\) について，2 つの独立な確率変数 \\(X, Y\\) が \\(F\\) に従うとする．このとき，\n\\[\n\\begin{align*}\nZ &= X + Y\\\\\nZ& \\sim F\n\\end{align*}\n\\]\nが成立するとき，確率分布 \\(F\\) は再生性をもつという．\n\n二項分布，負の二項分布，ポアソン分布，正規分布などは，再生性を持つことがしられています．\n\n\nTheorem 15.7 : 正規分布の再生性 \n正規分布は，location parameter, scale parameter両方について再生性を持つ．つまり，\n\\[\n\\begin{align*}\n&X \\sim N(\\mu_x, \\sigma^2_x), Y \\sim N(\\mu_y, \\sigma^2_y)\\\\\n\\Rightarrow& X +Y \\sim N(\\mu_x+\\mu_y, \\sigma^2_x + \\sigma^2_y)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof: MGFを用いた証明\n\n\n\n\n\n確率変数 \\(X, Y\\) は独立なので\n\\[\n\\begin{align*}\nM_{X+Y}(t)\n    &= M_{X}(t) M_{Y}(t)\\\\\n    &= \\exp\\left(\\mu_xt + \\frac{\\sigma^2_xt}{2}\\right)\\exp\\left(\\mu_yt + \\frac{\\sigma^2_yt}{2}\\right)\\\\\n    &= \\exp\\left((\\mu_x + \\mu_y)t + \\frac{(\\sigma^2_x + \\sigma^2_y)t}{2}\\right)\n\\end{align*}\n\\]\n\\(X+Y\\) のMGFが \\(N(\\mu_x + \\mu_y, \\sigma^2_x + \\sigma^2_y)\\) のMGFと一致するので\n\\[\nX+Y\\sim N(\\mu_x + \\mu_y, \\sigma^2_x + \\sigma^2_y)\n\\]\n\n\n\n\n\n\n\n\n\nProof: 畳み込みを用いた証明\n\n\n\n\n\n確率変数 \\(X, Y\\) のそれぞれの密度関数を \\(f_X(x), f_Y(y)\\) で表したとき，\\(Z =X+Y\\) の確率密度関数 \\(p(z)\\) は畳み込みにより以下のように表せます．\n\\[\np(z) = \\int^\\infty_{-\\infty} f_X(x)f_Y(z-x)\\mathrm{d}x\n\\]\n従って，\n\\[\n\\begin{align*}\np(z)\n    &= \\frac{1}{2\\pi\\sigma_X\\sigma_Y}\\int^\\infty_{-\\infty}\\exp\\left(-\\frac{(x -\\mu_x)^2}{2\\sigma_X^2}-\\frac{((z-x) -\\mu_y)^2}{2\\sigma_Y^2}\\right)\\mathrm{d}x\n\\end{align*}\n\\]\nここで，最終項の指数部分について，\\(x\\) についてまとめると\n\\[\n\\begin{align*}\n&-\\frac{(x -\\mu_x)^2}{2\\sigma_X^2}-\\frac{((z-x) -\\mu_y)^2}{2\\sigma_Y^2}\\\\\n    &= -\\frac{\\sigma_x^2 + \\sigma_y^2}{2\\sigma_x^2\\sigma_y^2}\\left(x - \\frac{z\\sigma_x^2 -\\mu_x\\sigma_y^2 + \\mu_y\\sigma_x^2}{\\sigma_x^2 + \\sigma_y^2}\\right)^2 - \\frac{(z - (\\mu_x+\\mu_y))^2}{2(\\sigma_x^2 + \\sigma_y^2)}\n\\end{align*}\n\\]\nここでガウス積分より，\n\\[\n\\int^\\infty_{-\\infty}\\exp\\left(-\\frac{\\sigma_x^2 + \\sigma_y^2}{2\\sigma_x^2\\sigma_y^2}\\left(x - C\\right)^2 \\right)\\mathrm{d}x = \\sqrt{\\frac{2\\pi\\sigma_x^2\\sigma_y^2}{\\sigma_x^2 + \\sigma_y^2}}\n\\]\n以上より，\n\\[\np(z) = \\frac{1}{\\sqrt{2\\pi\\sigma_x^2\\sigma^2_y}}\\exp\\left(-\\frac{(z - (\\mu_x+\\mu_y))^2}{2(\\sigma_x^2 + \\sigma_y^2)}\\right)\n\\]\nこれは，\\(N(\\mu_x + \\mu_y, \\sigma^2_x + \\sigma^2_y)\\) の確率密度関数と一致するので，\n\\[\nX+Y\\sim N(\\mu_x + \\mu_y, \\sigma^2_x + \\sigma^2_y)\n\\]\n\n\n\n\n\nTheorem 15.8 : \\(n\\)個の正規分布の再生性 \n確率変数 \\(X_1, \\cdots, X_n\\) が互いに独立に \\(N(\\mu_i, \\sigma^2_i)\\) に従うとする． \\((a_1, \\cdots, a_n, b)\\)を定数としたとき，確率変数 \\(Y = \\sum_i a_iX_i + b\\) について，\n\\[\nY\\sim N(a_1\\mu_1 + \\cdots + a_n\\mu_n +b, a_1^2\\sigma^2 + \\cdots, + a_n^s\\sigma^2)\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(a_iX_i \\sim N(a_i\\mu_i, a_i^2\\sigma^2)\\) 及び，互いに独立な確率変数の合計和なので\n\\[\n\\begin{align*}\nM_Y(t)\n    &= \\mathbb E[\\exp(tY)]\\\\[5pt]\n    &= \\mathbb E[\\exp(t(a_1X_1 + \\cdots + a_nX_n+b))]\\\\\n    &= \\exp(tb)\\prod_{i=1}^n\\exp\\left[ta_i\\mu_i + \\frac{t^2a_i^2\\sigma_i^2}{2}\\right]\\\\\n    &= \\exp\\left[(a_1\\mu_i + \\cdots+a_1\\mu_n + b)t + \\frac{t^2(a_1^2\\sigma_1^2 + \\cdots + a_n^2\\sigma_n^2)}{2}\\right]\n\\end{align*}\n\\]\nこれは確率分布 \\(N(a_1\\mu_1 + \\cdots, + a_n\\mu_n +b, a_1^2\\sigma^2 + \\cdots, + a_n^s\\sigma^2)\\) の積率母関数と一致するので\n\\[\nY\\sim N(a_1\\mu_1 + \\cdots + a_n\\mu_n +b, a_1^2\\sigma^2 + \\cdots, + a_n^s\\sigma^2)\n\\]\nが成立する．\n\n\n\n\n\nDifferential Entropy\n\nDef: 微分エントロピー \n連続確率変数 \\(X\\) について，確率密度関数が \\(p(x)\\) で与えられているとする．このとき，微分エントロピーは以下の形で定義される\n\\[\n\\mathrm{H}(X) = - \\int_{\\mathcal{X}} p(x) \\log_b p(x) \\, \\mathrm{d}x\n\\]\nなお，\\(b\\) は通常 \\(2, e\\) が用いられる\n\n平均 \\(\\mu\\), 分散 \\(\\sigma^2\\) をもつ確率分布のうち，正規分布は微分エントロピーを最大にする分布として知られています．\n ▶  \\(N(\\mu, \\sigma^2)\\) の微分エントロピー\n\\(N(\\mu, \\sigma^2)\\) の確率密度関数を \\(f(x)\\) として，微分エントロピーの定義より\n\\[\n\\begin{align*}\n\\mathrm{H}\n    &= - \\int_{-\\infty}^\\infty f(x) \\log_2 f(x) \\, \\mathrm{d}x\\\\\n    &= - \\int_{-\\infty}^\\infty f(x) \\log_2 \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\right) \\, \\mathrm{d}x\\\\\n    &= \\frac{\\log_2(2\\pi\\sigma^2)}{2}\\int_{-\\infty}^\\infty f(x)\\mathrm{d}x + \\frac{\\log_2 e}{2\\sigma^2} \\int_{-\\infty}^\\infty (x-\\mu)^2f(x)\\mathrm{d}x\\\\\n    &= \\frac{1}{2}\\log_2(2\\pi e\\sigma^2)\n\\end{align*}\n\\]\nなお，自然対数で表現する場合，\\(\\mathrm{H} = \\frac{1}{2}[1 + \\ln(2\\pi\\sigma^2)]\\)\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\nTheorem 15.9 \n平均 \\(\\mu\\), 分散 \\(\\sigma^2\\) をもつ確率分布のうち，正規分布は微分エントロピーを最大にする分布である．\n\n\n\n\n\n\n\n\nProof: ラグランジュの未定係数法\n\n\n\n\n\n（注意: 制約付き最大化問題を解くにあたって，ラグランジュの未定係数法が使用できると仮定してます）\n ▶  汎関数の定義\n\\[\n\\begin{align*}\n&F(p(x), \\lambda_1, \\lambda_2, \\lambda_3)\\\\\n&= -\\int^\\infty_{-\\infty}p(x)\\ln(p(x))\\,\\mathrm{d}x + \\lambda_1\\left[\\int^\\infty_{-\\infty}p(x)\\,\\mathrm{d}x-1\\right]\\\\\n&\\qquad+\\lambda_2\\int^\\infty_{-\\infty}(x-\\mu)p(x)\\,\\mathrm{d}x + \\lambda_3\\left[\\int^\\infty_{-\\infty}(x-\\mu)^2p(x)\\,\\mathrm{d}x-\\sigma^2\\right]\\\\\n\\end{align*}\n\\]\n ▶  極値条件の計算\n\n\\[\n\\begin{align*}\n\\frac{\\partial F}{\\partial p(x)} &= -\\int^\\infty_{-\\infty}(1 + \\ln(p(x)) - \\lambda_1 - \\lambda_2(x-\\mu) - \\lambda_3(x-\\mu)^2)\\,\\mathrm{d}x = 0\\tag{A}\\\\\n\\frac{\\partial F}{\\partial \\lambda_1} &= \\int^\\infty_{-\\infty}p(x)\\,\\mathrm{d}x-1 = 0\\tag{B}\\\\\n\\frac{\\partial F}{\\partial \\lambda_2} &= \\int^\\infty_{-\\infty}(x-\\mu)p(x)\\,\\mathrm{d}x = 0\\tag{C}\\\\\n\\frac{\\partial F}{\\partial \\lambda_3} &= \\int^\\infty_{-\\infty}(x-\\mu)^2p(x)\\,\\mathrm{d}x-\\sigma^2= 0\\tag{D}\n\\end{align*}\n\\]\n\n ▶  条件(A)の整理\n条件 \\(\\mathrm{(A)}\\) より以下を得る\n\n\\[\np(x) = \\exp(-1+\\lambda_1 + \\lambda_2(x-\\mu)+ \\lambda_3(x-\\mu)^2)\\tag{E}\n\\]\n\nなお扱いやすいように \\(z = x - \\mu\\) として以下の形で表す．\n\n\\[\np(x) = \\exp(-1+\\lambda_1 + \\lambda_2z+ \\lambda_3z^2)\\tag{E'}\n\\]\n\n ▶  \\(\\lambda_1\\) の消去\n\\(\\mathrm{(E')}\\) を \\(\\mathrm{(B)}\\) に代入すると，\\(\\frac{\\,\\mathrm{d}x}{\\,\\mathrm{d}z}=1\\) より\n\\[\n\\begin{align*}\n&\\int^\\infty_{-\\infty}\\exp(-1 + \\lambda_1 + + \\lambda_2z+ \\lambda_3z^2)\\,\\mathrm{d}z\\\\\n&=\\exp(-1+\\lambda_1)\\exp\\left(-\\frac{\\lambda^2}{4\\lambda_3}\\right) \\int^\\infty_{-\\infty}\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\,\\mathrm{d}z\\\\\n&=1\n\\end{align*}\n\\]\nこのとき，等号が成立するためには \\(\\lambda_3 &lt; 0\\) が必要であることが分かる．また，ガウス積分より\n\n\\[\n\\int^\\infty_{-\\infty}\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\,\\mathrm{d}z = \\frac{\\sqrt{\\pi}}{\\sqrt{-\\lambda_3}}\\tag{F}\n\\]\n\n従って，\n\n\\[\np(x) =  \\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\tag{G}\n\\]\n\n ▶  \\(\\lambda_2\\) の消去\n\\(\\mathrm{(G)}\\) と \\(\\mathrm{(C)}\\) より\n\n\\[\n\\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\int^\\infty_{-\\infty}z\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\,\\mathrm{d}z = 0\\tag{H}\n\\]\n\n\\(\\mathrm{(H)} = 0\\) が成立するためには，\\(\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\) が偶関数になる必要があるので\n\\[\n\\begin{gather*}\n\\frac{\\lambda_2}{2\\lambda_3} = 0\\\\\n\\Rightarrow \\lambda_2 = 0\n\\end{gather*}\n\\]\n従って，\n\n\\[\np(x) = \\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\exp\\left(\\lambda_3z^2\\right)\\tag{I}\n\\]\n\n ▶  \\(\\lambda_3\\) の消去\n\\(\\mathrm{(I)}, \\mathrm{(D)}\\) を整理すると\n\\[\n\\begin{align*}\n\\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\int^\\infty_{-\\infty} z^2\\exp\\left(\\lambda_3z^2\\right)\\,\\mathrm{d}z = \\sigma^2\n\\end{align*}\n\\]\n\\(\\int^\\infty_{-\\infty}x^2\\exp(-ax^2)\\mathrm{d}x = \\frac{\\sqrt{\\pi}}{2a\\sqrt{a}}\\) より\n\\[\n\\begin{align*}\n\\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\int^\\infty_{-\\infty} z^2\\exp\\left(\\lambda_3z^2\\right)\\,\\mathrm{d}z\n    &= \\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\frac{\\sqrt{\\pi}}{-2\\lambda_3\\sqrt{-\\lambda_3}}\\\\\n    &= \\frac{1}{-2\\lambda_3}\n\\end{align*}\n\\]\n従って，\n\\[\n\\lambda_3 = -\\frac{1}{2\\sigma^2}\n\\]\n以上より, 微分エントロピーを最大化する \\(p^*(x)\\) は\n\\[\np^*(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\nとなり，正規分布となることが分かる．\n\n\n\n\n📘 REMARKS \n\n\\(\\lambda_2 = 0\\) より location paramter \\(\\mu\\) を変化させても，微分エントロピーは限界的には増えないことが分かります\n\\(\\lambda_3 &lt; 0\\) より scale paramter \\(\\sigma^2\\) を増大させると，微分エントロピーは限界的に増大することも分かります\n\n\n\n\n他の確率分布との関係性\n\nExample 15.2 : 二項分布の極限分布としての正規分布&lt; \n\\(n\\) を正の整数として，\\(Y_n \\sim \\operatorname{Binom}(n, 1/2)\\) とし，\n\\[\nX_n = \\frac{Y_n - n/2}{\\sqrt{n}/2}\n\\]\nという確率変数を考えます．\\(X_n \\in (x - 2/\\sqrt{n}, x]\\) となるような確率を考えてみると\n\\[\n\\begin{align*}\n&\\Pr(x - 2/\\sqrt{n} &lt; X_n \\leq x)\\\\\n    &= \\Pr\\left(\\frac{\\sqrt{n}}{2}x + \\frac{n}{2} - 1 &lt; Y_n \\leq \\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\right)\\\\\n    &= \\Pr\\bigg(\\bigg\\lfloor Y_n = \\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rfloor\\bigg)\\\\\n    &= \\frac{n!}{\\bigg\\lfloor \\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rfloor ! \\bigg\\lceil -\\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rceil !}\\left(\\frac{1}{2}\\right)^n\n\\end{align*}\n\\]\nここでスターリングの公式より十分大きい正の整数 \\(m\\) について\n\\[\nm! \\approx \\sqrt{2\\pi m} m^m\\exp(-m)\n\\]\nと近似できるので\n\\[\n\\begin{align*}\n&\\lim_{n\\to\\infty}\\Pr(x - 2/\\sqrt{n} &lt; X_n \\leq x)\\\\\n    &= \\lim_{n\\to\\infty}\\frac{n!}{\\bigg\\lfloor \\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rfloor ! \\bigg\\lceil -\\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rceil !}\\left(\\frac{1}{2}\\right)^n\\\\\n    &= \\lim_{n\\to\\infty} \\frac{1}{\\sqrt{2\\pi\\left(\\frac{n}{4}-\\frac{x^2}{4}\\right)}\\left(1 - \\frac{x^2}{n}\\right)^{\\frac{n}{2}}\\left(1 + \\frac{x}{\\sqrt{n}}\\right)^{\\frac{\\sqrt{n}}{2}x}\\left(1 - \\frac{x}{\\sqrt{n}}\\right)^{-\\frac{\\sqrt{n}}{2}x}}\\left(\\frac{1}{2}\\right)^n\\\\\n    &= \\frac{1}{\\sqrt{2\\pi}\\exp(-x^2/2)\\exp(x^2/2)\\exp(x^2/2)}\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\exp(-x^2/2)\\\\[8pt]\n    &= \\phi(x)\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>正規分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/exponential_dist.html",
    "href": "posts/probability_distribution/exponential_dist.html",
    "title": "16  指数分布",
    "section": "",
    "text": "指数分布の性質\n指数分布は連続的な待ち時間分布の性質(=幾何分布の連続変数版)という特徴を持ちます．そのため後述しますが無記憶性の性質を持っています． 指数分布は，故障率が一定のシステムの偶発的な故障までの待ち時間（耐用年数や寿命）を当てはめる際に用いられたりします．\nCode\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\nfrom regmonkey_style import stylewizard as sw\n\nsw.set_templates(\"regmonkey_line\")\n\nfig, ax = plt.subplots()\nlambda_vals = [0.5, 1, 2]\nx_grid = np.linspace(0, 6, 200)\n\nfor _lambda in lambda_vals:\n    z = stats.expon(scale=1 / _lambda)\n    ax.plot(x_grid, z.pdf(x_grid), alpha=0.8, lw=2, label=f\"$\\lambda={_lambda}$\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"PDF\")\nplt.legend()\nplt.show()\n▶  期待値の導出\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\int^\\infty_0 x \\lambda \\exp(-\\lambda x)\\,\\mathrm{d}x\\\\\n    &= \\underbrace{[-x\\exp(-\\lambda x)]^\\infty_0}_{=0} + \\int^\\infty_0 \\exp(-\\lambda x)\\,\\mathrm{d}x\\\\\n    &= \\frac{1}{\\lambda}\\left[- \\exp(-\\lambda x)\\right]^\\infty_0\\\\\n    &= \\frac{1}{\\lambda}\n\\end{align*}\n\\]\n▶  分散の導出\n\\[\n\\begin{align*}\n\\mathbb E[X^2]\n    &= \\int^\\infty_0 x^2 \\lambda \\exp(-\\lambda x)\\,\\mathrm{d}x\\\\\n    &= \\underbrace{[-x^2\\exp(-\\lambda x)]^\\infty_0}_{=0} + \\frac{2}{\\lambda}\\underbrace{\\int^\\infty_0 x \\lambda \\exp(-\\lambda x)\\,\\mathrm{d}x}_{=\\mathbb E[X]}\\\\\n    &= \\frac{2}{\\lambda^2}\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\operatorname{Var}(X)\n    &= \\mathbb E[X^2] - (\\mathbb E[X])^2\\\\\n    &= \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2}\\\\\n    &= \\frac{1}{\\lambda^2}\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\\(\\mathbb E[X] = \\frac{1}{\\lambda}\\), \\(\\operatorname{Var}(x) = \\frac{1}{\\lambda^2}\\) より， 生起までの年数が指数分布かつ希少事例(\\(\\lambda\\) が小さい)場合，\\(\\mathbb [X] \\pm \\sigma\\) が0まで伸びています． 確率が小さいことと遠い将来にしか発生しないは同じではないことがわかります．\n▶  Modeの導出\n最頻値(Mode)はPDF \\(f_X(x)\\) を最大化する \\(x \\in\\mathcal{X}\\) なので\n\\[\n\\operatorname{Mode}(X) = \\arg\\max_x f_X(x)\n\\]\n\\(f_X(0) = \\lambda\\) また \\(x &gt; 0\\) の範囲で \\(0 &lt; \\exp(-\\lambda x) &lt; 1\\) なので，\n\\[\n\\operatorname{Mode}(X) = 0\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>指数分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/exponential_dist.html#指数分布の性質",
    "href": "posts/probability_distribution/exponential_dist.html#指数分布の性質",
    "title": "16  指数分布",
    "section": "",
    "text": "Def: 指数分布 \n連続確率変数 \\(X\\) がパラメータ \\(\\lambda &gt; 0\\) の指数分布に従う， \\(X \\sim \\operatorname{Exp}(\\lambda)\\)，のとき，その確率密度関数 \\(f(x)\\) は\n\\[\nf(x) = \\left\\{\\begin{array}{c}\n    \\lambda \\exp(-\\lambda x) & (x &gt; 0)\\\\\n    0 & \\text{otherwise}\n    \\end{array}\\right.\n\\]\n累積分布関数 \\(F(x)\\) は\n\\[\n\\begin{align*}\nF(x)\n    & = \\int^x_{-\\infty}f(x)\\,\\mathrm{d}x\\\\\n    &= [1 - \\exp(-\\lambda x)]\\mathbb 1(X &gt;0)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 16.1 : Median \n確率変数 \\(X \\sim \\operatorname{Exp}(\\lambda)\\) について，\n\\[\n\\operatorname{Median}(X) = \\frac{\\log 2}{\\lambda}\n\\]\n\n\n\n分位点関数\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\operatorname{Median}(X) = x^*\\) としたとき，\n\\[\n\\begin{gather}\n1 - \\exp(-\\lambda x^*) = \\frac{1}{2}\\\\\n\\Rightarrow x^* = \\frac{\\log 2}{\\lambda}\n\\end{gather}\n\\]\n\n\n\n\n\nTheorem 16.2 : Quantile function \n確率変数 \\(X \\sim \\operatorname{Exp}(\\lambda)\\) について，分位点関数は\n\\[\nQ_X(p) = \\left\\{\n\\begin{array}{rl}\n-\\infty \\; , & \\text{if} \\; p = 0 \\\\\n-\\frac{\\ln(1-p)}{\\lambda} \\; , & \\text{if} \\; p &gt; 0 \\; .\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n確率変数 \\(X \\sim \\operatorname{Exp}(\\lambda)\\) のCDFを \\(F_X(x)\\) とすると，\n\\[\nF_X(x) = 1 - \\exp(-\\lambda x)\n\\]\nこのとき分位点関数は\n\\[\nQ_X(p) = \\inf\\{x\\in\\mathbb R\\vert F_X(x) \\geq p\\}\n\\]\n\\(p=0\\) のとき, \\(Q_X(0) = -\\infty\\). また，\\(p &gt; 0\\) のとき，\n\\[\nQ_X(p) = F^{-1}_X(p)\n\\]\n従って，\n\\[\n\\begin{gather}\np = 1 - \\exp(-\\lambda x)\\\\\n\\Rightarrow \\exp(-\\lambda x) = 1 - p\\\\\n\\Rightarrow x = -\\frac{\\log(1 - p)}{\\lambda}\n\\end{gather}\n\\]\n\n\n\n\n\nMGF\n\n\nTheorem 16.3 : MGF \n\\(X\\sim\\operatorname{Exp}(\\lambda)\\) について，MGFは\n\\[\nM_X(t) = \\frac{\\lambda}{\\lambda - t}\n\\]\nただし，\\(t &lt; \\lambda\\) の範囲でのみ定義される．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nM_X(t) &= \\int_0^{\\infty} e^{tx} \\cdot f_X(x) \\,\\mathrm{d} x\\\\\n&= \\int_0^{\\infty} e^{tx}\\cdot \\lambda e^{-\\lambda x} \\mathrm{d}x\\\\\n&= \\int_0^{\\infty} \\lambda e^{x(t-\\lambda)} \\mathrm{d}x\\\\\n&= \\frac{\\lambda}{t-\\lambda} e^{x(t-\\lambda)} \\Big|_{x = 0}^{x = \\infty}\\\\\n&= \\lim_{x\\rightarrow \\infty} \\left[ \\frac{\\lambda}{t-\\lambda} e^{x(t-\\lambda)} - \\frac{\\lambda}{t-\\lambda}\\right]\\\\\n&= \\frac{\\lambda}{t-\\lambda} \\left[ \\lim_{x\\rightarrow \\infty} e^{x(t-\\lambda)} -1 \\right]\n\\end{align*}\n\\]\n\\(t = \\lambda\\) では \\(M_X(t)\\) が定義できないことがわかります．また，\\(t &gt; \\lambda\\) では\n\\[\n\\lim_{x\\to\\infty}e^{x(t-\\lambda)}\\to \\infty\n\\]\nとなり，同様に定義できないことがわかります．\\(t &lt; \\lambda\\) においてのみ以下のようにMGFは定義できます．\n\\[\n\\begin{align*}\nM_X(t) &= \\frac{\\lambda}{t-\\lambda} \\left[ \\lim_{x\\rightarrow \\infty} e^{x(t-\\lambda)} -1 \\right] \\\\\n&= \\frac{\\lambda}{t-\\lambda} \\left[ 0 - 1 \\right] \\\\\n&= \\frac{\\lambda}{\\lambda - t}\n\\end{align*}\n\\]\n\n\n\n\n\nガンマ分布との関係\n\n\nTheorem 16.4 ガンマ分布と指数分布 \n確率変数 \\(X \\sim \\operatorname{Gam}(1, \\lambda^{-1})\\) は \\(\\operatorname{Exp}(\\lambda)\\) と一致する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\operatorname{Gam}(a, b)\\) の確率密度関数は\n\\[\n\\frac{1}{b^a\\Gamma(a)}x^{a-1}\\exp(-x/b)\n\\]\nこれに \\(a = 1, b = \\lambda^{-1}\\) を代入すると\n\\[\n\\begin{align*}\n\\frac{1}{\\lambda^{-1}\\Gamma(1)}x^{1-1}\\exp(-\\lambda x) = \\lambda\\exp(-\\lambda x)\n\\end{align*}\n\\]\nこれは指数分布 \\(\\operatorname{Exp}(\\lambda)\\) の確率密度関数と一致する．\n\n\n\n\n\nTheorem 16.5 : 指数分布に従う確率変数の和とガンマ分布 \n\\(\\operatorname{Exp}(\\lambda)\\) に独立に従う確率変数 \\(X_1, \\cdots, X_n\\) について，\n\\[\nZ = X_1 + \\cdots +  X_n\n\\]\nこのとき，\n\\[\nZ \\sim \\operatorname{Gam}(n, \\lambda^{-1})\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(Z\\) についての特性関数は\n\\[\n\\begin{align*}\n\\phi(t) &= \\left(\\frac{\\lambda}{\\lambda - it}\\right)^n\\\\\n        &= (1 - it/\\lambda)^{-n}\n\\end{align*}\n\\]\n従って，\\(\\operatorname{Gam}(n, 1/\\lambda)\\) の特性関数と一致する．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>指数分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/exponential_dist.html#無記憶性",
    "href": "posts/probability_distribution/exponential_dist.html#無記憶性",
    "title": "16  指数分布",
    "section": "無記憶性",
    "text": "無記憶性\n\n\nTheorem 16.6 : 指数分布の無記憶性 \n\\(s, t\\) を非負の実数とし，確率変数 \\(X \\sim \\operatorname{Exp}(\\lambda)\\) のとき．\n\\[\nP(X \\geq s + t\\vert X \\geq s) = P(X\\geq t)\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nP(X \\geq s + t\\vert X \\geq s)\n    &= \\frac{P(X \\geq s + t, X \\geq s)}{P(X \\geq s)}\\\\\n    &= \\frac{P(X \\geq s + t)}{P(X \\geq s)}\n\\end{align*}\n\\]\n\\(\\operatorname{Exp}(\\lambda)\\) の上側確率は \\(P(X\\geq x) = \\exp(\\lambda x)\\) であるので，\n\\[\n\\frac{\\exp(-\\lambda (s + t))}{\\exp(-\\lambda s)} = \\exp(-\\lambda t) = P(X\\geq t)\n\\]\n\n\n\n\n\nTheorem 16.7 : 無記憶性を持つ連続型分布は指数分布のみ \n\\(\\mathbb R_{+} = [0, \\infty)\\) で定義される連続型確率確率変数 \\(X\\) が，任意の \\(s, t\\geq 0\\) について\n\\[\nP(X &gt; s + t \\vert X &gt; s) = P(X &gt; t)\n\\]\nを満たすとする．このとき \\(X\\) は指数分布に従う．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nテール確率 \\(S_X(t) = P(X &gt; t), 0 \\leq t &lt; \\infty\\) を考えます．このとき，\\(S(t)\\) は\n\\[\n\\begin{gather}\nS(0) = 1\\\\\n\\lim_{t\\infty}S(t) = 0\n\\end{gather}\n\\]\nを満たす単調非増加関数です．無記憶性の定義から\n\\[\n\\frac{S(s + t)}{S(s)} = S(t) \\Rightarrow S(s + t) = S(s)S(t)\n\\]\n両辺より \\(S(s)\\) を引いて \\(t &gt;0\\)で割ると，\n\\[\n\\begin{align*}\n\\frac{S(s + t) - S(s)}{t}\n    &= S(s)\\frac{S(t) - 1}{t}\\\\\n    &= S(s)\\frac{S(t) - S(0)}{t}\n\\end{align*}\n\\]\nこの両辺を \\(t\\to 0\\) と近づけると連続型確率変数なので\\(S(\\cdot)\\) は微分可能であるので\n\\[\nS^\\prime(s) = S(s)S^\\prime(0)\n\\]\nテール確率の単調非増加性を踏まえ \\(S^\\prime(0) = -\\lambda, (\\lambda &gt;0)\\) とおくと\n\\[\n\\frac{S^\\prime(s)}{S(s)} = -\\lambda\n\\]\nこれを \\(s\\) について不定積分すると\n\\[\n\\log(S(s)) = -\\lambda s + C\n\\]\n\\(S(0)=1\\) より \\(C= 0\\)． 従って， \\[\nS(s) = \\exp(-\\lambda s)\n\\]\n従って，この確率変数の分布確率は\n\\[\nF_X(x) = 1 - \\exp(-\\lambda x)\n\\]\nとなり，\\(\\operatorname{Exp}(\\lambda)\\) の分布関数と一致する．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>指数分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/exponential_dist.html#ハザード関数",
    "href": "posts/probability_distribution/exponential_dist.html#ハザード関数",
    "title": "16  指数分布",
    "section": "ハザード関数",
    "text": "ハザード関数\n\nDef: ハザード関数 \n非負の確率変数 \\(T\\) のハザード関数は次のように定義される\n\\[\n\\lambda(t) = \\lim_{\\mathrm{d}t\\rightarrow0} \\frac{P\\{t \\le T &lt; t + \\mathrm{d}t |\nT \\ge t \\} }{\\mathrm{d}t} = \\frac{f(t)}{1 - F(t)}\n\\]\n\n\\(t\\) まで動作している条件のもとで次の瞬間に故障する確率密度を表していると解釈できます．\n\\[\n\\begin{align*}\nP\\{t \\le T &lt; t + dt \\vert T \\ge t \\}\n    &= \\frac{P\\{t \\le T &lt; t + \\mathrm{d}t , T \\geq t\\}}{P(T\\geq t)}\\\\\n    &= \\frac{P\\{t \\le T &lt; t + \\mathrm{d}t\\}}{P(T\\geq t)}\\\\\n    &= \\frac{F(t + \\mathrm{d}t) - F(t)}{1 - F(t)}\n\\end{align*}\n\\]\nと展開できるので，\\(\\mathrm{d}t\\) で割り，\\(\\lim_{\\mathrm{d}t\\to 0}\\) とすることで \\(\\frac{f(t)}{1 - F(t)}\\) が得られることがわかります．\n ▶  指数分布のハザード関数\n\\(\\lambda \\in \\{0.2, 1, 2\\}\\) のパラメータを用いて各指数分布のハザード関数をplotしたのが次です．\n\n\nCode\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\n\nfig, ax = plt.subplots()\nlambda_vals = [0.5, 1, 2]\nx_grid = np.linspace(0, 6, 200)\n\nfor _lambda in lambda_vals:\n    z = stats.expon(scale=1 / _lambda)\n    ax.plot(x_grid, z.pdf(x_grid)/(1 - z.cdf(x_grid)), alpha=0.8, lw=2, label=f\"$\\lambda={_lambda}$\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"hazard function\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nこのようにしす分布については常に \\(\\lambda(x) = \\lambda\\) で時間 \\(x\\) には無関係になる = 瞬間故障確率は常に一定であることがわかります．これは指数分布の無記憶性とも整合的です．\n\\[\n\\begin{align*}\n\\frac{f(x)}{1 - F(x)} = \\frac{\\lambda\\exp(-\\lambda x)}{\\exp(-\\lambda x)} = \\lambda\n\\end{align*}\n\\]\nのように確かめることもできます\n\n\nTheorem 16.8 \n非負の連続型確率変数 \\(X\\) が \\(t \\geq 0\\) について\n\\[\nP(t &lt; X \\leq t + \\Delta t\\vert X &gt; t) = \\lambda \\Delta t + o(\\Delta t), \\quad\\lambda &gt; 0\n\\]\nをみたすとき, \\(X\\) は指数分布に従う\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nテール確率を \\(S(x)\\) と表すと\n\\[\n\\begin{align*}\nP(t &lt; X \\leq t + \\Delta t\\vert X &gt; t) = \\frac{S(t) - S(t + \\Delta t)}{S(t)}\n\\end{align*}\n\\]\n従って，\n\\[\n\\frac{S(t) - S(t + \\Delta t)}{S(t)}= \\lambda \\Delta t + o(\\Delta t)\n\\]\n両辺を \\(-\\Delta t\\) で割ると\n\\[\n\\frac{S(t + \\Delta t) - S(t)}{\\Delta t}\\frac{1}{S(t)}= -\\lambda - \\frac{o(\\Delta t)}{\\Delta t}\n\\]\n\\(\\Delta t\\to 0\\) に近づけると\n\\[\n\\begin{align*}\n\\lim_{\\Delta t\\to 0}\\frac{S(t + \\Delta t) - S(t)}{\\Delta t}\\frac{1}{S(t)} &= \\frac{S^\\prime(t)}{S(t)}\\\\\n\\lim_{\\Delta t\\to 0}-\\lambda - \\frac{o(\\Delta t)}{\\Delta t} &= -\\lambda\n\\end{align*}\n\\]\n従って，\\(S(0) = 1\\) 及び単調非増加関数であるので\n\\[\nS(t) =\\exp(-\\lambda t) \\Rightarrow F(t) = 1 - \\exp(-\\lambda t)\n\\]\n従って，\\(\\operatorname{Exp}(\\lambda)\\) の確率分布関数と一致することが分かる．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>指数分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/gamma_dist.html",
    "href": "posts/probability_distribution/gamma_dist.html",
    "title": "17  ガンマ分布",
    "section": "",
    "text": "ガンマ分布の性質\nCode\nfrom regmonkey_style import stylewizard as sw\nfrom scipy.stats import gamma\nimport plotly.graph_objs as go\nimport numpy as np\n\nsw.set_templates('regmonkey_line')\nN = 1000\nx = np.linspace(0, 6, N)[1:]\na= [0.5, 2, 5]\nscale = 1\ny_values =list( map(lambda shape: gamma(a=shape, scale=1).pdf(x), a))\n\n# Create traces for each shape parameter\ntraces = []\nfor i, shape in enumerate(a):\n    traces.append(go.Scatter(\n        x=x,\n        y=list(y_values)[i],\n        mode='lines',\n        name=f'Gamma(a={shape})'\n    ))\n# Create figure and add traces\nfig = go.Figure(data=traces)\nfig.update_layout(\n    yaxis=dict(\n        range=[0, 0.8],\n        fixedrange = True\n    ),\n    xaxis=dict(\n        range=[0, 5],\n        fixedrange = True\n    ),\n    title=\"Gamma Distribution PDF for Different Shape Parameters\",\n    xaxis_title=\"x\",\n    yaxis_title=\"PDF\"\n)\n# Show the plot\nfig.show()\n▶  確率密度関数の積分値が１になる確認\n\\(z = x/b\\) という変数変換を用いることで以下のように示すことができます．\n\\[\n\\begin{align*}\n\\int_0^\\infty f(x) \\mathrm{d}x\n    &= \\int_0^\\infty\\frac{1}{b^a\\Gamma(a)} x^{a-1} \\exp[-x/b]\\mathrm{d}x\\\\\n    &= \\int_0^\\infty\\frac{1}{b\\Gamma(a)} z^{a-1} \\exp[-z]b\\mathrm{d}z\\\\\n    &= \\frac{1}{\\Gamma(a)}\\underbrace{\\int_0^\\infty z^{a-1} \\exp[-z]\\mathrm{d}z}_{=\\Gamma(a)}\\\\\n    &=1\n\\end{align*}\n\\]\n▶  期待値の導出\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\int_0^\\infty x \\frac{1}{b^a\\Gamma(a)} x^{a-1} \\exp[-x/b] \\mathrm{d}x\\\\\n    &= ab\\int_0^\\infty \\underbrace{\\frac{1}{b^{a+1}\\Gamma(a+1)} x^{(a+1)-} \\exp[-x/b]}_{\\operatorname{Gam}(a+1, b)\\text{の密度関数}} \\mathrm{d}x\\\\\n    &=ab\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\nまたは，\\(z = x/b\\) という変数変換を用いて以下のようにも示せます．\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\int_0^\\infty x \\frac{1}{b^a\\Gamma(a)} x^{a-1} \\exp[-x/b] \\mathrm{d}x\\\\\n    &= \\int_0^\\infty \\frac{1}{b^a\\Gamma(a)} x^{a} \\exp[-x/b] \\mathrm{d}x\\\\\n    &= \\int_0^\\infty \\frac{1}{b^{a+1}\\Gamma(a)} (bz)^a \\exp[-z] \\mathrm{d}z\\\\\n    &= \\frac{1}{b\\Gamma(a)}\\underbrace{\\int_0^\\infty z^{(a+1)-1} \\exp[-z] \\mathrm{d}z}_{=\\Gamma(a+1)}\\\\\n    &= \\frac{a\\Gamma(a)}{b\\Gamma(a)} \\quad\\because \\Gamma(a+1) = a\\Gamma(a)\\\\\n    &= ab\n\\end{align*}\n\\]\n▶  分散の導出\n\\[\n\\begin{align*}\n\\mathbb E[X^2]\n    &= \\int_0^\\infty x^2 \\frac{1}{b^a\\Gamma(a)} x^{a-1} \\exp[-x/b] \\mathrm{d}x\\\\\n    &= a(a+1)b^2 \\int_0^\\infty \\underbrace{\\frac{1}{b^{a+2}\\Gamma(a+2)} x^{a+2-1} \\exp[-x/b]}_{=\\operatorname{Gam}(a+2, b)} \\mathrm{d}x\\\\\n    &= a(a+1)b^2\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\operatorname{Var}(X)\n    &= \\mathbb E[X^2] - \\mathbb E[X]^2\\\\\n    &= a(a+1)b^2 - (ab)^2\\\\\n    &= ab^2\n\\end{align*}\n\\]\n▶  k次モーメントの導出\n\\(k \\in \\mathbb Z_{++}\\) について，k次モーメントは以下のように計算できます．\n\\[\n\\begin{align*}\n\\mathbb E[X^k]\n    &= \\int_0^\\infty x^k \\frac{1}{b^a\\Gamma(a)}x^{a-1}\\exp[-x/b]\\,\\mathrm{d}x\\\\[5pt]\n    &= b^ka(a+1)\\cdots(a+k-1)\\underbrace{\\int_0^\\infty \\frac{1}{b^{a+k}\\Gamma(a+k)}x^{a+k-1}\\exp[-x/b]}_{=\\operatorname{Gam}(a+k, b)\\text{のpdf}}\\,\\mathrm{d}x\\\\\n    &= b^k \\prod_{i=0}^{k-1}(a+i)\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>ガンマ分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/gamma_dist.html#ガンマ分布の性質",
    "href": "posts/probability_distribution/gamma_dist.html#ガンマ分布の性質",
    "title": "17  ガンマ分布",
    "section": "",
    "text": "Def: ガンマ分布 \n確率変数 \\(\\operatorname{X}\\) がガンマ分布に従うとき，その確率密度関数は\n\\[\n\\begin{align*}\n\\operatorname{Gam}(x; a, b) &= \\frac{1}{b^a\\Gamma(a)} x^{a-1} \\exp[-x/b], \\quad x &gt; 0\\\\\n\\text{where}\\quad & a &gt;0, b &gt; 0\n\\end{align*}\n\\]\nこのとき，\\(X\\) はパラメータ \\((a, b)\\) のガンマ分布に従うという．\n\n\\(a\\): 形状パラメーター(shape parameter)\n\\(b\\): 尺度パラメーター(scale parameter)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 17.1 : スケールパラメータ \n\\(X\\sim\\operatorname{Gam}(a, b)\\) のとき，\\(c &gt;0\\) とする任意の定数について．\\(Y = cX\\) としたとき\n\\[\nY \\sim \\operatorname{Gam}(a, bc)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n問題設定より\n\\[\n\\begin{gather}\nY = g(X) = c X\\\\\nX = g^{-1}(Y) = \\frac{1}{c} Y\n\\end{gather}\n\\]\n\\(g(X)\\) は狭義単調増加関数なので\n\\[\nf_Y(y) = f_X(g^{-1}(y)) \\frac{\\mathrm{d}g^{-1}(y)}{\\mathrm{d}y}\n\\]\n従って，\n\\[\n\\begin{align*}\nf_Y(y)\n    &= \\frac{1}{b^a\\Gamma(a)}\\left(\\frac{y}{c}\\right)^{a-1}\\exp\\left(-\\frac{y}{bc}\\right)\\frac{1}{c}\\\\\n    &= \\frac{1}{(bc)^a\\Gamma(a)}y^{a-1}\\exp\\left(-\\frac{y}{bc}\\right)\n\\end{align*}\n\\]\nこれは \\(\\operatorname{Gam}(a-1, bc)\\) の確率密度関数と一致します．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>ガンマ分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/gamma_dist.html#mgf",
    "href": "posts/probability_distribution/gamma_dist.html#mgf",
    "title": "17  ガンマ分布",
    "section": "MGF",
    "text": "MGF\n\n\nTheorem 17.2 : ガンマ分布の積率母関数と特性関数 \n\\(\\operatorname{Gam}(a, b)\\) の積率母関数と特性関数は\n\\[\n\\begin{align*}\nM_X(t) &= (1 - bt)^{-a}\\\\\n\\phi(t) &= (1 - bit)^{-a}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nM_X(t) &= \\mathbb E[\\exp(tX)]\\\\\n    &= \\int_0^\\infty \\exp(tx)\\frac{1}{b^a\\Gamma(a)} x^{a-1} \\exp[-x/b] \\mathrm{d}x\\\\\n    &= \\int_0^\\infty\\frac{1}{b^a\\Gamma(a)} x^{a-1} \\exp\\left[-\\frac{1-bt}{b}x\\right] \\mathrm{d}x\\\\\n    &= (1-bt)^{-a}\\int_0^\\infty\\frac{1}{(b/(1-bt))^a\\Gamma(a)} x^{a-1} \\exp\\left[-\\frac{1-bt}{b}x\\right] \\mathrm{d}x\\\\\n    &= (1-bt)^{-a}\n\\end{align*}\n\\]\nただし，積率母関数が定義されるためには\n\\[\n\\frac{1-bt}{b}&gt;0 \\Rightarrow t&lt; \\frac{1}{b}\n\\]\nが必要となることがわかります．特性関数は \\(\\phi(t) = (1 - bit)^{-a}\\)",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>ガンマ分布</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_101/fisher_exact_test.html",
    "href": "posts/statistical_hypothesis_test_101/fisher_exact_test.html",
    "title": "18  Fisher’s exact test",
    "section": "",
    "text": "\\(2\\times 2\\)クロスセル表とFisher’s exact test\n各グループの合計という周辺の値が固定されていると考えたとき，(Treated, Positive)の人数という確率変数が従う分布は超幾何分布とみなすことができる． つまり，\nとしたとき，\\(X_{11}\\)の確率は\n\\[\n\\begin{align*}\n\\Pr(X_{11}=x) &= \\frac{{}_{x_{1\\cdot}}C_{x}\\times {}_{x_{2\\cdot}}C_{x_{\\cdot 1} - x} }{{}_{N}C_{x_{\\cdot 1}}}\\\\\n              &= \\frac{x_{\\cdot 1}!x_{\\cdot 2}!x_{2\\cdot}!x_{2\\cdot}!}{x_{11}!x_{12}!x_{21}!x_{22}!N!}\n\\end{align*}\n\\]\nこのとき，\\(x\\) の範囲は \\(\\max(0, x_{1\\cdot} - x_{2\\cdot}) \\leq x \\leq \\min(x_{1\\cdot}, x_{\\cdot 1})\\) になる．\n▶  Null hypothesis vs Alternative hypothesis\n上記の問題設定におけるFisher’s exact testにおける検定仮説設定例はと，両側検定ならば\n\\(H_0\\)の仮定の下では，\\(X_{11}\\)は超幾何分布(hypergeometric distribution)に従うはずなので，この仮定に基づいてP値を計算します．両側検定でのP値の計算方法例として\n\\[\n\\begin{align*}\n\\text{p-value} = \\sum_{x} \\Pr(X_{11}={x}) \\text{ s.t } \\{x \\vert \\Pr(X_{11}={x}) \\leq \\Pr(X_{11}={x_{11}})\\}\n\\end{align*}\n\\]",
    "crumbs": [
      "統計的仮説検定",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_101/fisher_exact_test.html#times-2クロスセル表とfishers-exact-test",
    "href": "posts/statistical_hypothesis_test_101/fisher_exact_test.html#times-2クロスセル表とfishers-exact-test",
    "title": "18  Fisher’s exact test",
    "section": "References",
    "text": "問題設定 \nある医薬品試験のRCTにて，５０人の患者を無作為にtreatedとプラセボ(control)に分けて，一定期間後の健康状態(Positive vs Negative)を確認したところ 以下のような結果になった．\n\n\n\n\n\n \n\n\n\n\n\n\nTreated\n\n\n\n\nControl\n\n\n\n\n合計\n\n\n\n\n\n\nPositive\n\n\n\n\n21\n\n\n\n\n15\n\n\n\n\n36\n\n\n\n\n\n\nNegative\n\n\n\n\n4\n\n\n\n\n10\n\n\n\n\n14\n\n\n\n\n\n\n合計\n\n\n\n\n25\n\n\n\n\n25\n\n\n\n\n50\n\n\n\n\n\nこのとき，プラセボとグループと医薬品投入グループ間で健康状態分布が異なるかどうか検定したい．\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nTreated\n\n\n\n\nControl\n\n\n\n\n合計\n\n\n\n\n\n\nPositive\n\n\n\n\n\\(X_{11}\\)\n\n\n\n\n\\(X_{12}\\)\n\n\n\n\n\\(x_{1\\cdot}\\)\n\n\n\n\n\n\nNegative\n\n\n\n\n\\(X_{21}\\)\n\n\n\n\n\\(X_{22}\\)\n\n\n\n\n\\(x_{2\\cdot}\\)\n\n\n\n\n\n\n合計\n\n\n\n\n\\(x_{\\cdot 1}\\)\n\n\n\n\n\\(x_{\\cdot 2}\\)\n\n\n\n\n\\(N\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0\\): 処置(Treatment)と一定期間後の健康状態(主要評価項目)は独立\n\\(H_1\\): 処置(Treatment)と一定期間後の健康状態(主要評価項目)は独立ではない \\(\\Rightarrow \\Pr(\\text{Positive}\\vert \\text{Treated})\\neq \\Pr(\\text{Positive}\\vert \\text{Control})\\)\n\n\n\n\n\n\nCode\nimport math\nimport numpy as np\nimport polars as pl\nimport plotly.express as px\n\n\ndef compute_prob(\n    x: int, positive: int, negative: int, treated: int, denom: int\n) -&gt; np.float64:\n    return math.comb(positive, x) * math.comb(negative, treated - x) / denom\n\n\nDENOM = math.comb(50, 25)\nX_DOMAIN = np.arange(11, 25)\n\nprob = list(\n    map(\n        lambda x: compute_prob(x, positive=36, negative=14, treated=25, denom=DENOM),\n        X_DOMAIN,\n    )\n)\n\n# create polars.DataFrame\ndf = pl.DataFrame({\"x\": X_DOMAIN, \"prob\": prob})\n\n# plotly\nfig = px.bar(df, x=\"x\", y=\"prob\", title=\"Null hypothesis下における確率分布\")\nfig.update_layout(\n    xaxis_title=\"TreatedにおけるPositiveの人数\", yaxis_title=\"probability\"\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\nexact p-valueの計算\n ▶  片側検定\n\\(X_{11} \\geq x\\) となる場合のp-valueをscipy.stats.fisher_exactで計算すると以下のようになります．\n\nfrom scipy.stats import fisher_exact\ntable = np.array([[21, 15], [4, 10]])\nres_greater = fisher_exact(table, alternative='greater')\nprint(\"scipy-p-value: {:.6f}\".format(res_greater.pvalue))\n\nscipy-p-value: 0.056829\n\n\n一方，上で計算したprobabilityに則って上側確率を見てみると\n\nprint(\"self-computed-pvalue: {:.6f}\".format(df.filter((pl.col(\"x\") &gt;= 21))['prob'].sum()))\n\nself-computed-pvalue: 0.056824\n\n\nと数値計算誤差を無視してしまえば大まかに一致することが確認できます．\n ▶  両側検定\n両側検定におけるp-valueは\n\\[\n\\begin{align*}\n\\text{p-value} = \\sum_{x} \\Pr(X_{11}={x}) \\text{ s.t } \\{x \\vert \\Pr(X_{11}={x}) \\leq \\Pr(X_{11}={x_{11}})\\}\n\\end{align*}\n\\]\nなので\n\nthreshold = df.filter((pl.col(\"x\") == 21))[\"prob\"].to_numpy()[0]\nres_twosided = fisher_exact(table, alternative=\"two-sided\")\nmyres_twosided = df.filter((pl.col(\"prob\") &lt;= threshold))[\"prob\"].sum()\nprint(\"\"\"scipy-p-value: {:.6f},self-computed-pvalue: {:.6f}\n      \"\"\".format(res_twosided.pvalue, myres_twosided))\n\nscipy-p-value: 0.113657,self-computed-pvalue: 0.113652\n      \n\n\nどちらの計算でもおよそ \\(11.37\\%\\) であることが確認できます．\n\n📘 REMARKS \n\n組み合わせの数が大きすぎ，exact p-valueの計算が難しい場合はMonte Carlo法を用いて計算します\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTreatment\nT\nT\nT\nT\nT\nC\nC\nC\nC\nC\n\n\nPromoted\n1\n1\n1\n1\n0\n1\n1\n0\n0\n0\n\n\n\nというTreatedのうち４人がPromotedされたデータが得られた場合，TreatedかつPromotedの人数を \\(X\\) としたとき， \\(\\Pr(X \\geq 4)\\) のついて計算参する場合は\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTreatment\nT\nT\nT\nT\nT\nC\nC\nC\nC\nC\n\n\nPromoted\n1\n0\n1\n0\n0\n1\n1\n1\n1\n1\n\n\n\nのように２行目についてPermutationをランダムに \\(Y\\) 回実施してサンプリングから \\(\\Pr(X \\geq 4)\\) を計算します（上の例では \\(X = 3\\)）となっている．このように計算されたp-valueはfisher’s exact p-valueのMonte Carlo approximationと呼んだりします．\n\n\n\nOdds ratio\nscipy.stats.fisher_exactではpvalueのほかにstatisticという返り値をもっています．\n\nprint(res_twosided.statistic)\n\n3.5\n\n\nこの 3.5 はいわゆるodds ratioで\n\\[\n3.5 = \\frac{21 / 4}{15 / 10}\n\\]\nで計算されます．\n\nDef: Odds \n確率事象 \\(A\\) についての odds は以下のように計算される\n\\[\n\\begin{align*}\n\\text{odds}(A) = \\frac{\\Pr(A)}{1 - \\Pr(A)} = \\frac{\\Pr(A)}{\\Pr(A^c)}\n\\end{align*}\n\\]\n\noddsを用いることで表現がシンプルになるケースとしてフェアな賭けにおいける倍率の計算が上げられます． 例として，確率事象 A に対して1円を賭ける状況を考えます．確率事象 A が発生しなかったら1円を失い，確率事象 A が発生したら1円はキープ & x 円のリターンを得られるとします．\nこのとき，この賭けがフェアであるためには，期待利得が0であることが必要ですが，以下のように \\(x = \\text{odds}(A^c)\\) とリターンが設定されているとフェアな賭けになります．\n\\[\n\\begin{align*}\n&\\text{expected return} = x \\times \\Pr(A) + (-1) \\times \\Pr(A^c)\\\\\n&\\Rightarrow x = \\frac{\\Pr(A^c)}{\\Pr(A)} \\because \\text{exptected return should be 0}\\\\\n& \\Rightarrow x = \\text{odds}(A^c) = 1/\\text{odds}(A)\n\\end{align*}\n\\]\n\nDef: Odds ratio \nとある母集団にたいして，とある疾患の発症を抑制すると謳っている新薬を考えます．\n\n疾患が発症したならPositive, 発症しなかったらNegative\n新薬を処方されたらTreated, されなかったらControl\n\nとして，母集団の各組み合わせに対する事前割当確率が以下のようなクロスセルで定義されているとします．\n\n\n\n\n\n \n\n\n\n\n\n\nTreated\n\n\n\n\nControl\n\n\n\n\n\n\nPositive\n\n\n\n\n\\(p_{11}\\)\n\n\n\n\n\\(p_{12}\\)\n\n\n\n\n\n\nNegative\n\n\n\n\n\\(p_{21}\\)\n\n\n\n\n\\(p_{22}\\)\n\n\n\n\n\nこのとき，treated/control間の疾患発症のodds ratioは\n\\[\n\\text{odds ratio} = \\frac{p_{11}p_{22}}{p_{21}p_{12}}\n\\]\nで表現される．\n\n\n仮に Treated, Control両方のグループで疾患発症がレアなイベントだとすると \\(1- \\Pr(\\text{Positive} \\vert \\text{Treated}), 1-\\Pr(\\text{Positive} \\vert \\text{Control})\\) はともに十分小さくなり，\n\\[\n\\text{odds}(\\text{Positive}\\vert\\text{Treated}) \\approx Pr(\\text{Positive} \\vert \\text{Treated})\n\\]\nとみなせるので\n\\[\n\\frac{\\text{odds}(\\text{Positive}\\vert\\text{Treated})}{\\text{odds}(\\text{Positive}\\vert\\text{Control})} \\approx \\frac{Pr(\\text{Positive} \\vert \\text{Treated})}{Pr(\\text{Positive} \\vert \\text{Control})}\n\\]\nOdds ratioが0.7だとすると，Treated は Controlにくらべ 30% ほど疾患発症確率が低いという解釈に繋がります．\n\nOdds ratioの推定と信頼区間\n\n\n\n\n\n \n\n\n\n\n\n\nTreated\n\n\n\n\nControl\n\n\n\n\n合計\n\n\n\n\n\n\nPositive\n\n\n\n\n\\(x_{11}\\)\n\n\n\n\n\\(x_{12}\\)\n\n\n\n\n\\(x_{1\\cdot}\\)\n\n\n\n\n\n\nNegative\n\n\n\n\n\\(x_{21}\\)\n\n\n\n\n\\(x_{22}\\)\n\n\n\n\n\\(x_{2\\cdot}\\)\n\n\n\n\n\n\n合計\n\n\n\n\n\\(x_{\\cdot 1}\\)\n\n\n\n\n\\(x_{\\cdot 2}\\)\n\n\n\n\n\\(N\\)\n\n\n\n\n\n上記のようなデータについて，prior odds ratio \\(\\theta\\) の推定は\n\\[\n\\hat\\theta = \\frac{\\hat p_{11}\\hat p_{22}}{\\hat p_{21}\\hat p_{12}} \\  \\ \\text{where } \\hat p_{ij} = \\frac{x_{ij}}{N}\n\\]\n従って，\n\\[\n\\hat\\theta = \\frac{x_{11}x_{22}}{x_{21}x_{12}}\n\\]\n ▶  Confidence Intervalの計算\nConfidence Intervalは，実務では \\(\\log(\\theta)\\) を用いたCLTとdelta methodによる近似で計算されます．\n\\[\n\\begin{align*}\n\\mathbf p = (p_{11},p_{12},p_{21},p_{22})\n\\end{align*}\n\\]\nとしたとき，もともとのテーブルはクラス4の多項分布とみなせるので \\(\\mathbf p\\) についての共分散行列 \\(\\Sigma\\) は\n\\[\n\\begin{align*}\n\\Sigma = \\frac{1}{n}\\left(\n    \\begin{array}{cccc}\n    (1-p_{11}) p_{11} & -p_{11} p_{12} & -p_{11} p_{21} & -p_{11} p_{22} \\\\\n     -p_{11} p_{12} & \\left(1-p_{12}\\right) p_{12} & -p_{12} p_{21} & -p_{12} p_{22} \\\\\n     -p_{11} p_{21} & -p_{12} p_{21} & \\left(1-p_{21}\\right) p_{21} & -p_{21} p_{22} \\\\\n     -p_{11} p_{22} & -p_{12} p_{22} & -p_{21} p_{22} & (1-p_{22}) p_{22}\n    \\end{array}\n\\right)\n\\end{align*}\n\\]\nまた，\\(\\log(\\theta) = \\log(p_{11}) - \\log(p_{12}) - \\log(p_{21}) + \\log(p_{22})\\) についての分散はdelta methodを用いて\n\\[\n\\begin{align*}\n&\\operatorname{Var}(\\log(\\mathrm{OR})) = (\\nabla f \\Sigma )\\times \\nabla f^T\\\\\n&\\nabla f = \\left(\\frac{1}{p_{11}},-\\frac{1}{p_{12}},-\\frac{1}{p_{21}},\\frac{1}{p_{22}}\\right)\n\\end{align*}\n\\]\nと表せます．これを推定値 \\(\\hat p_{ij}\\) を用いて計算すると\n\\[\n\\begin{align*}\n&\\widehat{\\operatorname{Var}(\\log(\\operatorname{OR})}=\\frac{1}{x_{11}}+\\frac{1}{x_{12}}+\\frac{1}{x_{21}}+\\frac{1}{x_{22}}\\\\\n&\\widehat{\\operatorname{SE}(\\log(\\operatorname{OR})}=\\sqrt{\\frac{1}{x_{11}}+\\frac{1}{x_{12}}+\\frac{1}{x_{21}}+\\frac{1}{x_{22}}}\n\\end{align*}\n\\]\n\\(\\mathbf p\\) は \\(N\\) が十分大きいときCLTより正規分布に近似できると考えられるので \\(\\log(\\hat\\theta)\\) についてのConfidence Intervalは\n\\[\n\\text{CI(log odds ratio)} = \\widehat{\\log(\\operatorname{OR})}\\pm z_{1-\\alpha/2}\\times \\widehat{\\operatorname{SE}(\\log(\\operatorname{OR})}\n\\]\nまた，odds ratioのConfidence Intervalは対数を再度変換すれば良いので\n\\[\n\\text{CI(odds ratio)} = \\exp(\\widehat{\\log(\\operatorname{OR})}\\pm z_{1-\\alpha/2}\\times \\widehat{\\operatorname{SE}(\\log(\\operatorname{OR})})\n\\]\nと計算できる．\n\n📘 REMARKS \n\n上記の方法でのConfidence intervalは \\(\\log(\\hat\\theta)\\) 自体の推定分散ではなく，CLTを用いているのであくまで分散についての極限分布を用いている\n\\(p_{21}\\) や \\(p_{12}\\) 自体は0になり得ることを考えると，\\(\\hat\\theta\\) や \\(\\log(\\hat\\theta)\\) が存在しないことも考えられる\n\n\n\n\nReferences\n\nPennState STAT 504 &gt; 4.5 - Fisher’s Exact Test",
    "crumbs": [
      "統計的仮説検定",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_101/fisher_exact_test.html#references",
    "href": "posts/statistical_hypothesis_test_101/fisher_exact_test.html#references",
    "title": "18  Fisher’s exact test",
    "section": "",
    "text": "PennState STAT 504 &gt; 4.5 - Fisher’s Exact Test",
    "crumbs": [
      "統計的仮説検定",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_101/ks_test.html",
    "href": "posts/statistical_hypothesis_test_101/ks_test.html",
    "title": "19  コルモゴロフ・スミルノフ検定",
    "section": "",
    "text": "コルモゴロフ・スミルノフ検定の性質\n\\(X_1,\\cdots, X_n\\) を独立に分布関数 \\(F(x)\\) に従う確率変数とします．この確率変数が特定の分布関数 \\(F_0(x)\\) に従うかを検定する問題を考えます:\n\\[\nH_0: F(x) = F_0(x), H_1: \\exists x_0 \\in \\mathbb R \\text{ such that }F(x) \\neq F_0(x)\n\\]\nこの問題の検定統計量として\n\\[\nD_n = \\sup_{x} \\vert F_n(x) - F_0(x)\\vert\n\\]\nを用いた検定をコルモゴロフ・スミルノフ検定といいます．\\(F_n(x)\\) は経験分布関数です．\nLLN(大数の法則)より，任意の \\(x\\in\\mathbb R\\) について，\n\\[\nF_n(x) \\to \\mathbb E \\mathbb 1(X_i \\leq x) = P(X_i\\leq x) = F(x)\n\\]\nが成立します．",
    "crumbs": [
      "統計的仮説検定",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>コルモゴロフ・スミルノフ検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_101/ks_test.html#コルモゴロフスミルノフ検定の性質",
    "href": "posts/statistical_hypothesis_test_101/ks_test.html#コルモゴロフスミルノフ検定の性質",
    "title": "19  コルモゴロフ・スミルノフ検定",
    "section": "",
    "text": "Example 19.1 \n確率変数 \\(X \\sim \\operatorname{U}(0,1)\\) を \\(Y = -log(X)\\) と変数変換すると\n\\[\nY \\sim \\operatorname{Exp}(1)\n\\]\nに従います．25個の標準一様分布に従う乱数を発生させ，それに基づいてKS test statisticを計算すると以下のようになります．\n\n\nCode\nimport numpy as np\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nfrom regmonkey_style.config import CONFIG\nfrom regmonkey_style import stylewizard as sw\n\nsw.set_templates(\"regmonkey_twoline\")\n\nnp.random.seed(2222)\n\n# DGP\nN = 25\nsample = -np.log(np.random.uniform(0, 1, N))\necdf = ECDF(sample)\nx = np.linspace(min(sample), max(sample), 1000)\ntrue_cdf = stats.expon.cdf(x)\necdf_values = ecdf(x)\n\n# compute statistic\nsup_idx, ks_statistic = np.argmax(np.abs(true_cdf - (ecdf_values))), np.max(\n    np.abs(true_cdf - (ecdf_values))\n)\nsup_x = x[sup_idx]\n\nfig, ax = plt.subplots()\n\nax.step(x, ecdf_values, label=\"Empirical CDF\")\nax.plot(x, true_cdf, label=\"True CDF\")\nax.annotate(\n    \"\",\n    xy=(sup_x, min(stats.expon.cdf(sup_x), ecdf(sup_x))),\n    xytext=(sup_x, max(stats.expon.cdf(sup_x), ecdf(sup_x))),\n    arrowprops=dict(\n        arrowstyle=\"&lt;-&gt;\", color=\"black\", linestyle=\"dashed\"\n    ),\n)\nax.text(\n    sup_x + 0.05,\n    (true_cdf[sup_idx] + ecdf_values[sup_idx]) / 2,\n    f\"KS Statistic: {ks_statistic:.3f}\",\n    color=\"black\",\n    ha=\"left\"\n)\nax.set_title(\"KS test statistic\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nscipy.stats.ks_1samp でexactにテストをしてみると\n\nstats.ks_1samp(sample, stats.expon(0).cdf, method='exact')\n\nKstestResult(statistic=np.float64(0.16519761568776414), pvalue=np.float64(0.45428250331574216), statistic_location=np.float64(0.5022002367852737), statistic_sign=np.int8(1))\n\n\n\n\nDef: 経験分布関数 \n\\(n\\) 個の実数値データ \\(D = \\{x^{(i)}\\in\\mathbb R\\vert i=1,\\cdots, n\\}\\) が与えられたとき，経験分布 \\(\\operatorname{Emp}(D)\\) の確率関数 \\(f_n(x)\\) は次のように定義される\n\\[\n\\begin{gather}\nf_n(x) = \\frac{1}{n}\\sum_{i=1}^n\\delta(x - x^{(i)})\\\\\n\\text{where }\\delta(\\cdot) \\text{ : Dirac delta function}\n\\end{gather}\n\\]\n累積分布関数（経験分布関数） \\(F_X(x)\\) は次のように定義される\n\\[\n\\begin{align*}\nF_n(x)\n    &= \\frac{1}{n}\\sum_{i=1}^n\\int^x_{-\\infty}\\delta(z - x^{(i)})\\,\\mathrm{d}z\\\\\n    &= \\frac{1}{n}\\mathbb 1(x^{(i)\\leq x})\n\\end{align*}\n\\]\n\n\n\n\n\n\nTheorem 19.1 \n\\(F(x)\\) が連続のとき，\n\\[\n\\sup_{x\\in\\mathbb R} \\vert F_n(x) - F(x)\\vert\n\\tag{19.1}\\]\nの分布は \\(F\\) に依存しない．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n累積分布関数 \\(F\\) の逆関数を次のように定義します\n\\[\nF^{-1}(y) = \\min \\{x: F(x)\\leq y\\}\n\\]\n\\(F^{-1}\\) を用いて Equation 19.1 を次のように変形します\n\\[\n\\begin{align*}\nP(\\sup_{x\\in\\mathbb R} \\vert F_n(x) - F(x)\\vert \\leq t)\n    = P(\\sup_{y\\in [0, 1]} \\vert F_n(F^{-1}(y)) - y\\vert \\leq t)\n\\end{align*}\n\\]\n経験分布関数 \\(F_n\\) について\n\\[\n\\begin{align*}\nF_n(F^{-1}(y))\n    &= \\frac{1}{n}\\sum \\mathbb 1(X_i \\leq F^{-1}(y))\\\\\n    &= \\frac{1}{n}\\sum \\mathbb 1(F(X_i) \\leq y)\n\\end{align*}\n\\]\n従って，\n\\[\nP(\\sup_{y\\in [0, 1]} \\vert F_n(F^{-1}(y)) - y\\vert \\leq t)\n    = P\\left(\\sup_{y\\in [0, 1]} \\left\\vert \\frac{1}{n}\\sum \\mathbb 1(F(X_i) \\leq y) - y\\right\\vert \\leq t\\right)\n\\]\n任意の分布の累積分布関数は標準一様分布に従うので，\n\\[\nF(X_i) \\sim \\operatorname{U}(0, 1)\n\\]\n\\(U_i = F(X_i) \\ \\ \\text{for } i \\in \\{1, \\cdots, n\\}\\) とすると，\n\\[\nP(\\sup_{y\\in [0, 1]} \\vert F_n(F^{-1}(y)) - y\\vert \\leq t)\n    = P\\left(\\sup_{y\\in [0, 1]} \\left\\vert \\frac{1}{n}\\sum \\mathbb 1(U_i \\leq y) - y\\right\\vert \\leq t\\right)\n\\]\n以上より， Equation 19.1 の分布は \\(F\\) に依存しないことが示されました．",
    "crumbs": [
      "統計的仮説検定",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>コルモゴロフ・スミルノフ検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html",
    "title": "20  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "",
    "text": "Fisher流検定の考え方\n２標本問題を考えたとき，２標本の平均の差がそのバラツキの大きさ（＝標準誤差）と比べて大きければ大きいほど 「母集団に差があり」のエビデンス力が高いという考えがFisher流検定となります．P値自体はサンプルサイズに依存すると留意していましたが， Fisher流ではP値が小さいほどエビデンス力が高いという解釈になります．さらに，\nというモノサシの提案をFisherはしました．これが現在の有意水準(significance level) 5% という慣習の由来であると言われています．\nなお，FisherはP値を統計家がデータの解析結果を「報告」するときのモノサシとしての提案にとどまっており， 効果があったか否かの「判定」は，統計家だけでなく関連専門家が参加するグループ討議によって，報告されたP値，分析対象，サンプルサイズ等を吟味して総合的に「判定」すべきであると考えてます．",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#fisher流検定の考え方",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#fisher流検定の考え方",
    "title": "20  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "",
    "text": "平均の差が標準誤差の２倍未満であれば平均の差はバラツキによる差であって考慮に値しない，\n2倍以上の差があるとき，偶然のみに支配されたバラツキに比べると指標の値が相対的に大きいと言える→初めて科学的に意味のある差であるか否かを検討する対象になりうる（正規分布を仮定したとき，約5％水準に相当）",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#neyman-pearson流検定の考え方",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#neyman-pearson流検定の考え方",
    "title": "20  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "Neyman-Pearson流検定の考え方",
    "text": "Neyman-Pearson流検定の考え方\nNeyman-Pearson流は統計的検定について\n\n\\(H_0: \\theta \\in \\Theta_0\\), Null hypothesis\n\\(H_1: \\theta \\not\\in \\Theta_0\\), Alternative hypothesis\n\nの２つを設定し，観察されたデータに基づいてどちらの仮説がより妥当な仮説であるかを判定する問題という統計的判定問題を考えました． 統計的判定問題のおける判定の誤りについて，\n\nType I Error: \\(H_0\\) が正しいのに誤って \\(H_0\\) を棄却するエラー\nType II Error: \\(H_1\\) が正しいのに誤って \\(H_0\\) を採択するエラー\n\nの２種類があるとし，Type I Errorの確率を \\(\\alpha\\) に抑えた上で，Type II Errorの確率 \\(\\beta\\) を最小にする制約付き最小化問題 として統計的判定問題を定式化しました．\n\n\n\n\n\n \n\n\n\n\nTruth\n\n\n\n\n\n\n\\(H_0\\)\n\n\n\n\n\\(H_1\\)\n\n\n\n\n\n\n\n\n\n\n\n検定結果\n\n\n\n\n\\(H_0\\)\n\n\n\n\n正しい(\\(1- \\alpha\\))\n\n\n\n\nType II Error(\\(\\beta\\))\n\n\n\n\n\n\n\\(H_1\\)\n\n\n\n\nType I Error(有意水準: \\(\\alpha\\))\n\n\n\n\n正しい（検出力: \\(1 - \\beta\\)）\n\n\n\n\n\n検定問題に対応する 検定統計量 \\(T\\), \\(H_0\\) の棄却域を \\(R\\) で表すとそれぞれ以下のように表現されます\n\nType I Error rate, \\(\\alpha = \\Pr(T \\in R \\vert H_0)\\)\nType II Error rate: \\(\\beta = \\Pr(T \\not\\in R \\vert H_1)\\)\n\nFisher流ではP値の大きさがエビデンス力という意味を持つことに対して，Neyman-Pearson流では\n\n事前に定められた有意水準 \\(\\alpha\\) をP値が下回るなら効果ありとの判定\nそうでないなら，効果なしとの判定\n\\(P = 0.00001\\) だろうが \\(P = 0.049\\) だろうがP値の水準自体には意味を求めない\n\nという違いがあります．\n\n📘 REMARKS \nNeyman-Pearson流では， \\(\\alpha, \\beta\\) を用いて統計的に効果があると言えるか？という統計的判定問題として仮説検定を定式化しましたが，\n\n統計的検定は決定するための方法ではなく，結果を報告するための方法である by F.Mostelller(1916-2006)\n\nと理解するにとどめたほうが良いとされています．",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#探索的リサーチと検証的リサーチ",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#探索的リサーチと検証的リサーチ",
    "title": "20  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "探索的リサーチと検証的リサーチ",
    "text": "探索的リサーチと検証的リサーチ\n特定の疾患をターゲットとして行われる医薬品の開発過程（詳細はこちら）を例にすると，\n\n候補化学物質について，発がん性試験，変異原性試験，薬効薬理研究など様々な試験をラットや細胞に対して探索的に実施\n健常なヒトを対象に臨床第I相試験として，安全性や薬物動態などを探索的に研究\n当該疾患の患者を対象に第II相臨床試験として，病気の程度によってどのような効き目を発揮するのか（有効性）、副作用はどの程度か（安全性）、またどのような使い方（投与量・間隔・期間など）をしたらよいか、を研究\n第III相臨床試験として，医薬品の有効性と安全性をRCTで検証\n\n第III相臨床試験においては，TreatmentのEffect Sizeの想定と十分なサンプルサイズを確保した上でRCTを実施，そして得られたデータに基づいて統計的意味における効果の有無を検証しています． このようなリサーチを検証的リサーチといいます．\n一方，それまでの動物試験，非臨床試験，臨床第I相試験，臨床第II相試験では，Effect Sizeやサンプルサイズが事前に統計的に設定される場合は少なく，あくまで 次の分析ステップに進む値するエビデンス収集や仮説立案という目的で実施されるリサーチです．このような分析を探索的リサーチと呼びます．\n\n検証的リサーチにおける仮説検定手順\nとあるPopulationを対象に実施するTreatmentの効果をRCTで仮説検定検証する場合，基本的には次のような一連の手順で実施します．\n\nTable: 検証的リサーチ手順\n\n\n\n\n\n\n手順\n説明\n\n\n\n\n手順(1)\n主要評価項目 \\(\\delta\\) を定義し，期待される水準 \\(\\delta_0\\) を見積もる\n\n\n手順(2)\n\\(H_0: \\delta = 0, H_1: \\delta \\neq 0\\) のようにHypothesesを言語化する\n\n\n手順(3)\n有意水準 \\(\\alpha\\), 検出力 \\(1 - \\beta\\) を定める\n\n\n手順(4)\n有意水準 \\(\\alpha\\), 検出力 \\(1 - \\beta\\) のもとで \\(\\delta_0\\) を検出するための必要サンプルサイズを計算する\n\n\n手順(5)\nPopulationからランダムにEntityをサンプリングして，手順(4)のサンプルサイズを満たすようにEntityをランダム or 層化ランダムでtreated/controlに割り当てる\n\n\n手順(6)\ntreated, controlのバランスチェック\n\n\n手順(7)\ntreated, controlがともに実験から逸脱しない形でそれぞれ処置を受けることを観察(= プロトコル遵守の確保)\n\n\n手順(8)\ntreated, controlのデータを収集し，Attritionなどの対応を実施した上で，主要評価項目, 検定統計量を計算\n\n\n手順(9)\n手順(8)で計算された検定統計量を元に，統計的検定を実施し，P値が有意水準 \\(\\alpha\\) 以下ならば効果があると統計的判断を下し，それ以外の場合では \\(H_0\\) が棄却できなかったとする\n\n\n\n上記の手順に則って，\\(H_0\\) が棄却された場合，少なくとも \\(\\delta \\geq \\delta_0\\) なのだろうという統計的判断がなされます．\n\n\n探索的リサーチと仮説検定\n探索的リサーチでは，多くの場合，サンプルサイズや特徴量バランスがコントロールできない観察データを対象に分析し， また次のリサーチに進むための仮説構築や検討に値する特徴量スクリーニングを目的とすることが多いです．このとき検定を実施するとしても，有意水準，検出力，Effect Sizeを想定した Neyman-Pearson流の検定の実施は難しく，偶然のバラツキにしては差が大きそうというインサイトを得ることを目的としたFisher流仮説検定の用い方となります．\nただ，P値に基づいて推論を行うのではなく，平均の差やハザード比などの指標や信頼区間，またその分野のドメイン知識を考慮した上で， 総合的に結果を解釈→仮説の構築をすることが重要です．",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#appendix-新薬誕生までのプロセス",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#appendix-新薬誕生までのプロセス",
    "title": "20  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "Appendix: 新薬誕生までのプロセス",
    "text": "Appendix: 新薬誕生までのプロセス\n\n\n\n出典: 治験の３つのステップ，群馬大学医学部附属病院 先端医療開発センター臨床研究推進部\n\n\n\nTable: 各工程における分析目的\n\n\n\n\n\n\n工程\n説明\n\n\n\n\n新規物質の探索・創製\n薬になりそうな新しい物質を探したり，作り出したりすること\n\n\n物理的化学的研究\n新規物質の構造や物理的・化学的な性状などを調べること\n\n\n薬効薬理研究\nどのような効果があるか，どのようなメカニズムで効果を現すのかなどを調べること\n\n\n薬物動態研究\nどのように，体内に吸収され，臓器などに分布し，代謝されて排泄されるかなどを調べること\n\n\n一般薬理研究\nどのような部位にどんな作用を及ぼすかなど，薬効薬理作用以外の安全性に関する作用を調べること\n\n\n一般毒性研究\n投与期間を短・中・長期などに分けて，毒性（安全性）を広く調べること\n\n\n特殊毒性研究\n発がん性や胎児への影響がないかなど，特別な毒性（安全性）を調べること\n\n\n臨床第I相試験（臨床薬理試験）\n少数の健康成人などについて，主に安全性や薬物動態などを調べる試験\n\n\n臨床第II相試験（探索的試験）\n比較的少数の患者さんについて，有効性と安全性などを調べる試験\n\n\n臨床第III相試験（検証的試験）\n多数の患者さんについて，標準的な「くすり」などと比較して有効性と安全性を確認する試験\n\n\n製造販売後調査\n製造販売後に多くの患者さんに使用されたときの安全性や有効性などの情報を集め，それを分析・ 評価して医療関係者などに伝えること",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/estimation/method-of-moments.html",
    "href": "posts/estimation/method-of-moments.html",
    "title": "21  モーメント法",
    "section": "",
    "text": "モーメント法\n\\(X_1, \\cdots, X_n\\) を確率関数 \\(f(x\\vert \\pmb\\theta), \\pmb\\theta(\\theta_1, \\cdots, \\theta_k)\\) に独立に従う確率変数とします． 各 \\(j\\)-th (\\(1 \\leq j \\leq k\\))までのモーメントを\n\\[\n\\mathbb E[X^j] = g_j(\\pmb\\theta)\n\\]\nとする．\n\\[\nX_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x\\vert \\pmb\\theta)\n\\]\nなので，大数の法則より\n\\[\n\\frac{1}{n}\\sum_{i=1}^nX^j_i \\overset{\\mathrm{p}}{\\to} \\mathbb E[X^j]\n\\]\nこれを用いて，\n\\[\n\\left\\{\\begin{array}{c}\n\\frac{1}{n}\\sum_{i=1}^nX_i = g_1(\\theta_1, \\cdots, \\theta_k)\\\\\n\\vdots\\\\\n\\frac{1}{n}\\sum_{i=1}^nX_i^j = g_j(\\theta_1, \\cdots, \\theta_k)\\\\\n\\vdots\\\\\n\\frac{1}{n}\\sum_{i=1}^nX_i^k = g_k(\\theta_1, \\cdots, \\theta_k)\n\\end{array}\\right.\n\\]\nという連立方程式を考えます．方程式の個数は未知パラメーターの個数と等しくなるように取ります． この連立方程式を \\(\\pmb\\theta\\) について解いた解 \\(\\widehat{\\pmb{\\theta}}\\) をモーメント推定量と呼びます．",
    "crumbs": [
      "統計的推定",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>モーメント法</span>"
    ]
  },
  {
    "objectID": "posts/estimation/method-of-moments.html#モーメント法",
    "href": "posts/estimation/method-of-moments.html#モーメント法",
    "title": "21  モーメント法",
    "section": "",
    "text": "Example 21.1 : 一様分布 \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} \\operatorname{U}[a, b]\\) であるとします． パラメータ \\((a, b)\\) は未知であるとします．\n\\[\n\\begin{align*}\n\\mathbb E[X_i] &= \\frac{a + b}{2}\\\\\n\\mathbb E[X_i^2] &= \\frac{(b-a)^2}{12}\n\\end{align*}\n\\]\nより\n\\[\n\\begin{align*}\n\\mu_1 = \\frac{1}{n}\\sum X_i &= \\frac{\\hat a +\\hat  b}{2}\\\\\n\\mu_2 = \\frac{1}{n}\\sum X_i^2 &= \\frac{1}{3}(\\hat a^2 + \\hat a\\hat b + \\hat b^2)\n\\end{align*}\n\\]\nこれを整理すると，\n\\[\n\\begin{align*}\n&4\\mu_1^2 - 3\\mu_2 = ab\\\\\n&\\Rightarrow a^2 - 2\\mu_1 a + 4\\mu_1^2 - 3\\mu_2\n\\end{align*}\n\\]\n従って，\\(a &lt; b\\) に注意すると\n\\[\n\\begin{align*}\n\\hat{a} = \\mu_1 - \\sqrt{3(\\mu_2-\\mu_1^2)} \\\\\n\\hat{b} = \\mu_1 + \\sqrt{3(\\mu_2-\\mu_1^2)}\n\\end{align*}\n\\]\nを得ます．\n ▶  Pythonでモーメント法の性質を確認する\n\\(X\\sim\\operatorname{U}[a, b]\\) について, \\(\\tilde a = \\min(X), \\tilde b = \\max(X)\\) でも推定できるので モーメント法(mm_a, mm_bとした)とmin-max推定量をbias及びefficiencyの観点から比較してみます．\n\\(X\\sim\\operatorname{U}[-3, 5]\\) について 100サンプルサイズを一度に生成し，それを500回繰り返しました． バッチごとに \\(a, b\\) を推定しています．\n\n\nCode\nimport numpy as np\nimport polars as pl\nimport plotly.express as px\n\nnp.random.seed(42)\n\na, b, N = -3, 5, 100\n\n\ndef compute_estimates():\n    X = np.random.uniform(a, b, N)\n    mu_1 = np.mean(X)\n    mu_2 = np.mean(X**2)\n    mm_term = np.sqrt(3 * (mu_2 - mu_1**2))\n    return min(X), max(X), mu_1 - mm_term, mu_1 + mm_term\n\n\ndf = pl.DataFrame(\n    list(map(lambda x: compute_estimates(), range(500))),\n    orient=\"row\",\n    schema=[\"min_a\", \"max_b\", \"mm_a\", \"mm_b\"],\n)\n\npx.histogram(\n    df,\n    x=[\"min_a\", \"mm_a\"],\n    opacity=0.7,\n    histnorm='probability',\n    title=\"\"\"\n    mm-estimator (mean, var)= ({:.2f}, {:.2f})&lt;br&gt;min-max-estimator (mean, var)= ({:.2f}, {:.2f})\n    \"\"\".format(\n        np.mean(df.select(\"mm_a\").to_numpy()),\n        np.var(df.select(\"mm_a\").to_numpy()),\n        np.mean(df.select(\"min_a\").to_numpy()),\n        np.var(df.select(\"min_a\").to_numpy()),\n    ))\n\n\n                                                \n\n\n\n\nCode\npx.histogram(\n    df,\n    x=[\"max_b\", \"mm_b\"],\n    opacity=0.7,\n    histnorm='probability',\n    title=\"\"\"\n    mm-estimator (mean, var)= ({:.2f}, {:.2f})&lt;br&gt;min-max-estimator (mean, var)= ({:.2f}, {:.2f})\n    \"\"\".format(\n        np.mean(df.select(\"mm_b\").to_numpy()),\n        np.var(df.select(\"mm_b\").to_numpy()),\n        np.mean(df.select(\"max_b\").to_numpy()),\n        np.var(df.select(\"max_b\").to_numpy()),\n    ))\n\n\n                                                \n\n\n\n\nExample 21.2 : 正規分布のモーメント法パラメーター推定 \n\\(X_i \\overset{\\mathrm{iid}}{\\sim} N(\\mu, \\sigma^2)\\) のとき，\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\mu\\\\\n\\mathbb E[X^2] &= \\sigma^2 + \\mu^2\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\frac{1}{n}\\sum X_i &= \\hat\\mu\\\\\n\\frac{1}{n}\\sum(X_i - \\overline X)^2 &= \\hat\\sigma^2\n\\end{align*}\n\\]",
    "crumbs": [
      "統計的推定",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>モーメント法</span>"
    ]
  },
  {
    "objectID": "posts/estimation/m-estimator.html",
    "href": "posts/estimation/m-estimator.html",
    "title": "22  M-Estimator",
    "section": "",
    "text": "M-Estimator\n\\(\\rho(\\cdot)\\) はロス関数(一般的には非負の関数)とも呼ばれたりします．この形状を操作することで，M-estimatorは\nに対応させることができます．\n\\[\n\\begin{gather}\n\\frac{\\mathrm{d}}{\\mathrm{d \\theta}} \\rho(x_i;\\theta) = 0\\\\\n\\sum_i \\varphi(x_i;\\theta) = 0 \\text{という形で表されることが多い}\n\\end{gather}\n\\]\nから導出される方程式を核関数(以下, \\(\\varphi(\\cdot)\\) で表記)とよんだりします．\n▶  標本平均とMedianとM-estimator\n標本平均 \\(\\overline{X}\\) もM-estimatorの枠組みで理解でき，\n\\[\n\\begin{align*}\n\\rho(x-\\mu) &= (x-\\mu)^2\\\\\n\\varphi(x-\\mu) &= (x-\\mu)\n\\end{align*}\n\\]\nに対応します．また，メディアンは\n\\[\n\\begin{align*}\n\\rho(x-a) &= \\vert x-a\\vert \\\\\n\\varphi(x-a) &= -\\mathbb 1(x &lt; a) + \\mathbb 1(x &gt; a)\n\\end{align*}\n\\]\nに対応します．",
    "crumbs": [
      "統計的推定",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>M-Estimator</span>"
    ]
  },
  {
    "objectID": "posts/estimation/m-estimator.html#m-estimator",
    "href": "posts/estimation/m-estimator.html#m-estimator",
    "title": "22  M-Estimator",
    "section": "",
    "text": "Def: M-estimator \n\\((x_i, y_i)\\) を母集団からのサイズ \\(n\\) のi.i.d ランダムサンプリングとする．このとき，\n\\[\n\\begin{align*}\n&\\hat\\beta = \\arg\\min_{\\beta} \\sum_{i=1}^n \\rho(x_i, y_i;\\beta)\\\\\n&\\text{where } \\rho \\text{ is some suitably chosen function}\n\\end{align*}\n\\] として推定量を考えるとき，これをM-estimatorと呼ぶ．\n\n\n\nMLE\nロバスト推定量\nベイズ事前分布に基づく推定量\n\n\n\n\n\n\n\n\n\n\n\nExample 22.1 : 正規分布パラメーターについてのMLE \n確率変数 \\(x_i\\sim N(\\mu, \\sigma^2)\\) について，\\(\\theta = (\\mu, \\sigma^2)\\) を推定したいとします．このとき，\n\\[\n\\begin{align*}\n\\hat\\theta\n    &= \\arg\\max \\sum_{i=1}^n\\log(\\phi(x_i; \\theta))\\\\\n    &= \\arg\\max \\sum_{i=1}^n \\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2} - \\frac{1}{2}\\log\\sigma^2- \\frac{1}{2}\\log(2\\pi)\\right)\n\\end{align*}\n\\]\n\\(\\mu, \\sigma^2\\) についてFOCを整理すると，\n\\[\n\\begin{align*}\n&\\sum_{i=1}^n \\frac{x_i - \\hat\\mu}{\\hat\\sigma^2} = 0\\\\\n&\\sum_{i=1}^n \\left(\\frac{(x_i - \\hat\\mu)^2}{2(\\hat\\sigma^2)^2} - \\frac{1}{2\\hat\\sigma^2}\\right) = 0\n\\end{align*}\n\\]\nつまり，\n\\[\n\\begin{align*}\n&\\sum_{i=1}^n(x_i - \\hat\\mu) = 0\\\\\n&\\sum_{i=1}^n[(x_i - \\hat\\mu)^2 - \\hat\\sigma^2] = 0\n\\end{align*}\n\\]\nこれをM-estimatorと対応させるならば，\\(\\rho(x_i; \\theta) = -\\log(\\phi(x_i))\\) と尤度に基づいたロス関数を用いた問題と理解できます．\nまた，\\(\\mu\\) に焦点をあてるならば，推定量は核関数が \\(\\varphi(x; \\theta)= x-\\mu\\) となります． 核関数の形状より，データ \\(x\\) が非常に大きくなると，核関数 \\(\\varphi(x; \\theta)\\) も非常に大きくなってしまいます． 結果として，外れ値 \\(x_{outlier}\\) が存在すると，M推定量が一つの \\(\\varphi(x_{outlier}; \\theta)\\) に振り回されてしまい， ロバスト推定にならないことがわかります．",
    "crumbs": [
      "統計的推定",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>M-Estimator</span>"
    ]
  },
  {
    "objectID": "posts/estimation/m-estimator.html#平均の推定と核関数の形状",
    "href": "posts/estimation/m-estimator.html#平均の推定と核関数の形状",
    "title": "22  M-Estimator",
    "section": "平均の推定と核関数の形状",
    "text": "平均の推定と核関数の形状\n\nimport numpy as np\n\nnp.random.seed(42)\n\nmu, sigma = 0, 1 # mean and standard deviation\nX_0 = np.random.normal(mu, sigma, 1000)\nX_1 = np.append(X_0, [100, 200, 50])\n\nprint(\n    \"\"\"X_0: sample mean = {:.2f}, median = {:.2f}\\nX_1: sample mean = {:.2f}, median = {:.2f}\n      \"\"\".format(\n        np.mean(X_0), np.median(X_0), np.mean(X_1), np.median(X_1)\n    )\n)\n\nX_0: sample mean = 0.02, median = 0.03\nX_1: sample mean = 0.37, median = 0.03\n      \n\n\nというOutlier混入データ X_1 について，核関数の形状をコントロールするとどのように推定されるのか確認していきます．\n\n刈り込み型\n\\[\n\\varphi(x-\\mu)\n    = \\left\\{\\begin{array}{c}\n    x - \\mu & \\vert x-\\mu\\vert \\leq c\\\\\n    0 & \\vert x-\\mu\\vert &gt; c\n    \\end{array}\\right.\n\\]\nという核関数を刈り込み型(trimmed type)と呼びます． ある閾値 \\(c\\) 以上のデータは使用しないことを意味し，刈り込み型平均の計算と対応します．\n\n\nCode\nfrom scipy import stats\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n## plot func\ndef plot_weights(support, weights_func, xlabels, xticks, title):\n    fig = plt.figure(figsize=(6, 5))\n    ax = fig.add_subplot(111)\n    ax.plot(support, support * weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_title(title)\n    return ax\n\n## M-estimator instance\nnorms = sm.robust.norms\n\n## 可視化\nc = 2\nsupport = np.linspace(-3 * c, 3 * c, 1000)\ntrimmed = norms.TrimmedMean(c=c)\nplot_weights(support, trimmed.weights, [\"-3*c\", \"0\", \"3*c\"], [-3 * c, 0, 3 * c], title='trimmed-type weight with c=2, mu=0')\n\n\n\n\n\n\n\n\n\n ▶  Pythonでの推定\n\n\nCode\nfrom plotly import express as px\n\ndef fit_trimmed_mean(c):\n    return (\n        sm.RLM(X_1, np.ones(len(X_1)), sm.robust.norms.TrimmedMean(c)).fit().params[0]\n    )\n\n\nc_range = np.linspace(0.01, 200, 50)\nrobust_means = list(map(fit_trimmed_mean, c_range))\nfig = px.line(\n    x=c_range,\n    y=robust_means,\n    markers=\"x\",\n    title='trimmed robust regressioon',\n    labels={'x': 'c-values', 'y':'estimated mean'}\n)\nfig.show()\n\n\n                                                \n\n\nX_1 のうち，outliers [100, 200, 50] を除去した平均が \\(0.02\\), outliers込の平均が \\(0.37\\) であることを考えると，適切な \\(c\\) の設定によって，outlier-robustな平均の推定ができています．また，cのレベルに応じて，loss functionで考慮されるサンプルが増えていくのは刈り込み平均の水準変化と対応していることがわかります．\n一方，数値計算上小さすぎるcを選択すると，何も除去されていない平均が推定値として返されてしまっている点に注意が必要です．\n\n\nフーバー型\n\\[\n\\varphi(x-\\mu)\n    = \\left\\{\\begin{array}{c}\n    -c & x -\\mu &lt; -c\\\\\n    x - \\mu & \\vert x-\\mu\\vert \\leq c\\\\\n    c & x-\\mu &gt; c\n    \\end{array}\\right.\n\\]\nという核関数をフーバー型と呼びます．閾値 \\(c\\) より離れたデータを全く使わない刈り込み型と比べ，影響度は弱めるがある程度は使うことを意図した核関数といえます．フーバー型は凸最適化の解になるので，推定値を得やすいという利点があります．\nただし，\n\\[\n\\lim_{\\vert x\\vert\\to\\infty}\\varphi(x) \\neq 0\n\\]\nであるので，Outlierの影響が必ずしもゼロになるわけではない = 最降下(redescending)の性質を持たない点に注意してください．\n\n\nCode\n## 可視化\nc = 2\nsupport = np.linspace(-3 * c, 3 * c, 1000)\nhuber_type = np.clip(support, -c, c)\n\nfig = plt.figure(figsize=(6, 5))\nax = fig.add_subplot(111)\nax.plot(support, huber_type)\nax.set_xticks([-3 * c, 0, 3 * c],)\nax.set_xticklabels([\"-3*c\", \"0\", \"3*c\"], fontsize=16)\nax.set_title('Huber-type with c=2, mu=0')\nplt.show()\n\n\n\n\n\n\n\n\n\n ▶  Pythonでの推定\n\n\nCode\ndef fit_huber_mean(c):\n    return (\n        sm.RLM(X_1, np.ones(len(X_1)), sm.robust.norms.HuberT(t=c)).fit().params[0]\n    )\n\n\nc_range = np.linspace(0.01, 200, 50)\nrobust_means = list(map(fit_huber_mean, c_range))\nfig = px.line(\n    x=c_range,\n    y=robust_means,\n    markers=\"x\",\n    title='Huber type robust regressioon',\n    labels={'x': 'c-values', 'y':'estimated mean'}\n)\nfig.show()\n\n\n                                                \n\n\ntrimmed mean regressionと比べ，滑らかな推定曲線となっています．cの水準の設定に応じてoutlierの影響が弱められていますが，少なからず影響が残ってしまう点に注意してください．\n\n\nTukey’s Biweight型\n\\[\n\\varphi(x-\\mu)\n    = \\left\\{\\begin{array}{c}\n    (x-\\mu)\\left\\{1 - \\left(\\frac{x-\\mu}{c}\\right)^2\\right\\}^2 & \\vert x - \\mu \\vert \\leq c\\\\\n    0 & \\vert x - \\mu \\vert &gt; c\n    \\end{array}\\right.\n\\]\nという核関数をTukey’s Biweight型と呼びます．平均 \\(\\mu\\) から離れていくと，核関数の関与が段々と増えていき，あるところからは減り，最終的には 0 になるという性質があります．つまり，最降下の性質を持つ核関数です．\n望ましい性質をもっているので，非常によく使われる核関数です．\n\n\nCode\ndef tukeys_biweight(x, mu, c):\n    x2 = np.clip(x-mu, -c, c)\n    return x2 * (1 - (x2/c)**2) ** 2\n\n## 可視化\nc = 2\nsupport = np.linspace(-3 * c, 3 * c, 1000)\ntukey_kernel = tukeys_biweight(support, 0, c)\n\nfig = plt.figure(figsize=(6, 5))\nax = fig.add_subplot(111)\nax.plot(support, tukey_kernel)\nax.set_xticks([-3 * c, 0, 3 * c],)\nax.set_xticklabels([\"-3*c\", \"0\", \"3*c\"], fontsize=16)\nax.set_title('Tukeys Biweight-type with c=2, mu=0')\nplt.show()\n\n\n\n\n\n\n\n\n\n ▶  Pythonでの推定\n\n\nCode\ndef fit_tukeysbiwieght_mean(c):\n    return (\n        sm.RLM(X_1, np.ones(len(X_1)), sm.robust.norms.TukeyBiweight(c=c)).fit().params[0]\n    )\n\n\nc_range = np.linspace(0.01, 200, 50)\nrobust_means = list(map(fit_tukeysbiwieght_mean, c_range))\nfig = px.line(\n    x=c_range,\n    y=robust_means,\n    markers=\"x\",\n    title='tukeys biwieght type robust regressioon',\n    labels={'x': 'c-values', 'y':'estimated mean'}\n)\nfig.show()\n\n\n                                                \n\n\n\n\n重み付き型\n正規分布の密度関数のベキ乗 \\(\\phi(x_i;\\mu,\\sigma)^\\gamma, (\\gamma &gt; 0)\\) を重みとして核関数を表現すると\n\\[\n\\varphi(x-\\mu)\n    = \\phi(x_i;\\mu,\\theta)^\\gamma(x-\\mu)\n\\]\n\\(x\\) よりも \\(\\exp(-x^2)\\) のほうが早く 0 に近づくので，\\(\\lim_{x\\to\\infty} \\phi(x_i;\\mu,\\sigma)^\\gamma(x-\\mu) = 0\\) という性質を持っており，Outlierに強い推定が期待できます．標準偏差 \\(\\sigma\\) については，正規化された中央絶対偏差 \\(\\operatorname{MADN}\\) などがよく使われます．\n実際に計算する場合は, \\(\\sigma_{MADN}\\) をMADNとすると\n\\[\n\\varphi(x-\\mu)\n    = \\frac{\\phi(x_i;\\mu,\\sigma_{MADN})^\\gamma}{\\sum_{i=1}^n \\phi(x_i;\\mu,\\sigma_{MADN})^\\gamma}(x-\\mu)\n\\]\nという形になります．\n\n\nCode\nfrom scipy.stats import norm\n\ndef norml_power_weight(x, mu, sigma, gamma):\n    weight = norm.pdf(x, mu, sigma) ** gamma\n    return weight * (x - mu) \n\n## 可視化\nc = 2\nsupport = np.linspace(-3 * c, 3 * c, 1000)\nnormal_power_kernel = norml_power_weight(support, 0, 1, 1)\n\nfig = plt.figure(figsize=(6, 5))\nax = fig.add_subplot(111)\nax.plot(support, normal_power_kernel)\nax.set_title('powered normal pdf with c=2, mu=0, gamma = 2')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n尤度型\n\n\nCode\nfrom statsmodels.miscmodels.tmodel import TLinearModel\n\nstats_tmodel = TLinearModel(X_1, np.ones(len(X_1))).fit()\nprint(stats_tmodel.params[0])\n\n\nrunning Tmodel initialize\nOptimization terminated successfully.\n         Current function value: 1.481456\n         Iterations: 127\n         Function evaluations: 236\n0.006243405777405073",
    "crumbs": [
      "統計的推定",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>M-Estimator</span>"
    ]
  },
  {
    "objectID": "posts/bayesian_estimation/bayesian_linear_regression.html",
    "href": "posts/bayesian_estimation/bayesian_linear_regression.html",
    "title": "23  ベイジアン線形回帰",
    "section": "",
    "text": "Recap of OLS\n\\((Y_i, \\pmb{x}_i)\\) をrandom vectorとして，\\(Y_i \\in \\mathbb R, \\pmb{x}_i \\in \\mathbb R^k\\) とします． 線形回帰では，\\((Y_i, \\pmb{x}_i)\\) について次のようにパラメーターに関して線形な関係があるとして回帰問題を考えます．\n\\[\n\\begin{align*}\nY_i &= \\pmb{x}_i^T\\pmb{\\beta} + \\epsilon_i \\quad (\\epsilon_i \\text{: errors})\n\\end{align*}\n\\]\nこれを行列表現にした場合，\\(\\pmb{Y} = (Y_1, \\cdots, Y_n)^T\\), \\(\\pmb{X} = (\\pmb{x}_1, \\cdots, \\pmb{x}_n)^T\\) として\n\\[\n\\pmb{Y} = \\pmb{X}\\pmb{\\beta} + \\pmb{\\epsilon}\n\\]\n議論の単純化のため，以下では\n\\[\n\\epsilon_1, \\cdots, \\epsilon_m \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2)\n\\tag{23.1}\\]\nつまり， Homoscedasticityの仮定の下で話を進めます．\n▶  \\(\\pmb\\beta\\) の推定\n仮定 (2), (3) より \\(\\mathbb E[Y_i\\vert \\pmb{x}_i] = \\pmb{x}_i^T\\pmb{\\beta}\\) であるので，\n\\[\n\\pmb{\\beta} = \\arg\\min_{b} \\mathbb E[(Y_i - \\pmb{x}_i^T\\pmb{b})^2]\n\\tag{23.2}\\]\nthe joint distribution \\((Y_i, \\pmb{x}_i)\\) は事前には知られていないので、sample analogを用いて以下のように推定します\n\\[\n\\begin{align*}\n\\widehat{\\pmb{\\beta}}\n    &= \\arg\\min_{b} \\frac{1}{N}\\sum_{i=1}^N(Y_i - \\pmb{x}_i^T\\pmb{b})^2\\\\\n    &= \\arg\\min_{b} (\\pmb{Y} - \\pmb{X}\\pmb{\\beta})^T(\\pmb{Y} - \\pmb{X}\\pmb{\\beta})\n\\end{align*}\n\\]\n上記について，\\(b\\) のFOCをとると\n\\[\n\\pmb{X}^T(\\pmb{Y} - \\pmb{X}\\widehat{\\pmb{\\beta}}) = 0\n\\]\n仮定 (5) より \\(\\operatorname{rank}(\\pmb{X}) = k\\), つまりfull rankであるので \\((\\pmb{X}^T\\pmb{X})^{-1}\\) がとれます．従って，\n\\[\n\\widehat{\\pmb{\\beta}} = (\\pmb{X}^T\\pmb{X})^{-1}(\\pmb{X}^T\\pmb{Y})\n\\tag{23.3}\\]\n▶  \\(\\widehat{\\pmb{\\beta}}\\) の漸近分散\npopulation \\(\\pmb{\\beta}\\) を用いてerror termを以下のように表します\n\\[\n\\begin{align*}\n\\pmb{Y}_i\n    &= \\pmb{x}_i^T\\pmb{\\beta} + (\\pmb{Y_i} - \\pmb{x}_i^T\\pmb{\\beta})\\\\\n    &= \\pmb{x}_i^T\\pmb{\\beta} + \\epsilon_i\n\\end{align*}\n\\]\n両辺に \\([\\pmb{X}^T\\pmb{X}]^{-1}\\pmb{X}^T\\) をかけると Equation 23.3 より\n\\[\n\\widehat{\\pmb{\\beta}} = \\pmb{\\beta} + \\left[\\sum\\pmb{x}_i\\pmb{x}_i^T\\right]^{-1}\\sum \\pmb{x}_i\\epsilon_i\n\\]\nつぎに \\(\\sqrt{n}(\\widehat{{\\pmb{\\beta}}} - \\pmb{\\beta})\\) で漸近分散を考えると，\n\\[\n\\sqrt{n}(\\widehat{{\\pmb{\\beta}}} - \\pmb{\\beta}) = n\\left[\\sum\\pmb{x}_i\\pmb{x}_i^T\\right]^{-1}\\frac{1}{\\sqrt{n}}\\sum \\pmb{x}_i\\epsilon_i\n\\]\n\\(\\mathbb E[\\pmb{x}_ie_i] = 0\\) であるので，\\(\\frac{1}{\\sqrt{n}}\\sum \\pmb{x}_i\\epsilon_i\\) は location は 0であることがわかります．従って，\n\\[\n\\frac{1}{\\sqrt{n}}\\sum \\pmb{x}_i\\epsilon_i = \\sqrt{n}\\left\\{\\frac{1}{n}\\sum \\pmb{x}_i\\epsilon_i - 0\\right\\}\\overset{\\mathrm{d}}{\\to} N(0, \\mathbb E[\\pmb{x}_i\\pmb{x}^T\\epsilon_i^2])\n\\]\nまた，\\(\\frac{1}{n}\\sum\\pmb{x}_i\\pmb{x}_i^T\\) が \\(\\mathbb E[\\pmb{x}_i\\pmb{x}_i^T]\\) に確率収束するので，スラツキーの定理とCLTにより\n\\[\n\\begin{align*}\n\\sqrt{n}(\\widehat{{\\pmb{\\beta}}} - \\pmb{\\beta}) \\overset{\\mathrm{d}}{\\to}  N(0, \\mathbb E[\\pmb{x}_i\\pmb{x}]^{-1}\\mathbb E[\\pmb{x}_i\\pmb{x}^T\\epsilon_i^2]\\mathbb E[\\pmb{x}_i\\pmb{x}]^{-1})\n\\end{align*}\n\\]\nEquation 23.1 より\n\\[\n\\mathbb E[\\pmb{x}_i\\pmb{x}^T\\epsilon_i^2] = \\sigma^2\\mathbb E[\\pmb{x}_i\\pmb{x}^T]\n\\]\nが成立するので\n\\[\n\\sqrt{n}(\\widehat{{\\pmb{\\beta}}} - \\pmb{\\beta}) \\overset{\\mathrm{d}}{\\to}  N(0, \\sigma^2\\mathbb E[\\pmb{x}_i\\pmb{x}]^{-1})\n\\]\n\\(\\pmb{X}\\) で条件付けた分散は\n\\[\n\\begin{align*}\n\\operatorname{Var}(\\widehat{\\pmb{\\beta}}\\vert \\pmb{X})\n    & = \\mathbb E[(\\pmb{X}^T\\pmb{X})^{-1}\\pmb{X}^T\\pmb\\epsilon\\pmb\\epsilon^T\\pmb{X}(\\pmb{X}^T\\pmb{X})^{-1}\\vert \\pmb{X}]\\\\\n&= (\\pmb{X}^T\\pmb{X})^{-1}\\pmb{X}^T\\mathbb E[\\pmb\\epsilon\\pmb\\epsilon^T\\vert \\pmb{X}](\\pmb{X}^T\\pmb{X})^{-1}\\pmb{X}^T\\\\\n&= \\sigma^2(\\pmb{X}^T\\pmb{X})^{-1}\n\\end{align*}\n\\]\n▶  \\(\\pmb{Y}\\) の条件付き同時分布\nEquation 23.1 より\\(\\pmb{Y}\\) の条件付き同時確率密度は\n\\[\n\\begin{align*}\np&(Y_1, \\cdots, Y_n\\vert \\pmb{x_1}, \\cdots, \\pmb{x_n}, \\pmb{\\beta}, \\sigma^2)\\\\\n&= \\prod p(Y_i\\vert \\pmb{x_i}, \\pmb{\\beta}, \\sigma^2) \\quad \\because\\text{i.i.d}\\\\\n&= (2\\pi\\sigma^2)^{-n/2}\\exp\\{- \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (Y_i - \\pmb{x}_i^T\\pmb{\\beta})^2\\}\n\\end{align*}\n\\tag{23.4}\\]\nつまり，\n\\[\n\\{\\pmb{Y} \\vert \\pmb{X}, \\pmb{\\beta}, \\sigma\\} \\sim \\operatorname{N}(\\pmb{X}\\pmb{\\beta}, \\sigma^2\\pmb{I})\n\\]\nEquation 23.4 の尤度最大化は\n\\[\n\\min\\sum_{i=1}^n (Y_i - \\pmb{x}_i^T\\pmb{\\beta})^2\n\\]\nを解くことで得られますが，これは Equation 23.2 の問題と対応していることがわかります．",
    "crumbs": [
      "ベイズ推定",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>ベイジアン線形回帰</span>"
    ]
  },
  {
    "objectID": "posts/bayesian_estimation/bayesian_linear_regression.html#recap-of-ols",
    "href": "posts/bayesian_estimation/bayesian_linear_regression.html#recap-of-ols",
    "title": "23  ベイジアン線形回帰",
    "section": "",
    "text": "Assumption: OLSの仮定 \n\nmutually independent and identical distribution\n\\[\n(Y_i, \\pmb{x_i^T}), i = 1, \\cdots, n \\text{ は i.i.d}\n\\]\nLinear model\n\\[\n\\pmb{Y} = \\pmb{X}\\pmb{\\beta} + \\pmb{\\epsilon}\n\\]\nerror term is conditional-mean zero\n\\[\n\\mathbb E[\\epsilon_i\\vert \\pmb{x}_i] = 0\n\\]\nfinite moments\n\\[\n\\begin{align*}\n    \\mathbb E[Y_i^2] &&lt; \\infty\\\\\n    \\mathbb E[\\|\\pmb{x}_i^2\\|] &&lt; \\infty\n\\end{align*}\n\\]\nFull rank condition\n\\(\\operatorname{rank}(\\pmb{X}) = k\\)，つまり完全な多重共線性がない",
    "crumbs": [
      "ベイズ推定",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>ベイジアン線形回帰</span>"
    ]
  },
  {
    "objectID": "posts/bayesian_estimation/bayesian_linear_regression.html#bayesian-linear-regression",
    "href": "posts/bayesian_estimation/bayesian_linear_regression.html#bayesian-linear-regression",
    "title": "23  ベイジアン線形回帰",
    "section": "Bayesian Linear Regression",
    "text": "Bayesian Linear Regression\n ▶  \\(\\pmb\\beta\\) の事後分布の導出\n\\(\\pmb{\\beta}\\) についての事前分布を\n\\[\n\\pmb{\\beta} \\sim \\operatorname{N}(\\pmb{\\beta}_0, \\pmb{\\Sigma}_0)\n\\]\n事後分布は\n\\[\n\\begin{align*}\np&(\\pmb\\beta\\vert \\pmb Y, \\pmb X, \\sigma^2)\\\\\n&\\propto p(\\pmb Y\\vert \\pmb X, \\pmb \\beta, \\sigma^2)\\times p(\\pmb\\beta)\\\\\n&\\propto \\exp\\{-\\frac{1}{2}(-2\\pmb\\beta^T\\pmb X^T\\pmb y/\\sigma^2 + \\pmb\\beta^T\\pmb X^T\\pmb X\\pmb\\beta/\\sigma^2) - \\frac{1}{2}(-2 \\pmb\\beta^T\\pmb\\Sigma_0^{-1}\\pmb\\beta_0 + \\pmb\\beta^T\\pmb\\Sigma_0^{-1}\\pmb\\beta)\\} \\\\\n&= \\exp\\{\\pmb\\beta^T(\\pmb\\Sigma_0^{-1}\\pmb\\beta_0 + \\pmb X^T\\pmb Y/\\sigma^2) - \\frac{1}{2}\\pmb\\beta^T(\\pmb\\Sigma_0^{-1} + \\pmb X^T\\pmb X/\\sigma^2)\\pmb\\beta\\}\n\\end{align*}\n\\]\n多変量正規分布を仮定しているので，以下のようにまとめることができます\n\\[\n\\begin{align*}\n\\operatorname{Var}(\\pmb\\beta\\vert\\pmb Y, \\pmb X, \\sigma^2)\n    &= (\\pmb\\Sigma_0^{-1} + \\pmb X^T\\pmb X/\\sigma^2)^{-1}\\\\\n\\mathbb E[\\pmb\\beta\\vert\\pmb Y, \\pmb X, \\sigma^2]\n    &= (\\pmb\\Sigma_0^{-1} + \\pmb X^T\\pmb X/\\sigma^2)^{-1}(\\pmb\\Sigma_0^{-1}\\pmb\\beta_0 + \\pmb X^T\\pmb Y/\\sigma^2)\n\\end{align*}\n\\tag{23.5}\\]\n\n\n\n\n\n\n🍵 Green Tea Break\n\n\n\nEquation 23.5 について\n\\[\n\\begin{gather}\n\\pmb\\beta_0 = \\pmb 0\\\\\n\\pmb\\Sigma_0 = \\frac{\\sigma^2}{\\lambda}\\pmb I\n\\end{gather}\n\\]\nと事前分布のパラメータを設定すると，\n\\[\n\\begin{align*}\n\\mathbb E[\\pmb\\beta\\vert\\pmb Y, \\pmb X, \\sigma^2] = (\\lambda\\pmb I + \\pmb X^T\\pmb X)^{-1}\\pmb X^T\\pmb Y\n\\end{align*}\n\\]\nとなり，次にようにペナルティ \\(\\lambda\\) のRidge repression推定量と一致することがわかります．\nRidge推定量は\n\\[\n\\pmb\\beta_{ridge} =\\arg\\min (\\pmb Y- \\pmb X\\pmb \\beta)^T(\\pmb Y-\\pmb X\\pmb \\beta) + \\lambda \\|\\pmb \\beta\\|^2\n\\]\nの解と一致します．これについて，FOCをとると\n\\[\n\\pmb\\beta_{ridge} = (\\pmb X^T\\pmb X + \\lambda \\pmb I)^{-1}\\pmb X^T\\pmb Y\n\\]\n参考までですが，Ridge推定量の条件付き漸近分散は\n\\[\n\\begin{align*}\n\\operatorname{Var}(\\pmb\\beta_{ridge}\\vert \\pmb X) &= \\sigma^2(\\pmb X^T\\pmb X + \\lambda I_p)^{-1}\\pmb X^T\\pmb X(\\pmb X^T\\pmb X + \\lambda \\pmb I)^{-1}\\\\\n&\\leq \\sigma^2(\\pmb X^T\\pmb X)^{-1} = V(\\pmb\\beta_{ols}\\vert \\pmb X)\n\\end{align*}\n\\]\n\n\n ▶  \\(\\sigma^2\\) の事前分布\n多くの正規モデルでは \\(\\sigma^2\\) の準共役事前分布は逆ガンマ分布です． \\(\\gamma = 1/\\sigma^2\\) を観測の精度とみなし，\n\\[\n\\gamma \\sim \\operatorname{Ga}(v_0/2, v_0\\sigma^2_0/2)\n\\]\nと事前分布を設定すると\n\\[\n\\begin{align*}\np&(\\gamma\\vert\\pmb Y, \\pmb X, \\pmb\\beta)\\\\[5pt]\n&\\propto p(\\gamma)\\times p(\\pmb Y \\vert \\gamma, \\pmb X, \\pmb\\beta)\\\\\n&\\propto \\left[\\gamma^{v_0/2 -1} \\exp(-\\gamma\\times v_0\\sigma^2_0/2)\\right] \\times \\left[\\gamma^{n/2}\\exp(-\\gamma \\times \\operatorname{SSR}(\\pmb\\beta)/2)\\right]\\\\\n&= \\gamma^{(v_0+n)/2 -1} \\exp(-\\gamma [v_0\\sigma^2_0 + \\operatorname{SSR}(\\pmb\\beta)]/2)\\\\[5pt]\n&\\text{where }\\quad \\operatorname{SSR}(\\pmb\\beta) = (\\pmb Y - \\pmb X\\pmb\\beta)^T(\\pmb Y - \\pmb X\\pmb\\beta)\n\\end{align*}\n\\]\nこれはガンマ分布とみなせるので，\\(\\gamma = 1/\\sigma^2\\) より，事後分布は\n\\[\n\\{\\sigma^2\\vert\\pmb Y, \\pmb X, \\pmb\\beta\\} \\sim \\operatorname{inverse-gamma}((v_0+n)/2, [v_0\\sigma^2_0 + \\operatorname{SSR}(\\pmb\\beta)]/2)\n\\]\n従って，ベイジアンアップデートは以下のような手順で行います.\n\n\n\n\n\n\n推定手順\n\n\n\n ▶  Step 1: \\(\\pmb\\beta\\) の更新\n\n\\(\\pmb V = \\operatorname{Var}(\\pmb\\beta\\vert \\pmb Y, \\pmb X, \\sigma^{2}_{(s)}), \\pmb m = \\mathbb E[\\pmb\\beta\\vert \\pmb Y, \\pmb X, \\sigma^{2}_{(s)}]\\) の計算\n\\(\\pmb\\beta_{(s+1)}\\) を次のように生成\n\n\\[\n\\pmb\\beta_{(s+1)}\\sim \\operatorname{N}(\\pmb m, \\pmb V)\n\\]\n ▶  Step 2: \\(\\sigma^2\\) の更新\n\n\\(\\operatorname{SSR}(\\pmb\\beta_{(s+1)})\\) を計算\n\\(\\sigma^{2}_{(s+1)}\\) を次のように生成\n\n\\[\n\\sigma^{2}_{(s+1)}\\sim \\operatorname{inverse-gamma}((v_0+n)/2, [v_0\\sigma^2_0 + \\operatorname{SSR}(\\pmb\\beta_{(s+1)})]/2)\n\\]",
    "crumbs": [
      "ベイズ推定",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>ベイジアン線形回帰</span>"
    ]
  },
  {
    "objectID": "posts/ExplanatoryDataAnalysis/gini_coefficient.html",
    "href": "posts/ExplanatoryDataAnalysis/gini_coefficient.html",
    "title": "24  ジニ係数",
    "section": "",
    "text": "ジニ係数とローレンツ曲線\nPythonで形状パラメター 2.5 のパレート分布の乱数と \\(\\operatorname{Unif}(0, 1)\\) を20個ずつ発生させ作画したのが以下\nCode\nimport numpy as np\nfrom scipy.stats import pareto\nimport polars as pl\nimport plotly.express as px\n\nnp.random.seed(42)\n\nSAMPLESIZE = 20\nshape_parameter = 1\nx = pareto.rvs(shape_parameter, size=SAMPLESIZE)\nx2 = np.random.uniform(0, 1, SAMPLESIZE)\n\n# Compute ratio\nrelative_freq = np.arange(0, SAMPLESIZE + 1) / SAMPLESIZE\nrelative_cumulative_pareto = np.insert(np.cumsum(sorted(x)) / np.sum(x), 0, 0)\nrelative_cumulative_uniform = np.insert(np.cumsum(sorted(x2)) / np.sum(x2), 0, 0)\n\n# create dataframe\ndf = pl.DataFrame(\n    {\n        \"relative_freq\": relative_freq,\n        \"pareto\": relative_cumulative_pareto,\n        \"uniform\": relative_cumulative_uniform,\n    }\n)\n\n\n# compute gini\ndef compute_gini(relative_freq, relative_cumulative):\n    a = relative_freq[1:-1] - relative_cumulative[1:-1]\n    b = relative_freq[2:] - relative_freq[:-2]\n    return np.sum(a * b)\n\n\n# plot\npareto_gini = compute_gini(relative_freq, relative_cumulative_pareto)\nuniform_gini = compute_gini(relative_freq, relative_cumulative_uniform)\nfig = px.line(\n    df,\n    x=\"relative_freq\",\n    y=[\"pareto\", \"uniform\"],\n    color_discrete_sequence=[\"blue\", \"red\"],\n    markers=\"x\",\n    title=\"\"\"pareto gini coefficient: {:.2f}, uniform gini coeffient:  {:.2f}\"\"\".format(\n        pareto_gini, uniform_gini\n    ),\n)\n\nfig.update_yaxes(\n    scaleanchor=\"x\",\n    scaleratio=1,\n)\nBASE_SIZE = 600\nfig.update_layout(\n    autosize=True,\n    width=BASE_SIZE,\n    height=BASE_SIZE,\n    shapes=[\n        dict(\n            type=\"line\",\n            line_dash=\"dot\",\n            yref=\"y\",\n            y0=0,\n            y1=1,\n            xref=\"x\",\n            x0=0,\n            x1=1,\n            line=dict(color=\"gray\"),\n            label=dict(text=\"完全平等線\", textposition=\"middle center\"),\n        )\n    ],\n)\n\nfig.show()\n上記のfigureにおける45度線は完全平等線と呼ばれる線です．この赤線とローレンツ曲線で囲まれたエリアの面積の２倍がジニ係数に相当します．ジニ係数は赤線と青線のエリアを三角形と台形に分けてそれぞれを計算し，その合計の２倍で計算することができます．\n上記のようにsampleジニ係数は台形の面積の２倍で計算しますが，母集団ジニ係数と比較して小さめに計算されます． サンプルサイズが十分大きい場合は無視できる程度ですが，度数分布表に基づく計算の場合やsmall sampleの場合は 過小方向バイアスの修正が必要になる場合があります．\nまた，ジニ係数の導出式より，MAD(Mean absolute difference)と sample meanの相対比にジニ係数が比例することがわかります．\n\\[\n\\begin{align*}\n\\operatorname{Gini} &= \\frac{1}{2n^2\\overline{x}}\\sum_{i=1}^n\\sum_{j=1}^n \\vert x_i - x_j\\vert\\\\\n                    &= \\frac{1}{2}\\frac{\\sum_{i=1}^n\\sum_{j=1}^n \\vert x_i - x_j\\vert}{n^2}\\frac{1}{\\overline x}\\\\\n                    &\\propto \\frac{\\operatorname{MAD}}{\\operatorname{sample mean}}\n\\end{align*}\n\\]\n▶  ジニ係数の特徴",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>ジニ係数</span>"
    ]
  },
  {
    "objectID": "posts/ExplanatoryDataAnalysis/gini_coefficient.html#ジニ係数とローレンツ曲線",
    "href": "posts/ExplanatoryDataAnalysis/gini_coefficient.html#ジニ係数とローレンツ曲線",
    "title": "24  ジニ係数",
    "section": "",
    "text": "Def: ローレンツ曲線 \nデータ \\(X = \\{x_1, \\cdots, x_n\\}\\) について，以下のような順序統計量をとる\n\\[\nx_{[1]} \\leq \\cdots \\leq x_{[i]} \\leq \\cdots \\leq x_{[n]}\n\\]\nこのとき，相対度数 \\(r_i\\) と累積比率 \\(I_i\\) をそれぞれ以下のように定義する：\n\\[\n\\begin{align*}\nr_i & = \\frac{i}{n}\\\\\nI_i &= \\frac{\\sum_{j=1}^i x_{[j]}}{\\sum_{j=1}^n x_{[j]}}\n\\end{align*}\n\\]\n点 \\((0,0), (r_1, I_1), \\cdots , (r_n, I_n)\\) を区分的に直線で結んで得られる曲線がローレンツ曲線である．\n\n\n\n\n\nDef: ジニ係数 \n\nジニ係数はデータの偏りを示す指標\n\n ▶  相対度数に基づくジニ係数\n\\(a_i\\) を度数表に基づく累積相対頻度， \\(b_i\\) を累積相対階級値としたとき\n\\[\n\\begin{align*}\n\\operatorname{Gini} = \\sum_{i=1}^{k-1} (a_i - b_i)(a_{i+1} - a_{i-1}) \\  \\ \\text{where } a_0 = 0, b_0 = 0\n\\end{align*}\n\\]\n ▶  データに基づくジニ係数\n\\[\n\\begin{align*}\n\\operatorname{Gini}\n    &= \\frac{2}{n}\\sum_{n-1}^{i=1}\\left(\\frac{i}{n} - \\frac{\\sum_{j=1}^i x_{[j]}}{n\\overline{x}}\\right) \\\\\n    &= \\frac{1}{2n^2\\overline{x}}\\sum_{i=1}^n\\sum_{j=1}^n \\vert x_i - x_j\\vert\n\\end{align*}\n\\]\n\n\n\n\n\n\nローレンツ曲線，ジニ係数ともに分布の尺度パラメーターに依存しない\n1点に集中する分布の場合，ローレンツ曲線は完全平等線と一致し，ジニ係数は0となる\nエントロピーと異なり，一様分布の場合にジニ係数が最大をとるとかではない\n\n\n📘 REMARKS \n\nデータの偏りや集中性を見る指標としてジニ係数は使用されますが，他にもエントロピーという指標がある\n\nカテゴリー別に分類されたデータにおいて，各カテゴリーの総体頻度を \\(\\hat p_i = f_i/n\\) としたとき\n\\[\n\\begin{align*}\nH(\\mathbf p) = -\\sum \\hat p_i \\log(\\hat p_i)\n\\end{align*}\n\\]\nHが大きいほどデータは一様になり，集中性があるほど指標は小さくなる．\n\n\n無限母集団におけるローレンツ曲線\n定義域 \\((0, \\infty)\\), 期待値 \\(\\mu\\) を持つ連続確率変数 \\(X\\) について，累積分布関数を \\(F(x)\\), 確率密度関数 \\(f(x)\\) とおきます．このとき，\\(z = F(x)\\) としたときのローレンツ曲線は\n\\[\n\\begin{align*}\n&F^{-1}(z) = x \\  \\ \\text{ (inverse function of cdf)}\\\\\n&L(z) =\\frac{\\int_0^{F^{-1}(z)} t f(t) \\mathrm{d}t}{\\mathbb E[X]}\n\\end{align*}\n\\]\nとして, \\((z, L(z))\\) で作られる曲線となります．ジニ係数は\n\\[\n\\begin{align*}\n\\operatorname{gini} &= 2\\int_0^1 (u - L(u))\\mathrm{d}u\\\\\n                    &= 1 - 2\\int_0^1 L(u)\\mathrm{d}u\n\\end{align*}\n\\]",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>ジニ係数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/permutation_combination.html",
    "href": "posts/mathematical_appendix/permutation_combination.html",
    "title": "Appendix A — 二項級数",
    "section": "",
    "text": "Theorem A.1 : 組み合わせの公式 \n\\(r\\in \\mathbb N_+, p \\in (0, 1)\\) に対し，次の等式が成立する\n\\[\n\\sum_{x=0}^\\infty {}_{r + x - 1}C_{x}(1 -p)^x = \\frac{1}{p^r}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>二項級数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/complex_number.html",
    "href": "posts/mathematical_appendix/complex_number.html",
    "title": "Appendix B — 複素数",
    "section": "",
    "text": "複素数の性質\n複素数 \\(z = a + bi\\) の実数の組 \\((a, b)\\) について，\n\\[\n\\begin{align*}\na &= \\text{Re}\\, z\\\\\nb &= \\text{Im}\\, z\n\\end{align*}\n\\]\nと表したりします．それぞれ，real part, imaginary partの略と理解できます．\n▶  複素数の加減乗除\n\\(z_1 = a + bi, z_2 = c + di\\) としたとき，四則演算は以下のように計算されます\n\\[\n\\begin{align*}\nz_1 + z_2 &= (a + c) + (b + d)i\\\\\nz_1 - z_2 &= (a - c) + (b - d)i\\\\\nz_1z_2    &= ac + (ad + bc)i + bdi^2\\\\\n          &= (ac - bd) + (ad + bc)i \\quad \\because i^2 =-1\\\\\n\\frac{z_1}{z_2}\n          &= \\frac{a + bi}{c + di}\\\\\n          &= \\frac{a + bi}{c + di}\\frac{c - di}{c - di}\\\\\n          &= \\frac{(a + bi)(c - di)}{c^2+d^2}\\\\\n          &= \\frac{(ac + bd) + (bc - ad)i}{c^2+d^2}\n\\end{align*}\n\\]\n共役複素数は，複素数平面上で実軸に関して対称移動させたものと解釈することができます．また，定義より自明ですが 複素数と共役複素数について，和と積が実数になるという特徴もあります．\n定義より，\\((a, b) = (0, 0)\\) のときは，\\(\\vert z\\vert =0\\)．逆に \\(\\vert z\\vert =0\\) ならば，\n\\[\na^2 + b^2 =0\n\\]\nを満たす実数の組は \\((0, 0)\\) しか存在しないので，\\(z = 0\\) となります．",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>複素数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/complex_number.html#複素数の性質",
    "href": "posts/mathematical_appendix/complex_number.html#複素数の性質",
    "title": "Appendix B — 複素数",
    "section": "",
    "text": "Def: 複素数 \n２つの実数 \\(a, b\\) を用いて\n\\[\nz = a + bi\n\\]\nと表される数 \\(z\\) を複素数という．\\(a\\) を上の複素数の実部，\\(b\\) を虚部と呼ぶ． 複素数全体の集合は一般に \\(\\mathbb C\\) と表される．\n\n\n\n\n\n\n\n\nDef: 共役複素数 \n複素数 \\(z = a+bi\\) に対し，\n\\[\n\\bar z = a - bi\n\\]\nを \\(z\\) の共役複素数（きょうやくふくそすう）と呼ぶ．\n\n\n\n\nTheorem B.1 \n２つの複素数 \\(z_1, z_2\\) について，次が成り立つ\n\\[\n\\begin{align*}\n\\overline{z_1 + z_2} &= \\bar z_1 + \\bar z_2\\\\\n\\overline{z_1z_2} &= \\bar z_1\\bar z_2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(z_1 = a_1 + b_1i, z_2 = a_2 + b_2i\\) としたとき，\n\\[\n\\begin{align*}\n\\overline{z_1 + z_2}\n    &= \\overline{(a_1 + a_2) + (b_1 +b_2)i}\\\\\n    &= (a_1 + a_2) - (b_1 +b_2)i\\\\\n    &= a_1 - b_1i + a_2 - b_2i\\\\\n    &= \\bar z_1 + \\bar z_2\n\\end{align*}\n\\]\n積については\n\\[\n\\begin{align*}\n\\overline{z_1z_2}\n    &= \\overline{(a_1a_2 - b_1b_2) + (a_1b_2 + a_2b_1)i}\\\\\n    &= (a_1a_2 - b_1b_2) - (a_1b_2 + a_2b_1)i\\\\\n    &= a_1a_2 - (a_1b_2 + a_2b_1)i + b_1b_2i^2\\\\\n    &= (a_1 - b_1i)(a_2 - b_2i)\\\\\n    &= \\bar z_1\\bar z_2\n\\end{align*}\n\\]\n\n\n\n\nDef: 複素数の絶対値 \n複素数 \\(z = a + bi\\) の絶対値 \\(\\vert z\\vert\\) は次のように計算される：\n\\[\n\\vert z \\vert = \\sqrt{a^2 + b^2}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>複素数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/complex_number.html#複素平面",
    "href": "posts/mathematical_appendix/complex_number.html#複素平面",
    "title": "Appendix B — 複素数",
    "section": "複素平面",
    "text": "複素平面\n複素数 \\(z = x + yi\\) が与えられたとき，２つの実数の組 \\((x, y)\\) が与えられた状況とも解釈できます． また，２つの実数の組 \\((x, y)\\) が与えられた状況とは，\\(xy\\)-平面上の点を与えられる状況とも考えることができます． つまり，複素数が与えられたとき，\n\n実部を \\(x\\) 軸\n虚部を \\(y\\) 軸\n\nとする \\(xy\\)-平面を考えることができます．\\(z = 1 + \\sqrt{3}\\) を描いてみると，\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npi = np.pi\n\n\n# Set parameters\nr = 2\ntheta = pi/3\nx = r * np.cos(theta)\nx_range = np.linspace(0, x, 1000)\ntheta_range = np.linspace(0, theta, 1000)\n\n# Plot\nfig = plt.figure(figsize=(6, 6))\nax = plt.subplot(111, projection='polar')\n\nax.plot((0, theta), (0, r), marker='o', color='b')          # Plot r\nax.plot(np.zeros(x_range.shape), x_range, color='b')       # Plot x\nax.plot(theta_range, x / np.cos(theta_range), color='b')        # Plot y\nax.plot(theta_range, np.full(theta_range.shape, 0.1), color='r')  # Plot theta\n\nax.margins(0) # Let the plot starts at origin\n\nax.set_title(\"Trigonometry of complex numbers\", va='bottom',\n    fontsize='x-large')\n\nax.set_rmax(2)\nax.set_rticks((0.5, 1, 1.5, 2))  # Less radial ticks\nax.set_rlabel_position(-88.5)    # Get radial labels away from plotted line\n\nax.text(theta, r+0.01 , r'$z = x + iy = 1 + \\sqrt{3}\\, i$')   # Label z\nax.text(theta+0.2, 1 , r'$\\vert z\\vert = 2$')                             # Label r\nax.text(0-0.2, 0.5, '$x = 1$')                            # Label x\nax.text(0.5, 1.2, r'$y = \\sqrt{3}$')                      # Label y\nax.text(0.25, 0.15, r'$\\text{arg } z = 60^o$')                   # Label theta\n\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n複素数が \\(xy\\)-平面上の点で表すことができるということは，原点からの長さと角度 \\((r, \\theta)\\) によって点を表現できることになります． このとき，以下のように表します:\n\\[\n\\begin{align*}\nr & = \\vert z\\vert\\\\\n\\theta &= \\operatorname{arg}z = \\arctan \\left(\\frac{y}{x}\\right)\n\\end{align*}\n\\]\nこのことから次の定義が導けます．\n\nDef: 複素数の極形式 \n複素平面上の点 \\(z = x + yi\\) は極座標を用いて次のように表せる:\n\\[\n\\begin{align*}\n&z = r(\\cos \\theta + i\\sin\\theta)\\\\\n&\\text{where } r = \\vert z\\vert, \\theta = \\operatorname{arg} z\n\\end{align*}\n\\]\n\n ▶  複素数の積と複素平面\n２つの複素数 \\(z_1 = r_1(\\cos\\theta_1 + i\\sin\\theta_1), z_2 = r_2(\\cos\\theta_2 + i\\sin\\theta_2)\\) に対して，その積は\n\\[\n\\begin{align*}\nz_1z_2\n    =& r_1r_2(\\cos\\theta_1 + i\\sin\\theta_1)(\\cos\\theta_2 + i\\sin\\theta_2)\\\\\n    =& r_1r_2\\{(\\cos\\theta_1\\cos\\theta_2 - \\sin\\theta_1\\sin\\theta_2) \\\\\n     & + i(\\sin\\theta_1\\cos\\theta_2 + \\cos\\theta_1\\sin\\theta_2)\\}\\\\\n    =& r_1r_2\\{\\cos(\\theta_1 + \\theta_2) + i\\sin(\\theta_1 + \\theta_2)\\} \\because{\\text{加法定理}}\n\\end{align*}\n\\]\n\\(xy\\)-平面で確認すると以下のようになります\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npi = np.pi\n\n\n# Set parameters\nr_1 = 1.5\nr_2 = 2\ntheta_1 = pi / 4\ntheta_2 = pi / 3\nx_1 = r_1 * np.cos(theta_1)\nx_2 = r_2 * np.cos(theta_2)\ntheta_range = np.linspace(0, theta, 1000)\n\n# Plot\nfig = plt.figure(figsize=(6, 6))\nax = plt.subplot(111, projection=\"polar\")\n\nax.plot((0, theta_1), (0, r_1), marker=\"o\", color=\"b\")\nax.plot((0, theta_2), (0, r_2), marker=\"o\", color=\"b\")\nax.plot((0, theta_1 + theta_2), (0, r_1 * r_2), marker=\"o\", color=\"b\")\n\nax.margins(0)  # Let the plot starts at origin\n\nax.set_title(\"Product of complex numbers\", va=\"bottom\", fontsize=\"x-large\")\n\nax.set_rmax(3)\nax.set_rticks((0.5, 1, 1.5, 2, 2.5, 3))  # Less radial ticks\nax.set_rlabel_position(-88.5)  # Get radial labels away from plotted line\n\nax.text(theta_1 + 0.1, r_1 + 0.01, r\"$z_1$\")\nax.text(theta_2 + 0.1, r_2 + 0.01, r\"$z_2$\")\nax.text(theta_1 + theta_2 + 0.1, r_1 * r_2 + 0.2, r\"$z_1z_2$\")\nax.text(theta_1 + theta_2 + 0.3, (r_1 * r_2) / 2, r\"$r_1r_2$\")\n\n\ntheta1_range = np.linspace(0, theta_1, 100)\ntheta2_range = np.linspace(0, theta_2, 100)\ntheta3_range = np.linspace(0, theta_1 + theta_2, 100)\n\nax.plot(theta1_range, np.full(theta1_range.shape, 0.2), color=\"r\")  # Plot theta\nax.plot(theta2_range, np.full(theta2_range.shape, 0.4), color=\"r\")  # Plot theta\nax.plot(theta3_range, np.full(theta3_range.shape, 0.8), color=\"r\")  # Plot theta\nax.text(0.25, 0.2, r\"$\\theta_1$\")  # Label theta\nax.text(0.5, 0.5, r\"$\\theta_2$\")  # Label theta\nax.text(1.8, 0.9, r\"$\\theta_1 + \\theta_2$\")  # Label theta\n\n\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n📘 REMARKS \n２つの複素数 \\(z_1, z_2\\) に対して，\n\\[\n\\begin{align*}\n\\vert z_1z_2\\vert &= \\vert z_1\\vert \\cdot \\vert z_2\\vert\\\\\n\\operatorname{arg}(z_1z_2) &= \\operatorname{arg} z_1 + \\operatorname{arg} z_2\n\\end{align*}\n\\]\n\n２つ以上の複素数の積についても，繰り返すことによって以下の関係性を得ます．\n\\[\n\\prod_i^nz_i = r_1r_2\\cdots r_n\\left\\{\\cos\\left(\\sum_i\\theta_i\\right) + i\\sin\\left(\\sum_i\\theta_i\\right)\\right\\}\n\\]\nこれはド・モアブルの公式と呼ばれているものです．\n\n\nTheorem B.2 : ド・モアブルの公式 \n\\(n\\in\\mathbb Z\\) としたとき\n\\[\n(\\cos\\theta + i\\sin\\theta)^n = \\cos n\\theta + i\\sin n\\theta\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(n \\geq 0\\) は自明なので，ここでは \\(n &lt; 0\\) の場合を示す．\n\\[\n(\\cos\\theta + i\\sin\\theta)^n = \\frac{1}{(\\cos\\theta + i\\sin\\theta)^{-n}}\n\\]\n\\(-n &gt; 0\\) より\n\\[\n\\begin{align*}\n\\frac{1}{(\\cos\\theta + i\\sin\\theta)^{-n}}\n    &= \\frac{1}{\\cos (-n\\theta) + i\\sin (-n\\theta)}\\\\\n    &= \\frac{1}{\\cos (-n\\theta) + i\\sin (-n\\theta)}\\frac{\\cos (-n\\theta) - i\\sin (-n\\theta)}{\\cos (-n\\theta) - i\\sin (-n\\theta)}\\\\\n    &= \\cos (-n\\theta) - i\\sin (-n\\theta)\\\\\n    &= \\cos (n\\theta) + i\\sin (n\\theta)\n\\end{align*}\n\\]\n従って，\\(n\\in \\mathbb Z\\) でド・モアブルの公式が成立することが分かる．",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>複素数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/complex_number.html#オイラーの公式",
    "href": "posts/mathematical_appendix/complex_number.html#オイラーの公式",
    "title": "Appendix B — 複素数",
    "section": "オイラーの公式",
    "text": "オイラーの公式\n\nDef: オイラーの公式 \n\\[\n\\begin{align*}\n\\exp(i\\theta) &= \\cos\\theta + i\\sin\\theta\\\\\n\\exp(-i\\theta) &= \\cos\\theta - i\\sin\\theta\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nオイラーの公式の導出\n\n\n\n\n\n\\(\\cos x, \\sin x, \\exp(ix)\\) をマクローリン展開します．\n\\[\n\\begin{align*}\n\\sin(x) =& x - \\frac{1}{3!}x^3 + \\frac{1}{5!}x^5 - \\cdots + (-1)^n\\frac{1}{(2n+1)!}x^{2n+1} + \\cdots\\\\\n\\cos(x) =& 1 - \\frac{1}{2!}x^2 + \\frac{1}{4!}x^4 - \\cdots + (-1)^n\\frac{1}{(2n)!}x^{2n} + \\cdots\\\\\n\\exp(ix)=& 1 + ix - \\frac{1}{2!}x^2 + i\\frac{1}{3!}x^3 + \\cdots + i^n\\frac{1}{n!}x^n + \\cdots\\\\\n        =& \\left(1 - \\frac{1}{2!}x^2 + \\frac{1}{4!}x^4 + \\cdots \\right)\\\\\n         &+ \\left( - \\frac{1}{3!}x^3 + \\frac{1}{5!}x^5 - \\cdots \\right)\n\\end{align*}\n\\]\n実部・虚部を比較すると，\n\\[\n\\exp(i\\theta) = \\cos\\theta + i\\sin\\theta\n\\]\nを得ます．また，\\(\\cos\\theta = \\cos(-\\theta), \\sin\\theta = -\\sin(-\\theta)\\) より，\n\\[\n\\exp(-i\\theta) = \\cos\\theta - i\\sin\\theta\n\\]\n\n\n\n ▶  オイラーの公式と加法定理\nオイラーの公式より加法定理を確認することができます．\n\\[\n\\begin{align*}\n\\cos(\\alpha+\\beta) + i\\sin(\\alpha+\\beta)\n    =& \\exp(i(\\alpha+\\beta))\\\\\n    =& \\exp(i\\alpha)\\exp(i\\beta)\\\\\n    =& (\\cos\\alpha\\cos\\beta - \\sin\\alpha\\sin\\beta)\\\\\n     &+ i(\\cos\\alpha\\sin\\beta + \\sin\\alpha\\cos\\beta)\n\\end{align*}\n\\]\n実部と虚部を比較すると\n\\[\n\\begin{align*}\n\\cos(\\alpha+\\beta) &= \\cos\\alpha\\cos\\beta - \\sin\\alpha\\sin\\beta\\\\\n\\sin(\\alpha+\\beta) &= \\sin\\alpha\\cos\\beta + \\cos\\alpha\\sin\\beta\n\\end{align*}\n\\]\n同様に\n\\[\n\\begin{align*}\n\\cos(\\alpha-\\beta) + i\\sin(\\alpha-\\beta)\n    =& \\exp(i(\\alpha-\\beta))\\\\\n    =& \\exp(i\\alpha)\\exp(-i\\beta)\\\\\n    =& (\\cos\\alpha\\cos\\beta + \\sin\\alpha\\sin\\beta)\\\\\n     &+ i(\\sin\\alpha\\cos\\beta - \\cos\\alpha\\sin\\beta )\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\cos(\\alpha-\\beta) &= \\cos\\alpha\\cos\\beta + \\sin\\alpha\\sin\\beta\\\\\n\\sin(\\alpha-\\beta) &= \\sin\\alpha\\cos\\beta - \\cos\\alpha\\sin\\beta\n\\end{align*}\n\\]\n ▶  三角関数の微分との関係\n\\(\\theta\\) について，\\(\\exp(i\\theta)\\) を微分すると\n\\[\n\\begin{align*}\n(\\exp(i\\theta))^\\prime\n    &= i\\exp(i\\theta)\\\\\n    &= i(\\cos\\theta + i\\sin\\theta)\\\\\n    &= -\\sin\\theta + i\\cos\\theta\n\\end{align*}\n\\]\n同様に\n\\[\n\\begin{align*}\n(\\cos\\theta + i\\sin\\theta)^\\prime\n    &= \\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}\\theta}\\cos\\theta + \\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}\\theta}i\\sin\\theta\n\\end{align*}\n\\]\n実部と虚部を比較することで，三角関数の微分の公式を得ることができます．\n ▶  三角関数の表現\nオイラーの公式より，三角関数を次のように表せます．\n\\[\n\\begin{align*}\n\\cos\\theta &= \\frac{\\exp(i\\theta)+\\exp(-i\\theta)}{2}\\\\\n\\sin\\theta &= \\frac{\\exp(i\\theta)-\\exp(-i\\theta)}{2i}\n\\end{align*}\n\\]\nこれを用いると以下の積分が簡単に計算できます．\n\\[\n\\begin{split}\n\\begin{aligned}\n\\int \\cos(\\theta) \\sin(\\theta) \\, \\mathrm{d}\\theta\n&=\n\\int\n\\frac{(e^{i\\theta} + e^{-i\\theta})}{2}\n\\frac{(e^{i\\theta} - e^{-i\\theta})}{2i}\n\\, \\mathrm{d}\\theta  \\\\\n&=\n\\frac{1}{4i}\n\\int\ne^{2i\\theta} - e^{-2i\\theta}\n\\, d\\theta  \\\\\n&=\n\\frac{1}{4i}\n\\bigg( \\frac{-i}{2} e^{2i\\theta} - \\frac{i}{2} e^{-2i\\theta} + C_1 \\bigg) \\\\\n&=\n-\\frac{1}{8}\n\\bigg[ \\bigg(e^{i\\theta}\\bigg)^2 + \\bigg(e^{-i\\theta}\\bigg)^2 - 2 \\bigg] + C_2 \\\\\n&=\n-\\frac{1}{8}  (e^{i\\theta} - e^{-i\\theta})^2  + C_2 \\\\\n&=\n\\frac{1}{2} \\bigg( \\frac{e^{i\\theta} - e^{-i\\theta}}{2i} \\bigg)^2 + C_2 \\\\\n&= \\frac{1}{2} \\sin^2(\\theta) + C_2\n\\end{aligned}\n\\end{split}\n\\]\n従って，\n\\[\n\\int^{2\\pi}_0 \\cos(\\theta) \\sin(\\theta) \\, \\mathrm{d}\\theta = 0\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>複素数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/beta_function.html",
    "href": "posts/mathematical_appendix/beta_function.html",
    "title": "Appendix C — ベータ関数",
    "section": "",
    "text": "ベータ関数の性質",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>ベータ関数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/beta_function.html#ベータ関数の性質",
    "href": "posts/mathematical_appendix/beta_function.html#ベータ関数の性質",
    "title": "Appendix C — ベータ関数",
    "section": "",
    "text": "Def: ベータ関数 \n実数 \\(a, b\\) について，ベータ関数 \\(\\operatorname{B}(a, b)\\) は以下のように定義される．\n\\[\n\\operatorname{B}(a, b) = \\int^1_0 x^{a-1}(1-x)^{b-1}\\mathrm{d}x\n\\]\nまた，ガンマ関数を用いて以下のように表せる\n\\[\n\\operatorname{B}(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\]\n\n\n\n\n\n\n\nProof: ベータ関数とガンマ関数の関係\n\n\n\n\n\nガンマ関数の定義を用いて，\n\\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b) &= \\int^\\infty_0 x^{a-1}\\exp(-x)\\mathrm{d}x \\int^\\infty_0 y^{b-1}\\exp(-y)\\mathrm{d}y \\\\\n                   &= \\int^\\infty_0 \\int^\\infty_0 x^{a-1}y^{b-1}\\exp(-x-y)\\mathrm{d}x \\mathrm{d}y \\\\\n\\end{align*}\n\\]\nここで, \\(x + y = u, x/(x + y) = v\\) という変数変換を考える．つまり，\n\\[\n\\begin{align*}\nx = uv, y = u(1-v)\n\\end{align*}\n\\]\nこのときのヤコビアンは\n\\[\n\\begin{align*}\n\\vert\\operatorname{det} J \\vert= u\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b) &=\\int^\\infty_0 \\int^\\infty_0 (uv)^{a-1}[u(1-v)]^{b-1}\\exp(-u) u\\mathrm{d}x \\mathrm{d}y \\\\\n                   &= \\int^\\infty_0 \\int^\\infty_0 u^{a+b-1}\\exp(-u) v^{a-1}(1-v)^{b-1}\\mathrm{d}x \\mathrm{d}y \\\\\n                   &= \\int^\\infty_0 \\int^1_0 u^{a+b-1}\\exp(-u) v^{a-1}(1-v)^{b-1}\\mathrm{d}v \\mathrm{d}u \\\\\n                   &= \\int^\\infty_0 u^{a+b-1}\\exp(-u) \\mathrm{d}u \\int^1_0 v^{a-1}(1-v)^{b-1}\\mathrm{d}v \\\\\n                   &= \\operatorname{\\Gamma}(a+b)\\operatorname{B}(a, b)\n\\end{align*}\n\\]\nつまり，\n\\[\n\\operatorname{B}(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\operatorname{\\Gamma}(a+b)}\n\\]\n\n\n\n\n\n\n\n\n\nProof: 座標変換\n\n\n\n\n\nガンマ関数は \\(z^2 = x\\) という変数変換を用いると以下のように変換できる\n\\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a) &= \\int^\\infty_0 x^{a-1}\\exp(-x)\\mathrm{d}x \\\\\n          &= \\int^\\infty_0 z^{2a-2}\\exp(-z^2)\\frac{\\mathrm{d}x}{\\mathrm{d}z}\\mathrm{d}z \\\\\n          &= 2\\int^\\infty_0 z^{2a-1}\\exp(-z^2)\\mathrm{d}z\n\\end{align*}\n\\]\nこれを用いて\n\\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b) &= 4\\int^\\infty_0 x^{2a-1}\\exp(-x^2)\\mathrm{d}x \\int^\\infty_0 y^{2b-1}\\exp(-y^2)\\mathrm{d}y \\\\\n                   &= 4\\int^\\infty_0 \\int^\\infty_0 x^{2a-1}y^{2b-1}\\exp(-(x^2+y^2))\\mathrm{d}x \\mathrm{d}y\n\\end{align*}\n\\]\nここで, \\(x = r\\cos\\theta, y=r\\sin\\theta\\) という変数変換を行う．このときのヤコビアンは\n\\[\n\\vert \\operatorname{det} J \\vert = r\n\\]\nよって， \\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b) &= 4\\int^\\infty_0 \\int^\\infty_0 x^{2a-1}y^{2b-1}\\exp(-(x^2+y^2))\\mathrm{d}x \\mathrm{d}y\\\\\n                   &= 4\\int^{\\pi/2}_0 \\int^\\infty_0 r^{2a+2b-2} \\cos^{2a-1}\\theta \\sin^{2b-1}\\theta \\exp(-r^2) r \\mathrm{d}r \\mathrm{d}\\theta\\\\\n                   &= \\left(2\\int^{\\pi/2}_0 \\cos^{2a-1}\\theta \\sin^{2b-1}\\theta \\mathrm{d}\\theta\\right) \\times \\left(\\int^\\infty_0r^{2(a+b)-1}\\exp(-r^2)\\mathrm{d}r \\right)\\\\\n                   &= \\operatorname{B}(a, b) \\times \\operatorname{\\Gamma}(a+b)\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{B}(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\operatorname{\\Gamma}(a+b)}\n\\]\n\n\n\n\nExample C.1 : ベータ関数のplot \n\n\nCode\nimport numpy as np\nfrom scipy.special import beta\nimport plotly.graph_objects as go\n\nX, Y = np.linspace(0, 10, 41)[1:], np.linspace(0, 10, 41)[1:]\nX, Y = np.meshgrid(X, Y)\nZ = beta(X, Y)\n\n# Create the 3D surface plot\nfig = go.Figure(\n    data=[\n        go.Surface(z=Z, x=X, y=Y, colorscale=\"Viridis\", opacity=0.7),\n        # Adding contours\n    ]\n)\n\n# Update layout for axis labels and limits\nfig.update_layout(\n    scene=dict(\n        xaxis=dict(title=\"X\"),\n        yaxis=dict(title=\"Y\"),\n        zaxis=dict(title=\"Z\"),\n    ),\n    title=\"3D Surface Plot of Beta Function\",\n)\n\n# Show the figure\nfig.show()\n\n\n                                                \n\n\n\n\n\nTheorem C.1 三角関数とベータ関数の関係 \n正の実数 \\(a, b\\) について，以下が成立する\n\\[\n\\operatorname{B}(a, b) = 2\\int^{\\pi/2}_0\\cos^{2a-1}\\theta\\sin^{2b-1}\\theta \\mathrm{d}\\theta\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nベータ関数について \\(x = \\cos^2\\theta\\) を用いた置換積分で以下のように示すことができます．\n\\[\n\\begin{align*}\n\\operatorname{B}(a, b) &= \\int^1_0 x^{a-1}(1-x)^{b-1} \\mathrm{d}x\\\\\n                       &= \\int^0_{\\pi/2} (\\cos^2\\theta)^{a-1}(1 - \\cos^2\\theta)^{b-1} \\cdot (-2\\cos\\theta\\sin\\theta) \\mathrm{d}\\theta\\\\\n                       &= 2\\int^{\\pi/2}_0 \\cos^{2a-1}\\theta \\sin^{2b-1}\\theta\\mathrm{d}\\theta\n\\end{align*}\n\\]\nなお, \\(\\sin^2\\theta = 1 - \\cos^2\\theta\\) 及び \\(\\displaystyle\\frac{\\mathrm{d}\\cos^2\\theta}{\\mathrm{d}\\theta} = -2\\cos\\theta\\sin\\theta\\) を用いている．\n\n\n\n\n\nTheorem C.2 引数の交換性 \n正の実数 \\(a, b\\) について，以下が成立する\n\\[\n\\operatorname{B}(a, b) = \\operatorname{B}(b, a)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\operatorname{B}(a, b) &= \\int^1_0 x^{a-1}(1-x)^{b-1} \\mathrm{d}x\\\\\n                       &= \\int^0_1 (1-z)^{a-1}z^{b-1} \\frac{\\mathrm{d}x}{\\mathrm{d}z}\\mathrm{d}z\\\\\n                       &= \\int^1_0 (1-z)^{a-1}z^{b-1} \\mathrm{d}z\\\\\n                       &= \\operatorname{B}(b, a)\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>ベータ関数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/gamma_function.html",
    "href": "posts/mathematical_appendix/gamma_function.html",
    "title": "Appendix D — ガンマ関数",
    "section": "",
    "text": "ガンマ関数の性質\nガンマ関数は，階乗関数を正の実数に拡張したものです． \\(n\\in\\mathbb N\\) について階乗関数は\n\\[\nn! = (n-1)!\\cdot n\n\\]\nと表されますが，ガンマ関数も\n\\[\n\\Gamma(z) = \\Gamma(z-1)\\cdot (z-1)\n\\]\nという性質があります．\nガンマ関数は，正の整数 \\(x\\) に対して，\\(y=x!\\) という平面上の点 \\((x, y)\\) を結ぶsmooth curveに対応します．\nCode\nfrom plotly import express as px\nimport numpy as np\nfrom scipy.special import gamma, factorial\n\nx = np.linspace(0, 6*1.05, 200)\nk = np.arange(1, 7)\n\nfig = px.line(x=x, y=gamma(x), title='Gamma function')\nfig.add_traces(\n    list(px.scatter(x=k, y=factorial(k-1)).select_traces())\n)\nfig.show()\n▶  \\(\\Gamma(n) = (n-1)!\\) の確認\nまず，\\(\\gamma(1) = 1\\) を確認します．\n\\[\n\\begin{align*}\n\\Gamma(1)\n    &= \\int^\\infty_0 x^{1-1}\\exp(-x)\\mathrm{d}x\\\\\n    &= \\int^\\infty_0 \\exp(-x)\\mathrm{d}x\\\\\n    &= \\left[-\\exp(-x)\\right]^\\infty_0\\\\\n    &= 1 = 0!\n\\end{align*}\n\\]\n続いて, \\(z\\in\\mathbb R_{++}\\) について，\\(\\Gamma(z) = \\Gamma(z-1)\\cdot (z-1)\\) が成立することを確認します．\n\\[\n\\begin{align*}\n\\Gamma(z+1)\n    &= \\int^\\infty_0 x^{z+1-1}\\exp(-x)\\mathrm{d}x\\\\\n    &= \\int^\\infty_0 x^{z}\\exp(-x)\\mathrm{d}x\\\\\n    &= [-x^z\\exp(-x)]^\\infty_0 + z\\int^\\infty_0 x^{z-1}\\exp(-x)\\mathrm{d}x\\\\\n    &= z\\Gamma(z)\n\\end{align*}\n\\]\nなお，\n\\[\n\\lim_{x\\to\\infty}-x^z\\exp(-x)=0\n\\]\nはロピタルの定理を用いています．",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ガンマ関数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/gamma_function.html#ガンマ関数の性質",
    "href": "posts/mathematical_appendix/gamma_function.html#ガンマ関数の性質",
    "title": "Appendix D — ガンマ関数",
    "section": "",
    "text": "Def: ガンマ関数 \n関数 \\(\\Gamma: \\mathbb R_{++}\\to\\mathbb R_{++}\\) を以下のように定義する：\n\\[\n\\Gamma(z) = \\int_0^\\infty x^{z-1}\\exp(-x)\\mathrm{d}x\n\\]\nこれをガンマ関数と呼ぶ．\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem D.1 \n\\(a &gt; 0\\) という定数をについて\n\\[\n\\int^\\infty_0t^{x-1}\\exp(-at)\\mathrm{d}t = \\frac{\\Gamma(x)}{a^x}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(u = at\\) という変数変換を考える．\\(\\frac{\\mathrm{d}t}{\\mathrm{d}u}=\\frac{1}{a}\\) であるので，\n\\[\n\\begin{align*}\n\\int^\\infty_0t^{x-1}\\exp(-at)\\mathrm{d}t\n    &= \\int^\\infty_0\\left(\\frac{u}{a}\\right)^{x-1}\\exp(-u)\\frac{1}{a}\\mathrm{d}u\\\\\n    &= \\frac{1}{a^x}\\int^\\infty_0u^{x-1}\\exp(-u)\\mathrm{d}u\\\\\n    &= \\frac{\\Gamma(x)}{a^x}\n\\end{align*}\n\\]\n\n\n\n\nExample D.1 \n\\[\n\\int^\\infty_{-\\infty}x^2\\exp(-ax^2)\\mathrm{d}x\n\\]\nも上述の定理を使うと簡単に計算できます．\\(x^2 = u\\) という変数変換を念頭に以下，\n\\[\n\\begin{align*}\n\\int^\\infty_{-\\infty}x^2\\exp(-ax^2)\\mathrm{d}x\n    &= 2\\int^\\infty_{0}x^2\\exp(-ax^2)\\mathrm{d}x \\quad\\because \\text{偶関数}\\\\\n    &= 2\\int^\\infty_{0}u\\exp(-au)\\times \\frac{1}{2\\sqrt{u}}\\mathrm{d} u\\\\\n    &= \\int^\\infty_{0}u^{\\frac{3}{2}-1}\\exp(-au)\\mathrm{d} u\\\\\n    &= \\frac{\\Gamma(3/2)}{a^{\\frac{3}{2}}}\\\\\n    &=\\frac{\\sqrt{\\pi}}{2a\\sqrt{a}}\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\n\nTheorem D.2 \nガンマ関数について，以下の等式が成り立つ\n\\[\n\\Gamma(s) = 2\\int_0^\\infty t^{2s-1}\\exp(-t^2)\\mathrm{d}t\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nガンマ関数について \\(t = u^2\\) という変数変換を行うと以下のように導けます．\n\\[\n\\begin{align*}\n\\Gamma(s)\n    &= \\int_0^\\infty t^{s-1}\\exp(-t)\\mathrm{d}t\\\\\n    &= \\int_0^\\infty u^{2s-2}\\exp(-u^2)\\times 2u \\mathrm{d}u\\\\\n    &= 2\\int_0^\\infty u^{2s-1}\\exp(-u^2)\\mathrm{d}u\\\\\n\\end{align*}\n\\]\n\n\n\n\nガンマ関数の有名な公式\n\n\nTheorem D.3 \n\\[\n\\Gamma\\bigg(\\frac{1}{2}\\bigg) = \\sqrt{\\pi}\n\\]\n\n\n\n\n\n\n\n\nProof: ガウス積分を用いる場合\n\n\n\n\n\n\\(t = u^2\\) という変数変換を考える．\n\\[\n\\begin{align*}\n\\int^\\infty_0t^{-1/2}\\exp(-t)\\mathrm{d}t\n    &= \\int^\\infty_0u^{-1}\\exp(-u^2)2u\\mathrm{d}u\\\\[3pt]\n    &= 2\\int^\\infty_0\\exp(-u^2)\\mathrm{d}u\\\\[3pt]\n    &= \\int^\\infty_{-\\infty}\\exp(-u^2)\\mathrm{d}u\\\\[3pt]\n    &= \\int^\\infty_{-\\infty}\\exp\\bigg(-\\frac{u^2}{2\\times(\\frac{1}{\\sqrt 2})^2}\\bigg)\\mathrm{d}u\\\\[3pt]\n    &= \\sqrt{2\\pi}\\frac{1}{\\sqrt 2}\\\\\n    &= \\sqrt{\\pi}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nProof: 極座標変換を用いる場合\n\n\n\n\n\n\\[\n\\begin{align*}\n\\Gamma\\bigg(\\frac{1}{2}\\bigg)^2 &= \\bigg(\\int^\\infty_0t^{-1/2}\\exp(-t)\\mathrm{d}t\\bigg)^2\\\\[3pt]\n                                &= \\int^\\infty_0x^{-1/2}\\exp(-x)\\mathrm{d}x\\int^\\infty_0y^{-1/2}\\exp(-y)\\mathrm{d}y\n\\end{align*}\n\\]\nここで，\\(x=u^2, y=v^2\\) と置換積分する．\n\\[\n\\begin{align*}\n\\int^\\infty_0x^{-1/2}\\exp(-x)\\mathrm{d}x\\int^\\infty_0y^{-1/2}\\exp(-y)\\mathrm{d}y\n    &= 4\\int^\\infty_0\\exp(-u^2)\\mathrm{d}u\\int^\\infty_0\\exp(-v^2)\\mathrm{d}v\\\\\n    &= 4\\int^\\infty_0\\int^\\infty_0\\exp[-(u^2+v^2)]\\mathrm{d}u\\mathrm{d}v\n\\end{align*}\n\\]\n更に，\\(u = r\\cos\\theta, v=r\\sin\\theta\\) という極座標変換を行う．\\(u, v\\geq 0\\) であることに留意すると，\\(r\\in(0, \\infty), \\theta\\in (0, \\pi/2)\\) になるので\n\\[\n\\begin{align*}\n4\\int^\\infty_0\\int^\\infty_0\\exp[-(u^2+v^2)]\\mathrm{d}u\\mathrm{d}v &= 4\\int^\\infty_0\\int^{\\pi/2}_0\\exp[-r^2]r \\mathrm{d}\\theta \\mathrm{d}r \\  \\  \\because u, v\\geq 0 \\\\[3pt]\n&= 4\\times \\frac{\\pi}{2} \\int^{\\infty}_0\\exp[-r^2]r\\mathrm{d}r\\\\[3pt]\n&= 4\\times \\frac{\\pi}{2}\\bigg[\\frac{\\exp(-r^2)}{-2}\\bigg]^\\infty_0\\\\[3pt]\n&= \\pi\n\\end{align*}\n\\]\nガンマ関数の定義より，\\(\\Gamma(1/2) &gt; 0\\) なので，\n\\[\n\\Gamma(1/2)^2 = \\pi \\Rightarrow \\Gamma(1/2) = \\sqrt{\\pi}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ガンマ関数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/jensen_inequality.html",
    "href": "posts/mathematical_appendix/jensen_inequality.html",
    "title": "Appendix E — Jensen’s Inequality",
    "section": "",
    "text": "Def: convex function \n区間 \\(I\\) で定義された関数 \\(f:I \\to \\mathbb R\\) がconvex(凸関数)であるとは, 任意の \\(0 &lt; t &lt; 1\\) について\n\\[\nf((1-t)x + ty) \\leq (1 - t)f(x) + t f(y)\\quad \\forall x, y \\in I, x \\neq y\n\\]\nstrictly convexであるとは\n\\[\nf((1-t)x + ty) &lt; (1 - t)f(x) + t f(y)\\quad \\forall x, y \\in I, x \\neq y\n\\]\n\n\n\nDef: concave function \n区間 \\(I\\) で定義された関数 \\(f:I \\to \\mathbb R\\) がconcave(凹関数)であるとは, 任意の \\(0 &lt; t &lt; 1\\) について\n\\[\nf((1-t)x + ty) \\geq (1 - t)f(x) + t f(y)\\quad \\forall x, y \\in I, x \\neq y\n\\]\nstrictly concaveであるとは\n\\[\nf((1-t)x + ty) &gt; (1 - t)f(x) + t f(y)\\quad \\forall x, y \\in I, x \\neq y\n\\]\n\n\n以下のような \\(\\exp(x), x^2, \\vert x\\vert\\) などが凸関数の例です.\n\n\nCode\nimport numpy as np\nimport plotly.express as px\nimport polars as pl\n\nx = np.linspace(-1, 1, 100)\nexp_x = np.exp(x)\nsquared_x = x**2\nabs_x = abs(x)\n\ndf = pl.DataFrame({\"x\": x, \"exp_x\": exp_x, \"squared_x\": squared_x, \"abs_x\": abs_x})\n\nfig = px.line(df, x=\"x\", y=[\"exp_x\", \"squared_x\", \"abs_x\"], title='example: convex fucntions')\nnewnames = {\"exp_x\": \"exp(x)\", \"squared_x\": \"x^2\", \"abs_x\": \"abs(x)\"}\nfig.for_each_trace(\n    lambda t: t.update(\n        name=newnames[t.name],\n        hovertemplate=t.hovertemplate.replace(t.name, newnames[t.name]),\n    )\n)\n\nfig.show()\n\n\n                                                \n\n\nまた, \\(\\ln(x), \\sqrt{x}\\) やconvext関数に \\(-1\\) を掛けたものは凹関数の例となります．\n\n\nCode\nx = np.linspace(0.05, 1.5, 100)\n\nln_x = np.log(x)\nsqrt_x = np.sqrt(x)\nsquared_x = -(x**2)\n\ndf = pl.DataFrame({\"x\": x, \"ln_x\": ln_x, \"sqrt_x\": sqrt_x, \"squared_x\": squared_x})\n\nfig = px.line(\n    df, x=\"x\", y=[\"ln_x\", \"sqrt_x\", \"squared_x\"], title=\"example: concave fucntions\"\n)\nnewnames = {\"ln_x\": \"log(x)\", \"sqrt_x\": \"sqrt(x)\", \"squared_x\": \"-x^2\"}\nfig.for_each_trace(\n    lambda t: t.update(\n        name=newnames[t.name],\n        hovertemplate=t.hovertemplate.replace(t.name, newnames[t.name]),\n    )\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\nTheorem E.1 \n関数 \\(f\\) が区間 \\([a, b]\\) で連続で \\((a, b)\\) で２回微分可能とする．このとき， 関数 \\(f\\) が凸関数であることの必要十分条件は\n\\[\nf^{\\prime\\prime}(x) \\geq 0 \\quad \\forall x\\in (a, b)\n\\]\n\n\n\n\n\nTheorem E.2 : Subgradient Inequality \n関数 \\(f\\) が区間 \\([a, b]\\) で凸関数であり，微分可能とする．このとき以下が成立する\n\\[\nf(y) \\geq f(x) + f^{\\prime}(x)(y-x) \\quad \\forall x, y\\in (a, b)\n\\]\n\n\n\n\n\nTheorem E.3 Jensen’s Inequality \n\\(\\mathbb E[X] = \\mu &lt; \\infty\\) 及び \\(I \\subset \\mathbb R\\) をサポートとする確率変数 \\(X\\) について，\\(g:I\\to \\mathbb R\\) というconvex functionを考える．\\(g\\) が 区間 \\(I\\) で微分可能としたとき，\n\\[\n\\mathbb E[g(X)] \\geq g(\\mathbb E[X])\n\\]\n\\(g\\) がstrictly convexの場合，\\(X\\) がdegenerateであることの必要十分条件は \\(\\mathbb E[g(X)] = g(\\mathbb E[X])\\)．\n\\(g(\\cdot)\\) がconcaveの場合は，\\(\\mathbb E[g(X)] \\leq g(\\mathbb E[X])\\) が成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(g(\\cdot)\\) はconvex functionなので,\n\\[\ng(X) \\geq g(\\mu) + g^\\prime(\\mu)(X - \\mu)\n\\]\n両辺について期待値をとると，\n\\[\n\\begin{align*}\n\\mathbb E[g(X)] &\\geq g(\\mathbb E[X]) + g^\\prime(\\mu)(\\mathbb E[X] - \\mu)\\\\\n                &= g(\\mathbb E[X])\n\\end{align*}\n\\]\n\n\n\n\nExample E.1 \n確率変数 \\(X &gt;0\\) がnon-degenerateであるとき，Jensen’s inequalityより \\(g(x) = 1/x\\) はstrictly convexなので\n\\[\n\\mathbb E\\left[\\frac{1}{X}\\right] &gt; \\frac{1}{\\mathbb E[X]}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\nExample E.2 : 不偏分散と標準偏差 \n確率変数 \\(X\\) について，\\(\\operatorname{Var}(X) = \\sigma^2\\) とする．母集団に対してランダムサンプリングを実施し，そこから得られた サンプルから不偏分散 \\(s^2\\) を得たとする．つまり，\n\\[\n\\mathbb E[s^2] = \\sigma^2\n\\]\nこのとき，\\(\\mathbb E[s] \\neq \\sigma\\) となってしまう．なぜなら，\\(h(x) = \\sqrt{x}\\) としたとき， 関数 \\(h\\) はstrictly concaveのため，Jensen’s inequalityより\n\\[\n\\begin{gather*}\n\\mathbb E[h(s^2)] &lt; h(\\mathbb E[s^2])\\\\\n\\Rightarrow \\mathbb E[S] &lt; \\sigma\n\\end{gather*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Jensen's Inequality</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/Lhopitals_rule.html",
    "href": "posts/mathematical_appendix/Lhopitals_rule.html",
    "title": "Appendix F — ロピタルの定理",
    "section": "",
    "text": "Theorem F.1 \n\\(x=a\\)に十分近い\\(x\\)について\\(f(x), g(x)\\)は微分可能とする. さらに\\(x=a\\)以外で\\(g(x)\\neq 0\\)とする\n(1). \\(\\lim_{x\\to a}f(x)=\\lim_{x\\to a}g(x)=0\\)のとき次式が成り立つ\n\\[\n\\lim_{x\\to a}\\frac{f(x)}{g(x)} = \\lim_{x\\to a}\\frac{f^\\prime(x)}{g^\\prime(x)}\n\\]\n(2). \\(\\lim_{x\\to a}\\vert f(x)\\vert= \\infty, \\lim_{x\\to a}\\vert g(x)\\vert=\\infty\\)のとき次式が成り立つ\n\\[\n\\lim_{x\\to a}\\frac{f(x)}{g(x)} = \\lim_{x\\to a}\\frac{f^\\prime(x)}{g^\\prime(x)}\n\\]\n(1), (2)で \\(a\\) を \\(\\infty, -\\infty\\) に置き換えても同様の命題が成り立つ.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n ▶  \\(a&lt;\\infty, \\lim_{x\\to a}f(x)=g(x)=0\\) の場合\n平均値の定理より\n\\[\n\\begin{align*}\n\\frac{f(x)}{g(x)} = \\frac{f(x)-f(a)}{g(x)-g(a)}= \\frac{f^\\prime(\\xi)}{g^\\prime(\\xi)}\n\\end{align*}\n\\]\nこのとき, \\(\\xi\\)は\\(a, x\\)の間の数. なお, \\(\\xi\\to a \\text{ as }x\\to a\\)なので\n\\[\n\\lim_{x\\to a}\\frac{f(x)}{g(x)} = \\frac{f^\\prime(a)}{g^\\prime(a)}\n\\]\n ▶  \\(a=\\infty, \\lim_{x\\to \\infty}f(x)=g(x)=0\\) の場合\n\\(x = \\frac{1}{t}\\)と変換し, 次の関数を考える\n\\[\n\\begin{align*}\n&h(t) = f(1/t)\\\\\n&k(t) = g(1/t)\\\\\n& \\lim_{t\\to 0} h(t) = \\lim_{t\\to 0} k(t) =  0\n\\end{align*}\n\\]\n従って,\n\\[\n\\begin{align*}\n&\\frac{h(t)}{k(t)} = \\frac{h(t)-h(0)}{k(t)-k(0)} = \\frac{h^\\prime(\\xi)}{k^\\prime(\\xi)}\\\\[3pt]\n&\\Rightarrow \\lim_{t\\to0}\\frac{h(t)}{k(t)} = \\lim_{t\\to0}\\frac{h^\\prime(t)}{k^\\prime(t)}\n\\end{align*}\n\\]\nよって,\n\\[\n\\lim_{x\\to\\infty}\\frac{f(x)}{g(x)} = \\lim_{x\\to\\infty}\\frac{f^\\prime(x)}{g^\\prime(x)}\n\\]\n\n\n\n\nExample F.1 : ロピタルの定理の使用例 \n\\[\n\\begin{align*}\n\\lim_{x\\to\\infty}\\frac{x^k}{e^x}\n      &= \\lim_{x\\to\\infty}\\frac{kx^{k-1}}{e^x}\\\\[3pt]\n      &= \\lim_{x\\to\\infty}\\frac{k(k-1)x^{k-2}}{e^x}\\\\[3pt]\n      &= \\cdots\\\\\n      &= \\lim_{x\\to\\infty}\\frac{k!}{e^x}\\\\[3pt]\n      &= 0\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>ロピタルの定理</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/chapter_header.html",
    "href": "posts/statistics101/chapter_header.html",
    "title": "統計学入門",
    "section": "",
    "text": "▶  統計学入門 のスコープ\n\n竹村彰通 (2020) をベースに，統計推測法に必要な基礎知識について勉強します\n日々のデータサイエンス分析のベースとなるような統計基礎概念や定義が勉強対象になりますが，これらについて数学的な定義とともにわかりやすい言語化ができるようになることを目的にしています\n\n ▶  記述統計と統計的推測\n\n 記述統計(descriptive statistics)\n\n調査や実験で得られたデータを整理して，その解釈を助けるような統計的分析のこと\n\n統計的推測(statistical inference)\n\n確率的な変動を多く含むデータに対して，そのDGP(= Data generating process)に何かしらの仮定を想定し，データから確率モデルの推定や検定を行う分析のこと\n\n\n\n「伏せられたトランプカードを透視することでスートを当てることができる！」という人がいたとします． この能力を試してみたところ５２枚のカードの内，４０枚を当てることができたというデータが得られました． この，40枚当てることができたというデータについて確率論的意味を判断をするというのが統計的推測です．\nこのように統計的推測とは確率モデルを想定してデータを解釈/判断する分析なので，確率論を中心とする数学的表現を用いたモデルの定式化 （=ランダムネスの法則を扱う数学理論）が必要となります．加えて，\n\n想定した確率モデルが正しそうか？\n与えられたデータと矛盾しないか？\n乖離がある場合，想定したモデルから導かれる結論はどの程度妥当すると言えるのだろうか？\n\nという分析上の判断も必要となります．そのため，統計的推測はデータ分析初心者にとっては敷居が高い手法となりますが， 一旦モデル化や仮定の妥当性についての説明がうまく行くと，手元に実際にあるデータの背後にあるメカニズムに基づいて推測が行えるようになります． 例えば，実際に観測できないPotential Outcomesの分布についての推定や将来の予測などがあります．\nこのように統計的推測とは，使いこなすのは大変ですが，使えるようになるととても強力なツールです．このノートを通じて，統計的推測の基礎やいくつかの応用分野の分析を見ながらマスターしたいなと思っています．\n ▶  測定の尺度\n\n\n\n\n\n\n\n\n種類\nmeasurement\n説明\n\n\n\n\nカテゴリカルデータ\n名義尺度(nominal scale)\nある対象が他と同一か，異なるかを表す測定例: 性別, 血液型\n\n\nカテゴリカルデータ\n順序尺度(ordinal scale)\n大小/優劣関係を表す測定例: ４段階評価の健康状態\n\n\n量的データ\n間隔尺度(interval scale)\n0が相対的な意味をもつ指標例: 気温，知能指数，標高\n\n\n量的データ\n比例尺度(ratio scale)\n0が絶対的な意味を持つ指標例: 身長，金額，時間の経過，絶対温度\n\n\n\n\n\n\n\n竹村彰通 (2020), 現代数理統計学, 学術図書出版社.",
    "crumbs": [
      "統計学入門"
    ]
  },
  {
    "objectID": "posts/probability_distribution/chapter_header.html",
    "href": "posts/probability_distribution/chapter_header.html",
    "title": "確率分布",
    "section": "",
    "text": "▶  この章のスコープ\nデータの背後にある確率モデルを構築するときには，研究対象の特徴と確率分布の特徴を踏まえて 適した分布の選択を行うことが望ましいです．例として，\n\n所得ならば正の値をとり，右に長い裾をもつので対数正規分布\n生物・人体測定値ならば正規分布\nシステムの耐久年数を表すならばガンマ分布\n捕獲再捕獲の確率ならば超幾何分布\n\nこの章では，代表的な確率分布に関して，確率分布の形状や特性値，確率分布がもっている性質について解説していきます．",
    "crumbs": [
      "確率分布"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/chapter_header.html",
    "href": "posts/statistical_hypothesis_test_201/chapter_header.html",
    "title": "統計的仮説検定の実践",
    "section": "",
    "text": "References",
    "crumbs": [
      "統計的仮説検定の実践"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/chapter_header.html#references",
    "href": "posts/statistical_hypothesis_test_201/chapter_header.html#references",
    "title": "統計的仮説検定の実践",
    "section": "",
    "text": "柳川堯 (2018), P値: その正しい理解と適用, 近代科学社.\n\n\n永田靖 (2003), サンプルサイズの決め方, 朝倉書店.",
    "crumbs": [
      "統計的仮説検定の実践"
    ]
  },
  {
    "objectID": "posts/estimation/chapter_header.html",
    "href": "posts/estimation/chapter_header.html",
    "title": "統計的推定",
    "section": "",
    "text": "点推定\n想定されるパラメトリックな確率分布が \\(k\\) 個の未知パラメータ\n\\[\n\\pmb{\\theta} = (\\theta_1, \\cdots, \\theta_k)\n\\]\nを含んでいるとし，確率関数を \\(f(x;\\pmb\\theta)\\) で表します．\\(\\pmb X = (X_1, \\cdots, X_n)\\) が \\(f(x;\\pmb\\theta)\\) から独立にサンプリングされるとすると，\n\\[\nX_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} f(x;\\pmb\\theta)\n\\]\nと表せます．",
    "crumbs": [
      "統計的推定"
    ]
  },
  {
    "objectID": "posts/estimation/chapter_header.html#点推定",
    "href": "posts/estimation/chapter_header.html#点推定",
    "title": "統計的推定",
    "section": "",
    "text": "Def: 点推定，推定量，推定値 \n\\(\\pmb X\\) の関数として，未知のパラメータ \\(\\pmb{\\theta}\\) を求めることを点推定と呼ぶ． また，この関数を \\(\\widehat{\\pmb{\\theta}}(\\pmb{X})\\) と表記し，\\(\\pmb{\\theta}\\) の推定量と呼ぶ． この関数に \\(\\pmb{X}\\) の実現値 \\(\\pmb{x}\\) を代入した \\(\\widehat{\\pmb{\\theta}}(\\pmb{x})\\) を推定値と呼ぶ．\n\n\n望ましい推定量の性質\n\nDef: 一致性(consistency) \n\\(n\\to\\infty\\) のとき，推定量 \\(\\widehat{\\pmb{\\theta}}(\\pmb{X})\\) が \\(\\pmb{\\theta}\\) に確率収束するとき， \\(\\widehat{\\pmb{\\theta}}(\\pmb{X})\\) は \\(\\pmb{\\theta}\\) の一致推定量という．\n\\[\n\\lim_{n\\to\\infty}\\Pr(\\vert\\widehat{{\\pmb\\theta}}-\\pmb\\theta\\vert &gt; \\epsilon; \\pmb\\theta) = 0 \\quad\\forall \\epsilon &gt; 0\n\\]",
    "crumbs": [
      "統計的推定"
    ]
  },
  {
    "objectID": "posts/bayesian_estimation/chapter_header.html",
    "href": "posts/bayesian_estimation/chapter_header.html",
    "title": "ベイズ推定",
    "section": "",
    "text": "ベイズ推測の考え方\nベイズ推測における確率は，未知の量について人々のもつ合理的な情報や信念(belief)を数値的に表したものとしています． 日常生活において，情報が更新されることにあわせて信念が更新されますが(例: 相手が飛車を振ってきたら，少なくとも居飛車はないだろうなど)， ベイズ推測でもベイズルールに基づいて，新たな情報の観測によって信念を更新していきます．\nある集団の一般的な特性を表すパラメータ \\(\\theta\\) について，その集団からサンプリングされた標本データ \\(y\\) が得られる前では， そのパラメータは不確定ですが，データ \\(y\\) が得られたあとでは，その情報がパラメータに関する不確実性(uncertainty)を減らすことになります． この不確実性の変化を測るのがベイズ推測の目的の一つとなります．",
    "crumbs": [
      "ベイズ推定"
    ]
  },
  {
    "objectID": "posts/bayesian_estimation/chapter_header.html#ベイズ推測の考え方",
    "href": "posts/bayesian_estimation/chapter_header.html#ベイズ推測の考え方",
    "title": "ベイズ推定",
    "section": "",
    "text": "ベイズルール\n\n\n\n\\[\np(\\theta\\vert y)\n    = \\frac{p(y\\vert \\theta)p(\\theta)}{\\int_{\\Theta}p(y\\vert\\tilde\\theta)p(\\tilde\\theta)\\,\\mathrm{d}\\tilde\\theta}\n\\]\n上に基づいて，観測データと事前分布から事後分布を更新しますが，ベイズルールが示しているのは 事後分布がどうあるべきではなくて，新たな情報を得たときに事後分布がどう変化するか？です．\n\n\n\nベイズ法による推定\n確率変数 \\(\\pmb X \\sim D(\\pmb\\theta)\\) について，確率分布 \\(D\\) を特徴づけるパラメーター \\(\\pmb\\theta\\) を推定したいとします．ベイズ推測では，同時確率（密度）関数 \\(f(\\pmb x\\vert \\pmb \\theta)\\) において \\(\\pmb \\theta\\) を確率変数とみなして確率分布を仮定します．\nこの \\(\\pmb \\theta\\) について仮定された分布のことを \\(\\pmb \\theta\\) についての事前分布といい，\n\\[\n\\pi(\\pmb \\theta\\vert \\pmb\\xi)\n\\]\nと表したりします．\\(\\pmb\\xi\\) は事前分布のパラメータでhyper-parameter(超母数)と呼ばれるものです． ここまでのモデルを整理すると次のようになります\n\\[\n\\begin{align*}\n\\left\\{\\begin{array}{c}\n\\pmb X\\vert \\pmb\\theta & \\sim f(\\pmb x\\vert \\pmb\\theta)\\\\\n\\pmb\\theta & \\sim \\pi(\\pmb \\theta\\vert \\pmb\\xi)\n\\end{array}\\right.\n\\end{align*}\n\\]\nこのとき，\\(\\pmb X = \\pmb x\\) を与えたときの \\(\\pmb\\theta\\) の事後分布は，\\(f_\\pi(\\pmb x\\vert\\pmb\\xi)\\) を \\(\\pmb X\\) の周辺分布とすると\n\\[\n\\begin{gather}\nf_\\pi(\\pmb x\\vert\\pmb\\xi) = \\int f(\\pmb x\\vert \\pmb\\theta)\\pi(\\pmb\\theta\\vert\\xi)\\,\\mathrm{d}\\theta\\\\\n\\pi(\\pmb\\theta\\vert \\pmb x, \\pmb \\xi) = \\frac{f(\\pmb x\\vert \\pmb\\theta)\\pi(\\pmb \\theta\\vert \\pmb\\xi)}{f_\\pi(\\pmb x\\vert\\pmb\\xi)}\n\\end{gather}\n\\]\nこの事後分布 \\(\\pi(\\pmb\\theta\\vert \\pmb x, \\pmb \\xi)\\) から推定量を導くのがベイズ法です．事後分布のモードを Bayesian maximum likelihood estimatorと呼んだりします．\n ▶  十分統計量とベイズ法\n\\(\\pmb\\theta\\) に対する十分統計量を \\(T(\\pmb X)\\) とすると，\\(\\pmb X\\) の条件付き確率密度関数は\n\\[\n\\begin{align*}\nf(\\pmb x\\vert\\pmb\\theta) &= h(\\pmb x) g(T(\\pmb x)\\vert\\pmb\\theta)\\\\\nh(\\pmb x) &= P(\\pmb X = \\pmb x\\vert T(\\pmb X))\n\\end{align*}\n\\]\nと十分統計量の定義より表せます．これを用いると事後分布 \\(\\pi(\\pmb\\theta\\vert \\pmb x, \\pmb \\xi)\\) は次のように整理できます:\n\\[\n\\begin{align*}\n\\pi(\\pmb\\theta\\vert \\pmb x, \\pmb \\xi)\n    &= \\frac{f(\\pmb x\\vert \\pmb\\theta) \\pi(\\pmb\\theta \\vert\\pmb\\xi)}{\\int f(\\pmb x\\vert \\pmb\\theta)\\pi(\\pmb\\theta\\vert\\xi)\\,\\mathrm{d}\\theta}\\\\[5pt]\n    &= \\frac{h(\\pmb x)g(T(\\pmb x) \\vert \\pmb\\theta)\\pi(\\pmb \\theta\\vert \\pmb\\xi)}{\\int h(\\pmb x)g(T(\\pmb x) \\vert \\pmb\\theta)\\pi(\\pmb\\theta\\vert\\xi)\\,\\mathrm{d}\\theta}\\\\[5pt]\n    &= \\frac{g(T(\\pmb x) \\vert \\pmb\\theta)\\pi(\\pmb \\theta\\vert \\pmb\\xi)}{\\int g(T(\\pmb x) \\vert \\pmb\\theta)\\pi(\\pmb\\theta\\vert\\xi)\\,\\mathrm{d}\\theta}\\\\\n\\end{align*}\n\\]\n\nExample 1 : 事前分布平均と標本平均の加重平均 \n確率変数 \\(X\\sim\\operatorname{Binom}(n, \\theta)\\) とし，\\(\\theta\\) に事前分布 \\(\\operatorname{Beta}(\\alpha, \\beta)\\) を仮定します． \\(x\\) をデータとして観測したとき，\\(\\theta\\) の事後分布は，\\(x, \\theta\\) の同時分布に比例するので\n\\[\n\\begin{align*}\nf(x\\vert\\theta)\\pi(\\theta\\vert\\alpha,\\beta)\n    &\\propto \\theta^x(1 - \\theta)^{n-x}\\theta^{\\alpha-1}(1 - \\theta)^{\\beta-1}\\\\[5pt]\n    &= \\theta^{x+\\alpha-1}(1 - \\theta)^{n-x+\\beta-1}\n\\end{align*}\n\\tag{1}\\]\n従って，\\(\\theta\\) の事後分布は\n\\[\n\\theta\\vert\\pmb x, \\alpha,\\beta \\sim \\operatorname{Beta}(x+\\alpha, n-x+\\beta)\n\\]\n事後分布の期待値をベイズ推定量 \\(\\hat\\theta\\) とすると, \\(w = \\alpha + \\beta, \\theta_0 = \\frac{\\alpha }{\\alpha + \\beta}\\) とおくことで\n\\[\n\\hat\\theta = \\frac{n}{w + n}\\frac{X}{n} + \\frac{w}{w + n}\\theta_0\n\\]\nベイズ更新後の \\(\\theta\\) の期待値は事前分布における期待値と標本平均の加重平均となっていることがわかります．\n\n\n\n\n\n\n🍵 Green Tea Break\n\n\n\nEquation 1 で \\({}_nC_{x}\\) (以下では \\(C(x)\\) と表記する) 及び ベータ分布の規格化定数(\\(\\operatorname{B}\\) と表記する)という項が消えていますが，次のように事後分布の計算には不必要であることがわかります\n\\[\n\\begin{align*}\n&\\frac{C(x)\\theta^x(1 - \\theta)^{n-x}\\operatorname{B}\\theta^{\\alpha-1}(1 - \\theta)^{\\beta-1}}{\\int C(x)\\theta^x(1 - \\theta)^{n-x}\\operatorname{B}\\theta^{\\alpha-1}(1 - \\theta)^{\\beta-1}\\mathrm{d}\\theta }\\\\\n&=\\frac{\\theta^{x+\\alpha-1}(1 - \\theta)^{n-x+\\beta-1}}{\\int \\theta^{x+\\alpha-1}(1 - \\theta)^{n-x+\\beta-1}\\mathrm{d}\\theta }\\\\\n\\end{align*}\n\\]\n結局分子と分母で打ち消されあうので考慮しなくて良いことがわかります．また最後の式は \\(\\operatorname{Beta}(x+\\alpha, n-x+\\beta)\\) となっていることがわかります． この例では，事前分布と事後分布が同じ分布族に入っています．このような事前分布を共役事前分布(conjugate prioor distribution)と呼びます．\n\n\n\n\nExample 2 : 正規分布とベイズ推定 \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} N(\\mu, \\sigma)\\), \\(\\mu\\) についての事前分布を \\(N(\\xi, \\tau^2)\\) とする． ここでは \\((\\sigma, \\xi, \\tau^2)\\) はハイパーパラメータとする．\n\\((x_1, \\cdots, x_n)\\) を観測した下での, \\(\\mu\\) の事後分布 \\(\\pi(\\mu\\vert \\pmb x)\\) は\n\\[\n\\begin{align*}\n\\pi(\\mu\\vert \\pmb x)\n    &\\propto \\phi(\\pmb x\\vert \\mu, \\sigma^2)\\pi(\\mu\\vert \\xi, \\tau^2)\\\\[5pt]\n    &\\propto \\exp\\left(-\\frac{\\sum (x_i -\\mu)^2}{2\\sigma^2}\\right)\\exp\\left(-\\frac{(\\mu - \\xi)^2}{2\\tau^2}\\right)\n\\end{align*}\n\\tag{2}\\]\nここで，\\(\\overline{x}\\) を標本平均とすると\n\\[\n\\begin{align*}\n\\sum (x_i -\\mu)^2\n    &= \\sum (x_i -\\overline{x} + \\overline{x} - \\mu)^2 \\\\\n    &= \\sum (x_i -\\overline{x})^2 + 2 \\underbrace{\\sum x_i(x_i -\\overline{x})}_{=0} + \\sum (\\overline{x} - \\mu)^2\\\\\n    &= \\sum (x_i -\\overline{x})^2 + n(\\overline{x} - \\mu)^2\n\\end{align*}\n\\]\n従って，Equation 2 は次のように整理できます\n\\[\n\\begin{align*}\n&\\exp\\left(-\\frac{\\sum (x_i -\\mu)^2}{2\\sigma^2}\\right)\\exp\\left(-\\frac{(\\mu - \\xi)^2}{2\\tau^2}\\right)\\\\\n&=\\exp\\left\\{-\\frac{1}{2}\\left(\\frac{\\sum (x_i - \\overline{x})^2 + n(\\overline{x} - \\mu)^2}{\\sigma^2} + \\frac{\\mu^2 - 2\\xi\\mu + \\xi^2}{\\tau^2}\\right)\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left(\\frac{n\\mu^2 - 2n\\overline{x}\\mu}{\\sigma^2} + \\frac{\\mu^2 - 2\\xi\\mu}{\\tau^2}\\right)\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left(\\frac{n}{\\sigma^2}{ + \\frac{1}{\\tau^2}}\\right)\\left(\\mu - \\frac{(n/\\sigma^2)\\overline{x} + (1/\\tau^2)\\xi}{n/\\sigma^2 + 1/\\tau^2}\\right)^2\\right\\}\n\\end{align*}\n\\]\n分子分母で \\(\\mu\\) と関係のない項はキャンセルアウトされるので\n\\[\n\\begin{align*}\n\\hat\\mu_{B} &= \\frac{(n/\\sigma^2)\\overline{x} + (1/\\tau^2)\\xi}{n/\\sigma^2 + 1/\\tau^2}\n\\end{align*}\n\\]\nと表記すると\n\\[\n\\begin{align*}\n\\pi(\\mu\\vert \\pmb x)\n    &= \\frac{\\exp\\left\\{\\frac{(\\mu - \\hat\\mu_{B})^2}{2(n/\\sigma^2 + 1/\\tau^2)^{-1}}\\right\\}}{\\int \\exp\\left\\{\\frac{(\\mu - \\hat\\mu_{B})^2}{2(n/\\sigma^2 + 1/\\tau^2)^{-1}}\\right\\} \\,\\mathrm{d}\\mu}\\\\\n    &=\\frac{1}{\\sqrt{2\\pi (n/\\sigma^2 + 1/\\tau^2)^{-1}}}\\exp\\left\\{\\frac{(\\mu - \\hat\\mu_{B})^2}{2(n/\\sigma^2 + 1/\\tau^2)^{-1}}\\right\\}\n\\end{align*}\n\\]\n従って，これは \\(N(\\hat\\mu_{B}, [n/\\sigma^2 + 1/\\tau^2]^{-1})\\) の密度関数の形をしているので\n\\[\n\\mu\\vert\\pmb{X} \\sim N(\\hat\\mu_{B}, [n/\\sigma^2 + 1/\\tau^2]^{-1})\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]",
    "crumbs": [
      "ベイズ推定"
    ]
  },
  {
    "objectID": "posts/robust_statistics/chapter_header.html",
    "href": "posts/robust_statistics/chapter_header.html",
    "title": "Data Analysis and Outliers",
    "section": "",
    "text": "▶  Robust Statistics のスコープ\n\n藤澤洋徳 (2017) をベースに，外れ値に対する分析上の対処方法について勉強する\n\n ▶  ロバスト推定とロバスト検定\n\nDef: ロバスト推定とロバスト検定 \n\nロバスト推定: 外れ値に頑健な推定(estimation)\nロバスト検定: 外れ値の混入に頑健な検定\n\n\n\n\n\n\n藤澤洋徳 (2017), ロバスト統計 : 外れ値への対処の仕方（ISMシリーズ : 進化する統計数理 / 統計数理研究所編, 6, 近代科学社.",
    "crumbs": [
      "Data Analysis and Outliers"
    ]
  },
  {
    "objectID": "posts/econometrics101/chapter_header.html",
    "href": "posts/econometrics101/chapter_header.html",
    "title": "Econometrics Topics",
    "section": "",
    "text": "Econometrics Topics のスコープ\n\n\n\n\n\nEconometric Analysisの基本的な考え\n ▶  ceteris paribus\n\nceteris paribusとは「holding all other relevant factors fixed」を意味する概念\n\nとある確率変数 \\(X\\) の変化が別の確率変数 \\(Y\\) の変化を引き起こす（cause）とデータから主張するためには，単に同時分布（相関関係）を確認するだけでは不十分で，他の変数を固定した上(ceteris paribus)で， \\(X\\) の変化が \\(Y\\) の変化を伴うことを示す必要があります．\n ▶  Asymptotics\n\nfinite sample propertyと対になる概念で，\\(N\\to\\infty\\) に飛ばした極限分布における統計量の性質のこと\ncross section dataの場合は, unit of observations を \\(N\\to\\infty\\)\npanel data analysisの場合は，time indexを固定した上で unit of entitie sを \\(N\\to\\infty\\)\n\n ▶  説明変数(regressor)が確率的である\n\n説明変数が非確率的である例として，実験データのように説明変数 \\(\\mathbf x_i\\) の水準について分析者が事前に決定できる場合がある\n\nこの場合，error termと説明変数の相関（内生性問題）は排除できる\n\n観測データの場合は，実験データのように説明変数 \\(\\mathbf x_i\\) の水準については決定できないため，「非確率的」という仮定は通常当てはまらない\n\n説明変数が確率的である場合，非確率的のもとでは一致性を満たす推定量(例: GLS)が一致性を満たさなくなるリスクがあります．",
    "crumbs": [
      "Econometrics Topics"
    ]
  }
]