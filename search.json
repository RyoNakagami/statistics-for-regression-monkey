[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Regression Monkeys",
    "section": "",
    "text": "Welcome\nこのQuarto Bookは以下のシリーズと連動して運用されています:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Statistics for Regression Monkeys",
    "section": "References",
    "text": "References\n\n\n柳川堯 (2018), P値:\nその正しい理解と適用, 近代科学社.\n\n\n永田靖 (2003), サンプルサイズの決め方,\n朝倉書店.\n\n\n竹村彰通 (2020), 現代数理統計学,\n学術図書出版社.\n\n\n藤澤洋徳 (2017), ロバスト統計\n: 外れ値への対処の仕方（ISMシリーズ : 進化する統計数理 /\n統計数理研究所編, 6, 近代科学社.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html",
    "href": "posts/statistics101/averages.html",
    "title": "1  代表値",
    "section": "",
    "text": "平均\n▶  標本平均\nデータ \\(X = \\{x_1, \\cdots, x_n\\}\\) が与えられとき，標本平均（sample mean） \\(\\overline{x}\\) は次になります：\n\\[\n\\overline{x} = \\frac{x_1 + \\cdots + x_n}{n}\n\\]\n標本平均は分布の代表値として最も使用されるものだが，外れ値に対して弱い性質がある．\nimport numpy as np\n\nX_0 = np.array([5.6, 5.7, 5.4, 5.5, 5.8, 5.2, 5.3, 5.6, 5.4, 55.5])\nX_1 = np.array([5.6, 5.7, 5.4, 5.5, 5.8, 5.2, 5.3, 5.6, 5.4])\n\nprint(\n    \"\"\"X_0: sample mean = {}, median = {}\\nX_1: sample mean = {}, median = {}\n      \"\"\".format(\n        np.mean(X_0), np.median(X_0), np.mean(X_1), np.median(X_1)\n    )\n)\n\nX_0: sample mean = 10.5, median = 5.55\nX_1: sample mean = 5.5, median = 5.5\n上記の例のように，medianは外れ値の混入があってもその影響は軽微ですが，標本平均は大きく変わっており外れ値に対して弱いことがわかる．\n▶  刈り込み平均\n外れ値の影響を弱めて標本平均を推定する方法として，刈り込み平均(trimmed mean)があります． 上側 \\(100\\alpha \\%\\) と下側 \\(100\\alpha \\%\\) を使わないで推定する方法で，\\(x_{[i]}\\) を順序統計値として\n\\[\n\\hat\\mu_\\alpha = \\frac{1}{n-2m} \\sum_{i=m+1}^{n-m}x_{[i]}, \\  \\ m = \\lfloor n\\alpha \\rfloor\n\\]\nで推定する方法を刈り込み平均という．利用にあたって，外れ地の割合を事前に想定する必要がありますが， 少々適当に推定しても妥当な推定になりやすい特徴があります．\n\\(\\alpha = 0.1\\) としてPythonで計算してみると以下，\nfrom scipy import stats\nself_trimmed_mean = np.mean([5.7, 5.4, 5.5, 5.8, 5.4, 5.3, 5.6, 5.6])\ntrimmed_mean = stats.trim_mean(X_0, 0.1)\nprint(self_trimmed_mean, trimmed_mean)\n\n5.5375 5.5375\nTrimmed meanはARE(Asymptotic relative efficiency, 漸近相対効率)という観点からも大抵の裾の重さに対して（重すぎるのは厳しいですが．．．）高いパフォーマンスがあることが知られています．\n▶  幾何平均\n\\(x_i &gt; 0\\) となるようなデータについて，幾何平均は以下のように計算されます：\n\\[\n\\overline{x}_G = \\bigg(\\prod_{i=1}^n x_i \\bigg)^{\\frac{1}{n}}\n\\]\n2000年から2005年までのXこくのでの物価上昇率が2%, 5%, 2%, 5%, 10%とあるとき，年平均上昇率は算術平均ではなく幾何平均で計算すべきで\n\\[\n(1.02 \\times 1.05 \\times 1.02 \\times 1.05 \\times 1.1)^{1/5} \\approx 1.0476\n\\]\nすなわち年平均約4.8%の増加と報告すべきとなります．なお，相加相乗平均より，幾何平均は算術平均より小さい値になることがわかります．もし大きい値を出してしまっていたら計算ミスを疑うべきです．\n幾何平均について対数をとると以下のように算術平均で表すことができます\n\\[\n\\log(\\overline{x}_G) = \\frac{1}{n}\\sum \\log(x_i)\n\\]\nここから，幾何平均を計算するときは一旦log transformationを実行し，算術平均を計算し，その後 \\(\\exp(\\cdot)\\) で元のスケールに戻すという形でよく計算されます．\nクラス分類の評価指標との関係では，sensitivity(感応度)とspecificity(特異度)の幾何平均を用いたG-Mean(geometric mean)という指標があります．\n\\[\n\\begin{align*}\n\\operatorname{G-mean} &= \\sqrt{\\operatorname{sensitivity} \\times \\operatorname{specificity}}\\\\\n                      &= \\sqrt{\\operatorname{recall} \\times \\operatorname{True Negative Rate}}\n\\end{align*}\n\\]\n▶  調和平均\n\\(x_i &gt; 0\\) となるようなデータについて，調和平均(harmonic mean)は以下のように計算されます：\n\\[\n\\frac{1}{\\overline{x}_H} = \\frac{1}{n}\\sum\\frac{1}{x_i}\n\\]\nとある車が距離 \\(\\alpha\\) の区間Aでは25km/h, 距離 \\(\\beta\\) の区間Bでは15km/hで走っていたとします．このとき，この車の平均時速は\n\\[\n\\frac{1}{\\text{平均時速}} = \\frac{\\alpha}{\\alpha + \\beta} \\frac{1}{25} + \\frac{\\beta}{\\alpha + \\beta} \\frac{1}{15}\n\\]\n\\(\\alpha = \\beta\\) のときは\n\\[\n\\frac{1}{\\text{平均時速}}  = \\frac{1}{2} \\bigg(\\frac{1}{25} + \\frac{1}{15}\\bigg)\n\\]\n平均を計算するにあたって，値が同じスケールの単位である必要であるため，上の平均時速の例では調和平均を利用することが 好ましいとされます．なお区間Aをx時間で25km/h, 区間Bをy時間で15km/hという場合はウェイトが時間単位で表されているので\n\\[\n\\text{平均時速} = \\frac{x}{x + y} \\times 25 + \\frac{y}{x + y} \\times 15\n\\]\nモデルの評価指標の１つにprecisionとrecallを用いたF1-scoreがありますが，precisionとrecallも分子はそれぞれTrue Positiveで共通していますが，分母がそれぞれ \\(\\operatorname{TP} + \\operatorname{FP}, \\operatorname{TP} + \\operatorname{FN}\\) と異なっているので，調和平均を用いて以下のように計算します：\n\\[\n\\begin{align*}\n\\operatorname{F1-score} &= \\frac{1}{\\frac{1}{2} \\left(\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}\\right)}\\\\\n&= \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}}\n\\end{align*}\n\\]\nなお，これはウェイトが等しい場合を意味しており，weighted harmonic meanへ拡張する場合は以下のように \\(\\operatorname{F_\\beta-score}\\) を用いて計算します\n\\[\n\\operatorname{F_\\beta-score} = \\frac{1 + \\beta^2}{\\frac{1}{\\text{precision}} + \\frac{\\beta^2}{\\text{recall}}}\n\\]\nウェイトが \\(\\frac{1}{1 + \\beta^2}, \\frac{\\beta^2}{1 + \\beta^2}\\) の形を取っているのは一見不自然に見えますが，その考察で面白いのがvan Rijs-bergen’s E (effectiveness) functionに基づいた説明です．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html#平均",
    "href": "posts/statistics101/averages.html#平均",
    "title": "1  代表値",
    "section": "",
    "text": "Theorem 1.1 \n定義域が \\(\\mathbb R_+\\) の確率変数 \\(X\\) を考える（つまり \\(X &gt; 0\\)）. このとき,\n\n\\(H_x\\): 調和平均\n\\(G_x\\): 幾何平均\n\\(\\overline{X}\\): 標本平均\n\nとして以下が常に成り立つ\n\\[\nH_x \\leq G_x \\leq \\overline{X}\n\\]\n\n\n\n\n\n\n\n\nProof: Jensen’s inequalityを用いた証明\n\n\n\n\n\nJensen’s inequalityより 関数 \\(g\\) を凸関数(convex function)とすると\n\\[\n\\frac{1}{n}\\sum_{i=1}^ng(x_i)\\geq g(\\overline{x})\n\\]\nという不等式が成り立つ.\n ▶  \\(\\overline{X} \\geq G_x\\) の証明\n\\(f(x) = -\\log(x)\\) とすると \\(f\\) は単調減少の凸関数であるので\n\\[\n\\begin{align*}\n-\\log(\\overline{x}) &\\leq -\\frac{1}{n}\\sum_{i=1}^n\\log(x_i)\\\\\n                    &= -\\log(\\prod_{i=1}^n x_i^{1/n})\\\\\n                    &= -\\log(G_x)\n\\end{align*}\n\\]\nつまり，\\(\\log(\\overline{x})\\geq \\log(G_x) \\Rightarrow \\overline{X} \\geq G_x\\)\n ▶  \\(G_x \\geq H_x\\) の証明\n\\(1/x_i = y_i\\) と変換すると\n\\[\n\\begin{align*}\nG_x &= \\left(\\prod \\frac{1}{y_i}\\right)^{1/n}\\\\\nH_x &= \\frac{1}{\\overline y}\n\\end{align*}\n\\]\nそれぞれについて \\(f(x) = \\log(x)\\) とすると\n\\[\n\\begin{align*}\n\\log(G_x) &= \\frac{1}{n}\\sum_{i=1}^n(-\\log(y_i))\\\\\n\\log(H_x) &= -\\log(\\overline y)\n\\end{align*}\n\\]\n\\(-\\log(\\cdot)\\) は凸関数であるので\n\\(\\log(G_x) \\geq \\log(H_x) \\Rightarrow G_x \\geq H_x\\) を得る．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html#中央値",
    "href": "posts/statistics101/averages.html#中央値",
    "title": "1  代表値",
    "section": "中央値",
    "text": "中央値\n\nDef: median \nデータ \\(x_1, \\cdots, x_n\\) を小さい順に並び替えた順序統計量\n\\[\nx_{[1]} &lt; \\cdots &lt; x_{[n]}\n\\]\nについて，真ん中の値を中央値という．つまり，\n\\[\n\\operatorname{Med}(X) = \\left\\{\\begin{array}{cl}\nx_{[k]} & \\text{where } n = 2k-1\\\\\n\\displaystyle \\frac{x_{[k]} + x_{[k+1]}}{2} & \\text{where } n = 2k\n\\end{array}\\right.\n\\]\n\n ▶  Hodges-Lehmann推定量\n標本の中からペアを選び，そのヘアの平均の中央値を用いて中央値を推定するのがホッジスレーマン推定値です．\n\\[\n\\hat\\mu_{HL} = \\operatorname{Med}\\bigg(\\bigg\\{\\frac{x_i + x_j}{2}\\bigg\\}_{1\\leq i \\leq j \\le n}\\bigg)\n\\]\ncomputation上少し重たいですが計算例として以下，\n\nimport itertools\n\ndef HL_mean(x: list[tuple]):\n    return np.median([np.mean(t) for t in itertools.combinations(x, 2)])\n\nX_0 = np.array([5.6, 5.7, 5.4, 5.5, 5.8, 5.2, 5.3, 5.6, 5.4, 55.5])\nprint(HL_mean(X_0))\n\n5.55\n\n\n\nTheorem: Asymptotic distribution of sample quantile-p \n\\(y_1, \\cdots, y_n\\) を density function \\(f\\) 及びquantile function \\(Q^{(p)}\\) を持つ分布からのi.i.dとします．このとき, sample quantile \\(\\hat Q^{(p)}\\) は\n\\[\n\\sqrt{n}(\\hat Q^{(p)} - Q^{(p)}) \\rightarrow_d \\mathbb N\\left(0,\\frac{p(1-p)}{f(Q^{(p)})^2}\\right)\n\\]\n\nここでは，連続変数分布を想定して解説します．連続なdensity function \\(f_x\\) を持つ連続確率分布 \\(F\\) という分布について\n\\[\nX = \\{x_1, \\cdots, x_n\\}  \\overset{\\mathrm{iid}}{\\sim} F\n\\]\nと確率変数列が与えられたとします．この確率変数に対して\n\\[\nZ_i\\equiv 1\\{x_i \\leq x\\}\n\\]\nという変数を考えます．この \\(Z_i\\) は Bernoulli分布に従うと考えられるので，\\(F\\) のCDFを \\(F_X\\) とおくと\n\\[\n\\begin{align*}\n\\mathbb E(Z_i) &=  \\mathbb E\\left(I\\{X_i\\le x\\}\\right) = P(X_i\\le x)=F_X(x)\\\\\n\\operatorname{Var}(Z_i) &= F_X(x)[1-F_X(x)]\n\\end{align*}\n\\]\nここで，\\(Z_i\\) のsample meanを以下のように定義する．\n\\[\nY_n(x) =  \\frac 1n\\sum_{i=1}^nZ_i\n\\]\nこのように定義した \\(Y_n(x)\\) はいわゆる経験分布関数 \\(F_n(x)\\) であるとみなせます．また，定義より\n\\[\n\\begin{align*}\n&E[F_n(x)] = F_X(x)\\\\\n&\\operatorname{Var}(F_n(x)) = (1/n)F_X(x)[1-F_X(x)]\\\\\n&\\sqrt n\\Big(F_n(x) - F_X(x)\\Big) \\rightarrow_d \\mathbb N\\left(0,F_X(x)[1-F_X(x)]\\right) \\because{\\text{CLT}}\n\\end{align*}\n\\]\nここでCDFの逆関数 \\(F^{-1}_X\\) とする(monotonicityより自明)と delta methodを用いると\n\\[\n\\begin{align*}\n&\\frac {d}{dt}F^{-1}_X(t) = \\frac 1{f_x\\left(F^{-1}_X(t)\\right)}\\\\\n&\\sqrt n\\Big(F^{-1}_X(F_n(x)) - F^{-1}_X(F_X(x))\\Big) \\rightarrow_d \\mathbb N\\left(0,\\frac {F_X(x)[1-F_X(x)]}{\\left[f_x\\left(F^{-1}_X(F_X(x))\\right)\\right]^2} \\right)\n\\end{align*}\n\\]\nつまり，\n\\[\n\\sqrt n\\Big(F^{-1}_X(F_n(x)) - x\\Big) \\rightarrow_d \\mathbb N\\left(0,\\frac {F_X(x)[1-F_X(x)]}{\\left[f_x(x)\\right]^2} \\right)\n\\]\nここで \\(x = m\\)(population median)と設定すると\n\\[\n\\sqrt n\\Big(F^{-1}_X(F_n(m)) - m\\Big) \\rightarrow_d \\mathbb N\\left(0,\\frac {1}{\\left[2f_x(m)\\right]^2} \\right)\n\\]\nまた，\n\\[\nF^{-1}_X(\\hat F_n(m)) = \\inf\\{x : F_X(x) \\geq \\hat F_n(m)\\} = \\inf\\{x : F_X(x) \\geq \\frac 1n \\sum_{i=1}^n I\\{X_i\\leq m\\}\\}\n\\]\nより, 不等式のRHSは 1/2 に収束するので \\(F^{-1}_X(\\hat F_n(m))\\) はsample mean \\(\\hat m\\) に収束することがわかる．従って，\n\\[\n\\sqrt n\\Big(\\hat m - m\\Big) \\rightarrow_d \\mathbb N\\left(0,\\frac {1}{\\left[2f_x(m)\\right]^2} \\right)\n\\]\n\n\nTheorem 1.2 標本平均とMedianの距離 \n独立に同一の分布に従う確率変数列 \\(\\{X_i\\}_{i=1}^n\\) を考える．この標本平均を \\(\\overline{X}\\), 不偏分散を \\(S^2\\), メディアン \\(X_{\\operatorname{med}}\\) とすると\n\\[\n\\vert \\overline{X} - X_{\\operatorname{med}}\\vert &lt; S\n\\]\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(X_{\\operatorname{med}}\\) はL1ノルムの和を最小にするような値であるので\n\\[\n\\begin{align*}\n\\vert \\overline{X} - X_{\\operatorname{med}}\\vert\n    &= \\bigg\\vert \\frac{1}{n}\\sum_{i=1}^n(X_i - X_{\\operatorname{med}})\\bigg\\vert\\\\\n    &\\leq \\frac{1}{n}\\sum_{i=1}^n\\bigg\\vert X_i - X_{\\operatorname{med}}\\bigg\\vert\\\\\n    &\\leq\\frac{1}{n}\\sum_{i=1}^n\\bigg\\vert X_i - \\overline{X}\\bigg\\vert\\\\\n    &\\leq\\sqrt{\\frac{1}{n}\\sum_{i=1}^n( X_i - \\overline{X})^2}\\\\\n    &=\\sqrt{\\frac{n-1}{n}}S\\\\\n    &\\leq S\n\\end{align*}\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html#分散と標準偏差",
    "href": "posts/statistics101/averages.html#分散と標準偏差",
    "title": "1  代表値",
    "section": "分散と標準偏差",
    "text": "分散と標準偏差\n\nDef: Variance \nmean \\(\\mu\\) をもつ確率変数 \\(X\\) の分散は\n\\[\n\\begin{align*}\n\\operatorname{Var}(X) &= \\mathbb E[(X - \\mu)^2]\\\\\n                      &= \\mathbb E[X^2] - \\mathbb E[X]^2\n\\end{align*}\n\\]\n標準偏差(standard deviation)は分散のsquare rootで定義される．\n\n上の定義より標準偏差について以下のことがわかります：\n\n\\(X\\) と同じ単位で表される\n\\(X - \\mathbb E[X]\\) の L2ノルムと解釈できる\npopulation meanからどれだけ分布がバラついているか(dispersion)を示す指標の一つ\n\n\n\nTheorem 1.3 \n確率変数 \\(X\\sim D(\\mu, \\sigma^2)\\) とする．このとき\n\\[\n\\begin{align*}\n\\mathbb E[\\vert X - \\mu\\vert] \\leq \\sigma\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof: Jensen’s Inequality\n\n\n\n\n\n\\(g(x) = \\sqrt{x}\\) という関数はconcaveなので，Jensen’s Inequalityより\n\\[\n\\begin{align*}\n\\mathbb E[\\vert X - \\mu \\vert]\n    &= \\mathbb E[\\sqrt{( X - \\mu)^2}]\\\\\n    &\\leq \\sqrt{\\mathbb E[( X - \\mu)^2]}\\\\\n    &= \\sigma\n\\end{align*}\n\\]\n\n\n\n\n変動係数\n確率変数のバラツキを表す指標として範囲(Range), 四分位範囲（IQR）, 分散がありますが，分布の中心の位置が著しく異なるような場合 には，これらを用いて分布の散らばり具合を比較することは難しいです．このような場合に，変動係数(Coefficient of variation) という単位のない統計量を用いたりします．\n\nDef: Coefficient of variation \n比例尺度にもとづく測定が行われ，その測定結果をnon-negative 確率変数 \\(X_i &gt;0\\) で表すとする．\\(\\{X_i\\}_{i=1}^n\\) の標本平均を \\(\\overline{X}\\), 標本標準偏差を \\(S\\) としたとき，変動係数は以下のように計算される\n\\[\n\\operatorname{C_V} = \\frac{S}{\\overline{X}}\n\\]\n\n上記の定義より以下のことがわかります\n\n変動係数は，実際のゼロ点を持つ測定値（i.e., 比率尺度）に対してのみ用いることができます\n変動係数の計算対象となる測定は，non-negativeである必要がある\nサイズ \\(N\\) のfinite sampleにおける \\(\\operatorname{C_V}\\) のレンジは \\(\\operatorname{CV}\\in[0, \\sqrt{N-1}]\\) である\n\n\nExample 1.1 東京 日平均気温の月平均値（℃） \n国土交通省気象庁より以下のように東京都の気象データを取得します．\n\n\nCode\nimport polars as pl\nimport plotly.express as px\n\ndata = {\n    \"Year\": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023],\n    \"Jan\": [5.8, 6.1, 5.8, 4.7, 5.6, 7.1, 5.4, 4.9, 5.7],\n    \"Feb\": [5.7, 7.2, 6.9, 5.4, 7.2, 8.3, 8.5, 5.2, 7.3],\n    \"Mar\": [10.3, 10.1, 8.5, 11.5, 10.6, 10.7, 12.8, 10.9, 12.9],\n    \"Apr\": [14.5, 15.4, 14.7, 17.0, 13.6, 12.8, 15.1, 15.3, 16.3],\n    \"May\": [21.1, 20.2, 20.0, 19.8, 20.0, 19.5, 19.6, 18.8, 19.0],\n    \"Jun\": [22.1, 22.4, 22.0, 22.4, 21.8, 23.2, 22.7, 23.0, 23.2],\n    \"Jul\": [26.2, 25.4, 27.3, 28.3, 24.1, 24.3, 25.9, 27.4, 28.7],\n    \"Aug\": [26.7, 27.1, 26.4, 28.1, 28.4, 29.1, 27.4, 27.5, 29.2],\n    \"Sep\": [22.6, 24.4, 22.8, 22.9, 25.1, 24.2, 22.3, 24.4, 26.7],\n    \"Oct\": [18.4, 18.7, 16.8, 19.1, 19.4, 17.5, 18.2, 17.2, 18.9],\n    \"Nov\": [13.9, 11.4, 11.9, 14.0, 13.1, 14.0, 13.7, 14.5, 14.4],\n    \"Dec\": [9.3, 8.9, 6.6, 8.3, 8.5, 7.7, 7.9, 7.5, 9.4],\n}\n\ndf = pl.DataFrame(data)\ndf_unpivoted = df.unpivot(index=\"Year\", variable_name=\"Month\", value_name=\"Celsius\")\npx.line(df_unpivoted, x=\"Month\", y=\"Celsius\", color=\"Year\", title=\"東京都月別平均気温\")\n\n\n                                                \n\n\nここで，以下のようにFahrenheitに変換してみます．\n\n\nCode\ndf_unpivoted = df_unpivoted.with_columns(\n    (pl.col(\"Celsius\") * 9 / 5 + 32).alias(\"Fahrenheit\")\n)\ndf_unpivoted.head()\n\n\n\nshape: (5, 4)\n\n\n\nYear\nMonth\nCelsius\nFahrenheit\n\n\ni64\nstr\nf64\nf64\n\n\n\n\n2015\n\"Jan\"\n5.8\n42.44\n\n\n2016\n\"Jan\"\n6.1\n42.98\n\n\n2017\n\"Jan\"\n5.8\n42.44\n\n\n2018\n\"Jan\"\n4.7\n40.46\n\n\n2019\n\"Jan\"\n5.6\n42.08\n\n\n\n\n\n\nここで，Celsius, Fahrenheit両方のカラムについて変動係数を計算します．\n\n\nCode\ndef compute_cv(df, col: str) -&gt; float:\n    return df_unpivoted[col].std() / df_unpivoted[col].mean()\n\n\nprint(\n    \"\"\"Celsius CV: {:.2f}, Fahrenheit CV: {:.2f}\"\"\".format(\n        compute_cv(df_unpivoted, \"Celsius\"), compute_cv(df_unpivoted, \"Fahrenheit\")\n    ))\n\n\nCelsius CV: 0.45, Fahrenheit CV: 0.22\n\n\nこのように，Celsius, Fahrenheitともに同じ温度を単位の違う方法で表しているのにも関わらず変動係数は異なります．Celsius, Fahrenheitともに間隔尺度であること，及び変数変換の観点からもlocation/scale parameterを異なる値で調整しているので同じデータを扱っているにも関わらずCVが一致しないという現象が発生してしまいます．\n\n ▶  Bias Correction\nサイズ \\(N\\) のsampleベースで計算された変動係数はpopulation変動係数 \\(\\gamma_V\\) と比較して過小推定されているということが知られています．population変動係数のunbiased estimate \\(\\widehat{\\operatorname{C_V}}\\) は以下のように計算されます\n\\[\n\\widehat{\\operatorname{C_V}} = \\left(1 + \\frac{1}{4N}\\right)\\operatorname{C_V}\n\\]\n ▶  \\(\\sqrt{n}\\) の法則\n確率変数 \\(X_1, \\cdots, X_n\\) 独立に同一の分布に従うとし，これらの期待値と分散を \\(\\mu, \\sigma^2\\) とします．\nこのとき，\\(\\tilde X_n = \\sum^n_{i=1} X_i\\) とすると，\n\\[\n\\begin{align*}\n\\mathbb E[\\tilde X_n] &= n\\mu\\\\\n\\operatorname{Var}(\\tilde X_n) &= n\\sigma^2\\\\\n\\operatorname{std}(\\tilde X_n) &= \\sqrt{n}\\sigma\n\\end{align*}\n\\]\n期待値は \\(n\\) のオーダーで増える一方，標準偏差は \\(\\sqrt{n}\\) オーダーなので\n\\[\n\\begin{align*}\n\\operatorname{CV}(\\tilde X_n) = \\frac{\\sqrt{n}\\sigma}{n\\mu} = \\frac{\\sigma}{\\sqrt{n}\\mu}\n\\end{align*}\n\\]\nここから，平均と標準偏差の比率は \\(n\\) が増えるほど，小さくなる = データのバラツキが相対的に小さくなることがわかります．\nとある店AとBについて，それぞれの各月の来店者人数は各店舗が立地している地域の人口に比例すると解釈して， \\(X_A\\sim\\operatorname{Binom}(200, 0.5), Y_B\\sim\\operatorname{Binom}(1000, 0.5)\\) である場合を考えます．\nそれぞれの店舗について来月来店人数を予測したいとき，\\(p_A = p_B = 0.5, N_A= 200, N_B= 1000\\) であると判断し，その下で期待値をとって \\(100, 500\\) と予測したとします． このとき，予測値を基準に予測誤差 5% を上回りそうな確率はそれぞれ\n\\[\n\\begin{align*}\n1 - \\Pr(\\vert X_A - 100\\vert &lt; 5) &\\approx 0.481\\\\\n1 - \\Pr(\\vert X_B - 500\\vert &lt; 25) &\\approx 0.114\n\\end{align*}\n\\]\nと店舗 B のほうがMAPE的により良い予測精度が得られそうなことがわかります．母集団parameterもそれぞれ正しく判断できているにも関わらず, MAPEという指標では店舗 B のほうが相対的に良さそうな結果がでてしまうと推察できます．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/averages.html#references",
    "href": "posts/statistics101/averages.html#references",
    "title": "1  代表値",
    "section": "References",
    "text": "References\n\nYutaka Sasaki, The truth of the F-measure, 2007",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>代表値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/random_variable.html",
    "href": "posts/statistics101/random_variable.html",
    "title": "2  確率変数",
    "section": "",
    "text": "確率変数\n\\[\n\\{\\omega\\in\\Omega\\vert X(\\omega)\\leq x\\} \\subset \\Omega\n\\]\nが可測集合であるならば，つまり，\n\\[\n\\{\\omega\\in\\Omega\\vert X(\\omega)\\leq x\\} \\in \\mathcal{B}\n\\]\nであるとき，任意の実数 \\(x\\) に対して，\\(X\\leq x\\) である確率は\n\\[\n\\Pr(X\\leq x) = P(\\{\\omega\\in\\Omega\\vert X(\\omega)\\leq x\\})\n\\]\nとして確率 \\(P\\) を用いて与えることができます．なお，この \\(x\\) を実現値 といい，一般的には確率変数を大文字， 実現値を小文字で表します．実現値の全体を，\\(X\\) の標本空間といい，\n\\[\n\\mathcal{X} = \\{X(\\omega)\\vert\\omega\\in\\Omega\\}\n\\]\nと表されます．\n任意の \\(a\\in\\mathbb R\\) に対して, \\(x\\) を右から \\(a\\) に近づけると \\(F_X(x)\\) が \\(F_X(a)\\) に収束するとき， \\(F_X(x)\\) は右連続といいます．\\(F_X(x)\\) が右連続かつ左連続であるとき，\\(F_X(x)\\) は点 \\(a\\) で連続であるといいます．\nCode\nimport numpy as np\nfrom scipy.stats import binom\nimport plotly.graph_objs as go\nimport plotly.io as pio\n\nn, p = 5, 0.4\nx = np.arange(-1, 7)\nprob = binom.cdf(x, n, p)\n\n# Create a step plot with right-continuous steps\ntrace = go.Scatter(\n    x=x,\n    y=prob,\n    mode='lines+markers',\n    line_shape='hv',  # Horizontal then vertical step-wise\n    marker=dict(symbol='0'),\n    name='CDF is Right-Continuous',\n)\n\n# Layout for the plot\nlayout = go.Layout(\n    title='CDF is Right-Continuous',\n    xaxis=dict(title='X'),\n    yaxis=dict(title='累積確率')\n)\n\n# Create the figure\nfig = go.Figure(data=[trace], layout=layout)\n\n# Display the plot\npio.show(fig)\n上記は \\(X\\sim\\operatorname{Binom}(5, 0.4)\\) のCDFを描いたもので，右側連続で階段関数の形状になっています．一般的に，CDFの形状が階段関数のとき，\\(X\\) は離散型確率変数，\\(F_X(x)\\) が連続関数のとき \\(X\\) は連続型確率変数と分類します．\nQuantile関数は，累積分布関数の逆関数に相当する関数ですが，左連続という点で違いがあります． \\(X\\sim\\operatorname{Bernoulli}(p)\\) を考えたとき，\n\\[\n\\begin{align*}\nF_X(x) &= (1-p)\\mathbb 1(x\\geq 0) + p\\mathbb 1(x\\geq 1)\\\\\nQ_X(u) &= \\mathbb 1(u &gt; 1-p)\n\\end{align*}\n\\]\nと定義されます．\\(p=0.5\\) とすると，\\(F_X(1) = 1, Q_X(0.6) = 1\\) となります．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>確率変数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/random_variable.html#確率変数",
    "href": "posts/statistics101/random_variable.html#確率変数",
    "title": "2  確率変数",
    "section": "",
    "text": "Def: 確率変数 \n\\(\\Omega\\) を全事象，\\(\\mathcal{B}\\) を \\(\\Omega\\) の可測集合族，\\(P\\) を \\((\\Omega, \\mathcal{B})\\) 上の確率とするとき， \\(\\omega\\in\\Omega\\) に対して実数値 \\(X(\\omega) \\in \\mathbb{R}\\) を対応させる関数 \\(X\\) を確率変数という．\n\n\n\n\n\n\n\n\n\n\nDef: 累積分布関数 \n確率変数 \\(X\\) の累積分布関数を \\(F_X(x)\\) で表し，\n\\[\nF_X(x) = \\Pr(X\\leq x) \\quad x\\in \\mathbb{R}\n\\]\nで定義する．\n\n\n\nTheorem 2.1 \n関数 \\(F\\) がある確率変数の分布関数になるために必要十分条件は，３つの条件が成り立つことである\n\n\\(\\lim_{x\\to-\\infty}F(x) = 0, \\lim_{x\\to\\infty}F(x) = 1\\)\n\\(F(x)\\) は \\(x\\) の非減少関数である\n\\(F(x)\\) は右連続関数である\n\n\n\n\n\n\n\n\nTheorem 2.2 \n連続型確率変数 \\(X\\) について，その分布関数を \\(F_X(x)\\) とする．新たに \\(Y = F_X(x)\\) という確率変数を考えたとき，\n\\[\nY \\sim \\operatorname{Uniform}(0, 1)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(F_X: \\mathbb R\\to [0, 1]\\) なので, \\(\\operatorname{support}(Y) = [0, 1]\\). \\(Y\\) についての累積分布関数を \\(G(y)\\) とすると，\n\\[\n\\begin{align*}\nG(y) &= \\Pr(Y \\leq y)\n\\end{align*}\n\\]\n関数 \\(F_X(\\cdot)\\) は単調増加関数で，区間 \\(y\\in [0, 1]\\) で逆関数 \\(X = F^{-1}_X(Y)\\) が定義できるため\n\\[\n\\begin{align*}\nG(y) &= \\Pr(Y \\leq y)\\\\\n     &= \\Pr(F_X(X)\\leq y)\\\\\n     &= \\Pr(X\\leq F^{-1}_X(y))\\\\\n     &= F_X(F^{-1}_X(y))\n\\end{align*}\n\\]\nこのとき，両辺を \\(y\\) で微分する．\n\\[\n\\frac{\\mathrm{d}G(y)}{\\mathrm{d}y}=g(y)\n\\]\nとすると，\\(g(y)\\) はpdfに相当する．RHSについて，\n\\[\n\\begin{align*}\n\\frac{\\mathrm{d}F_X(F^{-1}_X(y))}{\\mathrm{d}y}\n    &= f_X(F^{-1}_X(y))[f_X(F^{-1}_X(y))]^{-1} = 1\n\\end{align*}\n\\]\n\\(y\\in [0, 1]\\) において \\(g(y) = 1\\) が成立することから，\n\\[\nY\\sim\\operatorname{Uniform}(0, 1)\n\\]\n\n\n\n\nDef: Quantile function（分位点関数） \n確率変数 \\(X\\) についての quantile function \\(Q_X: (0, 1)\\to \\mathbb R\\) は\n\\[\nQ_X(u) = \\inf\\{x\\in\\mathbb R: F_X(x) \\geq u\\}\n\\]\nと左連続で定義される．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>確率変数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/random_variable.html#連続確率変数",
    "href": "posts/statistics101/random_variable.html#連続確率変数",
    "title": "2  確率変数",
    "section": "連続確率変数",
    "text": "連続確率変数\n\nDef: 絶対連続型の確率変数 \n累積分布関数 \\(F\\) をもつ確率変数 \\(X\\) が次の条件を満たす確率密度関数 \\(f\\) を持つとき，絶対連続(absolutely continuous)という：\n\\[\n\\begin{gather}\nf(x) \\geq 0 \\quad \\forall x\\\\\nF(b) - F(a) = \\int^b_a f(x)\\mathrm{d}x \\quad \\text{where } a\\leq b\n\\end{gather}\n\\]\n\n \\(f\\) のnon-negativity性質は，累積分布関数はnon-decreasingであること，及び \\(F^\\prime(x) = f(x)\\) であることから分かる． また確率変数 \\(X\\) が確率密度関数 \\(f(x)\\) を持つとき，「\\(X\\) は \\(f(x)\\) に従う」とよく言われる．\n\n\nTheorem 2.3 変数変換と確率密度関数 \n\\(f\\) を確率密度関数，\\(a &gt; 0\\) とし，\n\\[\ng(x) = af(ax)\n\\]\nと関数 \\(g\\) を定義すると\n\\[\n\\int^\\infty_{-\\infty} g(x)\\mathrm{d} x = 1\n\\]\nとなる\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(a &gt; 0, f\\geq 0\\) より \\(g\\geq 0\\) は自明．また，\\(ax = z\\) と変数変換すると\n\\[\n\\begin{align*}\n\\int^\\infty_{-\\infty} g(x)\\mathrm{d} x &= \\int^\\infty_{-\\infty} af(ax)\\mathrm{d} x \\\\\n                                       &= \\int^\\infty_{-\\infty} af(z) \\frac{\\mathrm{d} x}{\\mathrm{d} z}\\mathrm{d} z\\\\\n                                       &= \\int^\\infty_{-\\infty} af(z) \\frac{1}{a}\\mathrm{d} z = 1\n\\end{align*}\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>確率変数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/expectation.html",
    "href": "posts/statistics101/expectation.html",
    "title": "3  期待値",
    "section": "",
    "text": "期待値の性質\n定義より確率密度関数で重みづけた平均が確率変数の期待値になると解釈することができます．meanは分布の位置を表すパラメーターとも解釈できるので location parameter（位置母数）と呼ぶこともあります．一方，標準偏差 \\(\\sigma\\) はscale parameter（尺度母数）といいます．\n▶  離散確率変数の変数変換と期待値\n離散型確率変数 \\(X\\) について \\(Y = g(X)\\) を考えたとき，\\(A_y = \\{x\\vert g(x)=y\\}\\) とおくと\n\\[\n\\begin{align*}\n\\mathbb E[g(X)]\n    &= \\sum_x p(x)g(x)\\\\\n    &= \\sum_y\\sum_{A_y} p(x)g(x)\\\\\n    &= \\sum_y\\sum_{A_y} p(x)y\\\\\n    &= \\sum_y y \\sum_{A_y} p(x)\\\\\n    &= \\sum_y \\Pr(Y=y)y\\\\\n    &= \\mathbb E[Y]\n\\end{align*}\n\\]\n上の式展開では, \\(\\Pr(X=x) = p(x)\\) としています．\n▶  期待値と重心\n期待値の解釈の１つとして 確率変数 \\(X\\) の重心と考えるパターンがあります．\\(x_i\\) の値を \\(p_i\\) の確率でとる離散型確率変数 \\(X\\) の場合，\nこのとき，重心 \\(\\mu\\) はモーメントの釣り合い = 右回りモーメントが0になる地点となりますが\n\\[\n\\begin{align*}\n\\text{右回りモーメント}\n    &= \\sum_i (x_i - \\mu)p_i\\\\\n    &= \\sum x_ip_i - \\sum_i p_i \\mu\\\\\n    &= \\mathbb E[X] - \\mu = 0\n\\end{align*}\n\\]\n従って，\\(\\mathbb E[X] = \\mu\\) より，期待値と重心が対応することがわかります．連続型確率変数でも確率密度関数を重さのある棒の断面積と みなすことで離散型と同じく期待値と重心が対応することを確かめることができます．\n\\(\\lim_{b\\to\\infty}\\bigg[x(1 - F(x))\\bigg]^b_0\\) について，\\(\\lim_{b\\to\\infty}b(1 - F(b))=0\\) とは限らない点に注意が必要です．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>期待値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/expectation.html#期待値の性質",
    "href": "posts/statistics101/expectation.html#期待値の性質",
    "title": "3  期待値",
    "section": "",
    "text": "Def: 連続確率変数の期待値 \n\\(f\\) を確率変数 \\(X\\) の確率密度関数とする．\\(\\int_{\\mathbb R} \\vert x\\vert f(x) \\mathrm{d}x &lt; \\infty\\) のとき，\\(X\\) の期待値は以下のように定義する:\n\\[\n\\mathbb E[X] = \\int_{\\mathbb R} x f(x) \\mathrm{d}x\n\\]\nまた，\\(X\\) の関数 \\(g(X)\\) の期待値は \\(\\int_{\\mathbb R} \\vert g(x)\\vert f(x) \\mathrm{d}x &lt; \\infty\\) ならば\n\\[\n\\mathbb E[g(X)] = \\int_{\\mathbb R} g(x) f(x) \\mathrm{d}x\n\\]\n\n\n\nExample 3.1 指数分布の期待値 \nrate parameter \\(\\lambda\\) の指数分布に従う確率変数 \\(X\\) を考えます．\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\int^\\infty_0 x \\lambda \\exp(-\\lambda x)\\mathrm{d}x\\\\\n             &= \\bigg[-x\\exp(-\\lambda x)\\bigg]^\\infty_0 + \\int^\\infty_0 \\exp(-\\lambda x)\\mathrm{d}x\\\\\n             &= \\int^\\infty_0 \\exp(-\\lambda x)\\mathrm{d}x\\\\\n             &= -\\frac{1}{\\lambda}\\bigg[\\exp(-\\lambda x)\\bigg]^\\infty_0\\\\\n             &= \\frac{1}{\\lambda}\n\\end{align*}\n\\]\n指数分布は電球の寿命などに応用される分布ですが，rate parameter \\(\\lambda\\) が小さいほど期待値（= 電球の寿命）が大きくなることが分かります．\n\n\nExample 3.2 期待値が定義できない離散分布 \n確率変数 \\(X\\) のsupportを加算集合 \\(\\{2, 2^2, 2^3, \\cdots\\}\\) とする．確率関数を\n\\[\n\\Pr(X = 2^i) = \\frac{1}{2^i} \\quad (i = 1, 2, \\cdots)\n\\]\nこのとき，\n\\[\n\\sum_{i=1}^\\infty \\Pr(X=2^i) = \\sum_{i=1}^\\infty\\frac{1}{2^i} = 1\n\\]\nと確率の公理を満たしていることが分かる．一方，\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_{i=1}^\\infty 2^i \\frac{1}{2^i}\\\\\n    &= \\sum_{i=1}^\\infty 1 = \\infty\n\\end{align*}\n\\]\n従って，確率変数 \\(X\\) の分布は，期待値が定義できない分布であることがわかる．\n\n\nExample 3.3 期待値が定義できない連続分布 \n確率密度関数 \\[\nf(x) = \\begin{cases}\n0 & x &lt; 1\\\\\n\\frac{1}{x^2} & x\\geq 1\n\\end{cases}\n\\]\nという確率変数 \\(X\\) を考える．\n\\[\n\\begin{align*}\n\\int_1^\\infty f(x) \\mathrm{d}x\n    &= \\left[\\frac{1}{x}\\right]^1_\\infty = 1\n\\end{align*}\n\\]\n一方，\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\int_1^\\infty xf(x) \\mathrm{d}x\\\\\n    &= \\int_1^\\infty\\frac{1}{x}\\mathrm{d}x\\\\\n    &= \\left[\\log(x)\\right]_1^\\infty = \\infty\n\\end{align*}\n\\]\n従って，確率変数 \\(X\\) の分布は，期待値が定義できない分布であることがわかる．\n\n\n\n\n\n\n\n\n重さのない棒の中央を原点とする\n原点から右側をプラス，左側をマイナスとして，原券からの距離 \\(x_i\\) の場所に重さ \\(p_i\\) のおもりを吊り下げる\n\n\n\n\n\n\nTheorem 3.1 Tail probabilities \n\\([0, b]\\) の定義域をもつ非負確率変数 \\(X\\) を考える．\\(F\\) を累積分布関数とするとき\n\\[\n\\mathbb E[X] = \\int_0^b (1 - F(x))\\mathrm{d}x\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\bigg[xF(x)\\bigg]^b_0 = \\int^b_0xf(x) \\mathrm{d}x + \\int^b_0F(x) \\mathrm{d}x\n\\]\nを用いると\n\\[\n\\begin{align*}\n\\mathbb E[X] &= b - \\int^b_0F(x) \\mathrm{d}x\\\\\n             &= \\int^b_0 1 \\mathrm{d}x - \\int^b_0F(x) \\mathrm{d}x\\\\\n             &= \\int_0^b (1 - F(x))\\mathrm{d}x\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 3.2 tail probabilities \n\\(\\mathbb E[\\vert X\\vert]&lt;\\infty\\) をもつ非負の確率変数 \\(X\\) について，\\(F(x)\\) を分布関数とすると\n\\[\n\\mathbb E[X] = \\int_0^\\infty (1 - F(x))\\mathrm{d}x\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\int_0^\\infty (1 - F(x))\\mathrm{d}x\n    &= \\int_0^\\infty \\Pr(X\\geq x)\\mathrm{d}x\\\\\n    &= \\int_0^\\infty \\mathbb E[\\mathbb 1(X\\geq x)]\\mathrm{d}x\\\\\n    &= \\int_0^\\infty \\int_0^\\infty \\mathbb 1(X\\geq x) \\mathrm{d}F(x)\\mathrm{d}x\\\\\n    &= \\int_0^\\infty \\left\\{\\int_0^\\infty \\mathbb 1(X\\geq x) \\mathrm{d}x\\right\\}\\mathrm{d}F(x) \\quad\\because\\text{積分の順序交換}\\\\\n    &= \\mathbb E\\left[\\int_0^\\infty \\mathbb 1(X\\geq x) \\mathrm{d}x\\right]\\\\\n    &= \\mathbb E\\left[\\int_0^X 1 \\mathrm{d}x\\right]\\\\\n    &= \\mathbb E[X]\n\\end{align*}\n\\]\n\n\n\n\n\n\nTheorem 3.3 \n\\([0, \\infty)\\) の定義域をもつ非負確率変数 \\(X\\) を考える．\\(\\mathbb E[\\vert X^{p+1} \\vert] &lt;\\infty\\) が定義可能及び， \\(F\\) を累積分布関数とするとき\n\\[\n\\mathbb E[X^p] = \\int_0^\\infty px^{p-1} (1 - F(x))\\mathrm{d}x \\quad \\text{where } p &gt; 0\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\bigg[x^p(1 - F(x))\\bigg]^\\infty_0 = \\int^\\infty_0 p x^{p-1}(1 -F(x))\\mathrm{d}x - \\int^\\infty_0 x^{p}f(x)\\mathrm{d}x\n\\end{align*}\n\\]\n\\(\\text{RHS} = 0\\) であるので\n\\[\n\\mathbb E[X^p] = \\int_0^\\infty px^{p-1} (1 - F(x))\\mathrm{d}x\n\\]\n\n\n\n\nExample 3.4 \n同様の考えで定義域を \\(0,1,2,3,\\cdots\\) とする離散確率変数 \\(X\\) について\n\\[\n\\mathbb E[X] = \\sum_{k=0}^\\infty \\Pr(X &gt; k)\n\\]\nが成立します．\n\\[\n\\begin{align*}\n\\Pr(X &gt; k) &= \\Pr(X = k+1) + \\Pr(X = k+2) + \\cdots\\\\\n           &= \\sum_{l=k+1}^\\infty \\Pr(X=l)\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\sum_{k=0}^\\infty \\Pr(X &gt; k) &= \\sum_{k=0}^\\infty \\sum_{l=k+1}^\\infty \\Pr(X=l)\\\\\n                             &= \\sum_{l=1}^\\infty\\sum_{k=0}^{l-1}\\Pr(X=l) \\quad\\because \\Pr(X=l) &gt; 0 \\\\\n                             &= \\sum_{l=1}^\\infty l\\Pr(X=l)\\\\\n                             &= \\sum_{l=0}^\\infty l\\Pr(X=l)\\\\\n                             &= \\mathbb E[X]\n\\end{align*}\n\\] \\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\nExample 3.5 \n\\(0,1,2,3,\\cdots\\) とする離散確率変数 \\(X\\) について\n\\[\n\\mathbb E[X^2] = \\sum_{k=0}^\\infty \\Pr(X &gt; k)(2k+1)\n\\]\nも成立する．\n\\[\n\\begin{align*}\n\\sum_{k=0}^\\infty \\Pr(X &gt; k)(2k+1)\n    &= \\sum_{k=0}^\\infty \\sum_{l=k+1}^\\infty \\Pr(X=l)(2k+1)\\\\\n    &= \\sum_{l=1}^\\infty \\sum_{k=0}^{l-1}\\Pr(X=l)(2k+1)\\\\\n    &= \\sum_{l=1}^\\infty \\Pr(X=l)\\sum_{k=0}^{l-1}(2k+1)\\\\\n    &= \\sum_{l=1}^\\infty l^2\\Pr(X=l)\\\\\n    &= \\mathbb E[X^2]\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\n\nTheorem 3.4 期待値の線型性 \n\\(a, b\\) を実数，確率変数 \\(X, Y\\) について以下が成り立つ\n\\[\n\\mathbb E[aX + bY] = a\\mathbb E[X] + b\\mathbb E[Y]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n確率変数 \\(X, Y\\) が有限加算な標本空間で定義されているケースにて以下を示す．\n\n\\(\\mathbb E[X + Y] = \\mathbb E[X] + \\mathbb E[Y]\\)\n\\(\\mathbb E[cX] = c\\mathbb E[X]\\)\n\n ▶  1. \\(\\mathbb E[X + Y] = \\mathbb E[X] + \\mathbb E[Y]\\)\n確率変数 \\(X\\) は \\(\\{x_1, \\cdots, x_m\\}\\), 確率変数 \\(Y\\) は \\(\\{y_1, \\cdots, y_n\\}\\) の値をそれぞれ取りうるとする． このとき，\\(Z = X + Y\\) の標本空間 \\(\\{z_1, \\cdots, z_k\\}\\) について \\(k\\leq m + n\\) が成り立つ．\n\\(A_l = \\{(i,j): x_i + y_j = z_l\\}\\) としたとき，\n\\[\n\\begin{align*}\n\\mathbb E[X+Y]\n    &= \\sum_{l=1}^kz_l\\Pr(A_l)\\\\\n    &= \\sum_{l=1}^k\\sum_{(i,j)\\in Z_l}(x_i + y_j)\\Pr(x_i, y_j)\\\\\n    &= \\sum_{i=1}^m\\sum_{j=1}^n(x_i + y_j)\\Pr(x_i, y_j)\\\\\n    &= \\sum_{i=1}^m\\sum_{j=1}^nx_i\\Pr(x_i, y_j) + y_j\\Pr(x_i, y_j)\\\\\n    &= \\sum_{i=1}^m\\sum_{j=1}^n[x_i\\Pr(x_i, y_j) + y_j\\Pr(x_i, y_j)]\\\\\n    &= \\sum_{i=1}^mx_i\\sum_{j=1}^nPr(x_i, y_j) + \\sum_{j=1}^ny_j\\sum_{i=1}^m\\Pr(x_i, y_j)\\\\\n    &=\\sum_{i=1}^mx_i \\Pr(x_i) + \\sum_{j=1}^ny_j \\Pr(y_j)\\\\\n    &= \\mathbb E[X] + \\mathbb E[Y]\n\\end{align*}\n\\]\n ▶  2. \\(\\mathbb E[cX] = c\\mathbb E[X]\\)\n\\[\n\\begin{align*}\n\\mathbb E[cX]\n    &= \\sum_{i=1}^m cx_i = \\Pr(cX = cx_i)\\\\\n    &= c\\sum_{i=1}^m x_i = \\Pr(X = x_i)\\\\\n    &= c\\mathbb E[X]\n\\end{align*}\n\\]\n\n\n\n\nExample 3.6 : 変数変換と分散 \nmean \\(\\mu\\) をもつ確率変数 \\(X\\) と実数 \\(a, b\\) について\n\\[\n\\operatorname{Var}(aX + b) = a^2\\operatorname{Var}(X)\n\\]\nが成立します．証明は以下，\n\\[\n\\begin{align*}\n\\operatorname{Var}(aX + b)\n    &= \\mathbb E[(aX + b) - (a\\mu +b)^2]\\\\\n    &= \\mathbb E[a^2(X - \\mu)^2]\\\\\n    &= a^2 \\mathbb E[(X - \\mu)^2]\\\\\n    &= a^2\\operatorname{Var}(X)\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\n\nTheorem 3.5 positive operator \n確率変数 \\(X, Y\\) について，\\(X\\geq Y\\) が成り立つとき，\n\\[\n\\mathbb E[X] \\geq \\mathbb E[Y]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(X\\geq Y\\) より \\(X - Y \\geq 0\\). 期待値はpositive operatorなので\n\\[\n\\mathbb E[X - Y] \\geq 0\n\\]\n従って，期待値の線型性を用いると\n\\[\n\\begin{align*}\n\\mathbb E[X - Y] &= \\mathbb E[X] - \\mathbb E[Y] \\geq 0\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 3.6 \n確率変数 \\(X\\) について,\n\\[\n\\mathbb E[\\vert X \\vert] \\geq \\vert \\mathbb E[X] \\vert\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\vert X\\vert \\geq X\\) より\n\\[\n\\mathbb E[\\vert X \\vert] \\geq \\mathbb E[X]\n\\]\nまた, \\(\\vert X\\vert + X \\geq 0\\) より，\\(\\mathbb E[\\vert X\\vert + X] \\geq 0\\)， つまり，\n\\[\n\\mathbb E[\\vert X \\vert] \\geq -\\mathbb E[X]\n\\]\n以上より，\\(\\mathbb E[\\vert X \\vert] \\geq \\vert \\mathbb E[X] \\vert\\)\n\n\n\n\n\nTheorem 3.7 互いに独立な確率変数の積の期待値 \n\\(\\mathbb E[\\vert X\\vert ]&lt;\\infty, \\mathbb E[\\vert Y\\vert ]&lt;\\infty\\) を満たす, 確率空間 \\((\\Omega, \\mathscr{F},P)\\) 上で定義された確率変数 \\(X, Y\\) を考える． \\(X \\perp Y\\) であるとき，次が成立する\n\\[\n\\mathbb E[XY] = \\mathbb E[X]\\mathbb E[Y]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[XY] &= \\int\\int_\\Omega xy f(x, y)\\mathrm{d}x\\mathrm{d}y\\\\\n              &= \\int\\int_\\Omega xy f_X(x)f_Y(y)\\mathrm{d}x\\mathrm{d}y \\quad\\because{\\text{independence}}\\\\\n              &= \\left(\\int xf_X(x)\\mathrm{d}x\\right)\\left(\\int yf_Y(y)\\mathrm{d}y\\right)\\\\\n              &= \\mathbb E[X]\\mathbb E[Y]\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 3.8 Schwarz inquality \n確率変数 \\(X, Y\\) についてシュワルツの不等式が成立することを示せ\n\\[\n\\left(\\mathbb E[XY]\\right)^2 \\leq \\mathbb E[X^2]\\mathbb E[Y^2]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nQuadratic functionを以下のように定義します\n\\[\n\\begin{align*}\ng(t)\n    &= \\mathbb E[(tX - Y)^2]\\\\\n    &= t^2\\mathbb E[X^2] - 2t\\mathbb E[XY] + E[Y^2]\\\\\n    &\\geq 0\n\\end{align*}\n\\]\nこのとき，\\(g(t)\\) はnon-negativeなので判別式について以下が成立する\n\\[\nD/4 = \\left(\\mathbb E[XY]\\right)^2 - \\mathbb E[X^2]\\mathbb E[Y^2]\\leq 0\n\\]\n従って，\\(\\left(\\mathbb E[XY]\\right)^2 \\leq \\mathbb E[X^2]\\mathbb E[Y^2]\\)\n\n\n\n\n\nTheorem 3.9 : Triangle inequality \n確率変数 \\(X, Y\\) について，以下のような三角不等式が成立することを示せ\n\\[\n\\sqrt{\\mathbb E[(X+Y)^2]} \\leq \\sqrt{\\mathbb E[X^2]} + \\sqrt{\\mathbb E[Y^2]}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nシュワルツの不等式を用いて以下のように示せる\n\\[\n\\begin{align*}\n\\mathbb E[(X+Y)^2]\n    &= \\mathbb E[X^2] + 2\\mathbb E[XY] + \\mathbb E[Y^2]\\\\\n    &= \\mathbb E[X^2] + 2\\sqrt{(\\mathbb E[XY])^2} + \\mathbb E[Y^2]\\\\\n    &\\leq \\mathbb E[X^2] + 2\\sqrt{\\mathbb E[X^2]\\mathbb E[Y^2]} + \\mathbb E[Y^2]\\\\\n    &= (\\sqrt{\\mathbb E[X^2]} + \\sqrt{\\mathbb E[Y^2]})^2\n\\end{align*}\n\\]\n両辺について，square rootをとると，\n\\[\n\\sqrt{\\mathbb E[(X+Y)^2]} \\leq \\sqrt{\\mathbb E[X^2]} + \\sqrt{\\mathbb E[Y^2]}\n\\]\n\n\n\n\n条件付き期待値\n\n\nTheorem 3.10 Law of Total Expectation \n\\[\n\\mathbb E[Y] = \\mathbb E[\\mathbb E[Y\\vert X]]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[\\mathbb E[Y\\vert X]]\n    &= \\int \\mathbb E[Y\\vert X=u]f_X(u)\\mathrm{d}u\\\\\n    &= \\int \\left[\\int t f_Y(t\\vert x=u)\\mathrm{d}t\\right]f_X(u)\\mathrm{d}u\\\\\n    &= \\int \\int t f_Y(t\\vert x=u)f_X(u)\\mathrm{d}u\\mathrm{d}t\\\\\n    &= \\int t\\left[\\int f_{X,Y}(u, t)\\mathrm{d}u\\right]\\mathrm{d}t\\\\\n    &= \\int t f_Y(t)\\mathrm{d}t\\\\\n    &= \\mathbb E[Y]\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 3.11 : CEF Decomposition Property \n確率変数 \\(X, Y\\) について，\n\\[\nY = \\mathbb E[Y\\vert X] + \\epsilon\n\\]\nとしたとき，\n\n\\(\\epsilon\\) は \\(X\\) について mean-independent, i.e., \\(\\mathbb E[\\epsilon\\vert X] = 0\\)\n\\(\\epsilon\\) は \\(X\\) の任意の関数に対して無相関, i.e., \\(\\operatorname{Cov}(h(X), \\epsilon) = 0\\)\n\\(\\operatorname{Var}(\\epsilon) = \\mathbb E[\\operatorname{Var}(Y\\vert X)]\\)\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n ▶  (1)\n\\[\n\\begin{align*}\n\\mathbb E[\\epsilon\\vert X]\n    &= \\mathbb E[Y - \\mathbb E[Y\\vert X]\\vert X]\\\\\n    &= \\mathbb E[Y\\vert X] - \\mathbb E[Y\\vert X]\\\\\n    &= 0\n\\end{align*}\n\\]\n ▶  (2)\n\\[\n\\begin{align*}\n\\mathbb E[h(X)\\epsilon] &= E[\\mathbb E[h(X)\\epsilon\\vert X]]\\\\\n                        &= E[h(X)\\mathbb E[\\epsilon\\vert X]]\\\\\n                        &= 0 \\quad \\because{\\text{mean independence}}\n\\end{align*}\n\\]\n ▶  (3)\n条件付き期待値の公式 \\(\\mathbb E[g(X, Y)] = \\mathbb E_X[\\mathbb E_{Y\\vert X}[g(X, Y)\\vert X]]\\) より\n\\[\n\\begin{align*}\n\\operatorname{Var}(\\epsilon)\n    &= \\mathbb E[(Y - \\mathbb E[Y\\vert X] )^2]\\\\\n    &= \\mathbb E\\{\\mathbb E[(Y - \\mathbb E[Y\\vert X])^2 \\vert X]\\}\\\\\n    &= \\mathbb E[\\operatorname{Var}(Y\\vert X)]\n\\end{align*}\n\\]\n\n\n\n\n📘 REMARKS \nCEF Decomposition Propertyは，確率変数 \\(Y\\) は確率変数 \\(X\\) で説明できるパートと，\\(X\\) の任意の関数と直行（orthogonal） な誤差項のパートに分解できることを示しています．\n\n\n\nMoments\n\nDef: モーメント \n\\(0 &lt; r &lt; \\infty\\) を満たすような正の整数 \\(r\\) に対して，確率変数 \\(X\\) のthe r-th moment（原点周りのモーメント）は以下のように定義される\n\\[\n\\mathbb E[X^r] = \\int_{\\mathbb R} x^r \\mathrm{d}F_X(x)\n\\]\nthe r-th central moment(平均周りのモーメント)は \\(\\mathbb E[(X - \\mathbb E[X])^r]\\) と定義される．\n\n原点周りのモーメントと平均周りのモーメントは以下のような関係で理解することができます．\\(\\mathbb E[X] = \\mu\\) とすると，\n\\[\n\\begin{align*}\n(X - \\mu + \\mu)^r\n    &= \\sum_{k=0}^k \\left(\\begin{array}{c}r\\\\ k\\end{array}\\right)(X - \\mu)^k\\mu^{r-k}\n\\end{align*}\n\\]\n両辺の期待値をとると\n\\[\n\\begin{align*}\n\\mathbb E[X^r] = \\sum_{k=0}^k \\left(\\begin{array}{c}r\\\\ k\\end{array}\\right)\\mathbb E[(X - \\mu)^k]\\mu^{r-k}\n\\end{align*}\n\\]\nまた，平均周りのモーメントを原点周りのモーメントで表すとなると，\n\\[\n\\begin{align*}\n(X - \\mu)^r = \\sum_{k=0}^r(-1)^{r-k}X^r\\mu^{r-k}\n\\end{align*}\n\\]\n同様に期待値をとると\n\\[\n\\begin{align*}\n\\mathbb E[(X - \\mu)^r] = \\sum_{k=0}^r(-1)^{r-k}\\mathbb E[X^r]\\mu^{r-k}\n\\end{align*}\n\\]\n\n\nTheorem 3.12 : higher moment and lower moment \n\\(\\mathbb E[\\vert X\\vert^r] &lt; \\infty\\) のとき，\\(0 &lt; q &lt; r\\) について，\\(\\mathbb E[\\vert X\\vert^q] &lt; \\infty\\) が成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n関数 \\(g:\\mathbb R\\to \\mathbb R\\) を\n\\[\ng(x) = \\vert x\\vert^r + 1\n\\]\n関数 \\(g:\\mathbb h\\to \\mathbb R\\) を \\(h(x) = \\vert x\\vert^q\\) と定義する．\\(0 &lt; q &lt; r\\) より，\n\\[\nh(x) &lt; g(x) \\quad \\forall x \\in \\operatorname{support}(X)\n\\]\nこのとき，\n\\[\n\\begin{align*}\n\\int_{\\mathbb{R}} |x|^r + 1 \\mathrm{d}F\n    &= \\int_{\\mathbb{R}} |x|^r\\mathrm{d}F + \\underbrace{\\int_{\\mathbb{R}} 1\\mathrm{d}F}_{=1}\\\\\n    &&gt; \\int_{\\mathbb{R}} |x|^s\\mathrm{d}F\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\infty &&gt; \\mathbb E[\\vert X\\vert^r] + 1\\\\\n       &&gt; \\int_{\\mathbb{R}} |x|^q \\mathrm{d}F \\\\\n       &= \\mathbb E[\\vert X\\vert^q]\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 3.13 : Mean minimizes squared error \n確率変数 \\(X\\) について，\\(\\mathbb E[\\vert X\\vert^2]&lt;\\infty\\) とする．このとき，\n\\[\n\\mathbb E[X] = \\arg\\min_b \\mathbb E[(X - b)^2]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\mu_X = \\mathbb E[X], \\sigma^2 = \\mathbb E[(X - \\mu_X)^2]\\) とおく．\n\\[\n\\begin{align*}\nE[(X - b)^2]\n    &= \\mathbb E[(X - \\mu_X + \\mu_X - b)^2]\\\\\\\n    &= \\mathbb E[(X - \\mu_X)^2] + \\mathbb E[(\\mu_X - b)^2] + \\mathbb E[(X - \\mu_X)(\\mu_x - b)]\\\\\n    &= \\sigma^2 + \\mathbb E[(\\mu_X - b)^2] + (\\mu_X - b)\\mathbb E[X - \\mu_X]\\\\\n    &= \\sigma^2 + \\mathbb E[(\\mu_X - b)^2]\n\\end{align*}\n\\]\n\\(\\mathbb E[(\\mu_X - b)^2] \\geq 0\\) が成立し，また, 等号成立条件は \\(\\mu_X = b\\)．従って，\n\\[\n\\mathbb E[X] = \\arg\\min_b \\mathbb E[(X - b)^2]\n\\]\n\n\n\n\n\nMarkov and Chebyshev Inequalities\n確率変数 \\(X\\) について，確率密度関数や分布関数がわかっている状況は少ないです．また，データが得られたとしても それらを計算することはかんたんではありません．その中で，\n\n\\(X\\) が mean \\(\\mu\\) からどれくらい離れる可能性があるのか\n\\(\\Pr(\\vert X \\leq a\\vert )\\) のupper boundはどれくらいか？\n\nという統計的推測をしたいときに使用されるMarkov and Chebyshev Inequalitiesを解説します．\n\n\nTheorem 3.14 Markov’s Inequality \nnon-negative 確率変数 \\(X \\geq 0\\)，constant \\(k &gt;0\\) について以下が成立する\n\\[\n\\Pr(X \\geq k) \\leq \\frac{\\mathbb E[X]}{k}\n\\]\nつまり，\n\\[\n\\Pr(X \\geq k\\mathbb E[X]) \\leq \\frac{1}{k}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\int_0^\\infty xf(x)\\mathrm{d}x\\\\\n             &= \\int_0^k xf(x)\\mathrm{d}x + \\int_k^\\infty xf(x)\\mathrm{d}x\\\\\n             &\\geq \\int_k^\\infty xf(x)\\mathrm{d}x\\\\\n             &\\geq \\int_k^\\infty kf(x)\\mathrm{d}x\\\\\n             &= k \\Pr(X \\geq k)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nProof: 変数変換\n\n\n\n\n\n\\[\nY =\n\\begin{cases}\n    0 & \\text{if} X &lt; k\\\\[5pt]\n    k & \\text{if} X \\geq k\n\\end{cases}\n\\]\nのように変数変換をすると常に \\(Y \\leq X\\) であるので \\(\\mathbb E[Y] \\leq \\mathbb E[X]\\).\n\\[\n\\begin{align*}\n&\\mathbb E[Y] = k\\Pr(X\\geq k)\\\\\n\\Rightarrow &\\Pr(X\\geq k)\\leq \\frac{\\mathbb E[X]}{k}\n\\end{align*}\n\\]\n\n\n\n\n📘 REMARKS \n\nMarkov’s inequalityは 確率変数 \\(X\\) がnon-negative, population mean \\(\\mu\\) の知識のみで使用可能\n一方，bound幅は大きく，weakest inequalityである\n\n\n\nExample 3.7 \n点数範囲が \\(\\Omega_x=[0, 110]\\) の試験をついて，そのテストスコア確率変数 \\(X\\) を考える．分布の情報はわからないが population meanは 25 であることが知られている．このとき，\\(\\Pr(X \\geq 100)\\) のupper boundはMarkov’s inequalityを用いて 以下のように計算できます．\n\\(X\\) がnon-negativeなので\n\\[\n\\begin{align*}\n\\Pr(X\\geq 100) &\\leq \\frac{25}{100}\\\\\n               &= \\frac{1}{4}\n\\end{align*}\n\\]\n\n\nExample 3.8 : weak inequality \n\\(X_i \\overset{\\mathrm{iid}}{\\sim} \\operatorname{Bernoulli}(0.2)\\) を20回繰り返す試行を考える．この試行の結果のアウトカムを \\(Y\\) としたとき，\n\\[\n\\Pr(Y \\geq 16) = \\sum_{k=16}^{20} {}_{20}C_{k} 0.2^k 0.8^{20-k} \\approx 1.38\\cdot 10^{-8}\n\\]\n一方，Markov’s inequalityを用いると\n\\[\n\\begin{align*}\n\\Pr(Y \\geq 16) \\leq \\frac{4}{16} = \\frac{1}{4}\n\\end{align*}\n\\]\nこのように，bound幅は大きいことが分かる．\n\n\n\nTheorem 3.15 Chebyshev’s inequality \n\\(X \\sim D(\\mu, \\sigma^2)\\) とする．ただし，\\(D\\) の形状はわからない．実数 \\(\\alpha &gt;0\\) について，以下が成立する\n\\[\n\\Pr(\\vert X - \\mu \\vert \\geq \\alpha) \\leq \\frac{\\sigma^2}{\\alpha^2}\n\\]\nつまり，\n\\[\n\\Pr(\\vert X - \\mu \\vert \\geq \\alpha \\sigma) \\leq \\frac{1}{\\alpha^2}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(I = \\{x: \\vert x -\\mu \\vert \\geq k\\}\\) とする．\n\\[\n\\begin{align*}\n\\sigma^2 &= \\int_{\\mathbb R} (x - \\mu)^2f(x)\\mathrm{d}x\\\\\n         &\\geq  \\int_{I} (x - \\mu)^2f(x)\\mathrm{d}x\\\\\n         &\\geq  \\int_{I} k^2f(x)\\mathrm{d}x\\\\\n         &= k^2 \\Pr(\\vert x - \\mu\\vert \\geq k)\n\\end{align*}\n\\]\n以上より，\\(\\displaystyle\\Pr(\\vert X - \\mu \\vert \\geq k) \\leq \\frac{\\sigma^2}{k^2}\\) を得る．\n\n\n\n\n\n\n\n\n\nProof: using Markov’s inequality\n\n\n\n\n\n\\((x - \\mu)^2\\) を確率変数と考えると，non-negative確率変数になる，つまりMarkov’s inequalityを用いることができるので\n\\[\n\\begin{align*}\n\\Pr(\\vert x - \\mu\\vert \\geq k) &= \\Pr((x - \\mu)^2 \\geq k^2)\\\\\n                               &\\leq \\frac{\\mathbb E[(x - \\mu)^2]}{k^2} \\because{\\text{Markov's inequality}}\\\\\n                               &= \\frac{\\sigma^2}{k^2}\n\\end{align*}\n\\]\n\n\n\n\nExample 3.9 Markov’s inequality vs Chebyshev’s inequality \n\\(X \\sim \\operatorname{Binom}(n=20, p=0.2)\\) について，weak inequality で確認したように，Markov’s inequalityのより\n\\[\n\\Pr(X \\geq 16) = \\Pr(X \\geq 4\\mathbb E[X]) \\leq \\frac{1}{4}\n\\]\n一方，Chebyshev’s inequalityを用いると\n\\[\n\\begin{align*}\n\\Pr(X \\geq 16) &\\leq \\Pr(\\vert X - 4\\vert \\geq 12)\\\\\n               &\\leq \\frac{\\operatorname{Var}(X)}{12^2}\\\\\n               &\\leq \\frac{3.2}{12^2}\\\\\n               &= \\frac{1}{45}\n\\end{align*}\n\\]\n\n\n📘 REMARKS \n\nChebyshev’s inequalityはMarkov’s inqualityと異なり，確率変数 \\(X\\) がnon-negativeである必要はない\nmeanからの距離についての情報を得ることができる\n\n\n\n\nWeak Law of Large Numbers\n\n\nTheorem 3.16 Weak Law of Large Numbers \n平均 \\(\\mu\\), 分散 \\(\\sigma^2\\) の分布に独立に従う確率変数 \\(X_1, \\cdots, X_n\\) を考える．標本平均を \\(\\overline{X_n} = \\frac{1}{n}\\sum_{i=1}^nX_i\\) とする．\nこのとき，任意の実数 \\(\\epsilon &gt;0\\) に対して，\n\\[\n\\lim_{n\\to\\infty}\\Pr(\\vert \\overline{X_n} - \\mu \\vert &gt; \\epsilon) = 0\n\\]\nつまり，標本平均は母平均に確率収束する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nChebyshev’s inequalityを用いて以下のように示せる\n\\[\n\\begin{align*}\n\\lim_{n\\to\\infty}\\Pr(\\vert \\overline{X_n} - \\mu \\vert &gt; \\epsilon)\n                &\\leq \\lim_{n\\to\\infty} \\frac{\\operatorname{Var}(\\overline{X_n})}{\\epsilon^2}\\\\\n                &= \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n\\epsilon^2}\\\\\n                &=0\n\\end{align*}\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>期待値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/expectation.html#分散",
    "href": "posts/statistics101/expectation.html#分散",
    "title": "3  期待値",
    "section": "分散",
    "text": "分散\n\n\nTheorem 3.17 : Bienaymé Equality \n互いに独立な確率変数 \\(X, Y\\) について以下が成立する\n\\[\n\\operatorname{Var}(X+Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\operatorname{Var}(X+Y)\n    &= \\mathbb E[((X+Y) - (\\mu_X+\\mu_Y))^2]\\\\\n    &= \\mathbb E[((X- \\mu_X)+(Y - \\mu_Y))^2]\\\\\n    &= \\mathbb E[(X- \\mu_X)^2] + 2\\mathbb E[(X- \\mu_X)(Y- \\mu_Y)] + \\mathbb E[(Y- \\mu_Y)^2]\\\\\n    &= \\mathbb E[(X- \\mu_X)^2] + 2\\mathbb E[(X- \\mu_X)]\\mathbb E[(Y- \\mu_Y)] + \\mathbb E[(Y- \\mu_Y)^2] \\quad \\because{\\text{独立性}}\\\\\n    &= \\operatorname{Var}(X) + \\operatorname{Var}(Y)\n\\end{align*}\n\\]\n\n\n\nなお，確率変数 \\(X, Y\\) が独立ではない場合は\n\\[\n\\operatorname{Var}(X + Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) + 2\\operatorname{Cov}(X, Y)\n\\]\nが成立します．\n\n条件付き分散\n確率変数 \\(X, Y\\) についての条件付き分散は以下のような意味を持つ\n\n\\(\\operatorname{Var}(X\\vert Y=y)\\) は，\\(Y = y\\) と固定したときの \\(X\\) の分散\n\\(\\operatorname{Var}(X\\vert Y)\\) は，\\(Y\\) がランダムに選ばれた値に固定された場合の \\(X\\) の分散\n\n\\(\\operatorname{Var}(X\\vert Y)\\) は \\(Y\\) のランダムネスに依存した確率変数である一方， \\(\\operatorname{Var}(X\\vert Y=y)\\) は \\(y\\) の関数という違いがある\n\n\nTheorem 3.18 条件付き分散 \n\\[\n\\operatorname{Var}(Y\\vert X) = \\mathbb E[(Y^2\\vert X)] - (\\mathbb E[(Y\\vert X)])^2 = \\mathbb E[(Y - \\mathbb E[Y\\vert X])^2\\vert X]\n\\]\n\n\n\n\n\nTheorem 3.19 Law of Total Variance \n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\mathbb E[Y\\vert X]) + \\mathbb E_X[\\operatorname{Var}(Y\\vert X)]\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\epsilon = Y - E\\mathbb E[Y\\vert X]\\) としたとき，\\(\\epsilon\\) と \\(E\\mathbb E[Y\\vert X]\\) は無相関なので，\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\mathbb E[Y\\vert X]) + \\operatorname{Var}(\\epsilon)\n\\]\n\\(\\mathbb E[\\epsilon] = 0\\) より，\n\\[\n\\begin{align*}\n\\operatorname{Var}(\\epsilon)\n    &= \\mathbb E[\\epsilon^2] - (\\mathbb E[\\epsilon])^2\\\\\n    &= \\mathbb E[\\epsilon^2]\\\\\n    &= \\mathbb E_X(\\mathbb E[\\epsilon^2\\vert X])\\\\\n    &= \\mathbb E_X[\\operatorname{Var}(Y\\vert X)]\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\mathbb E[Y\\vert X]) + \\mathbb E_X[\\operatorname{Var}(Y\\vert X)]\n\\]\n\n\n\nLaw of Total Varianceより \\(Y\\) の分散は，CEFの分散 + 誤差項の分散に分解できることを示しています． 実務における分析において，賃金のバラツキを\n\n賃金を説明する各個人の特徴のバラツキ\n特徴で説明することのできない賃金のバラツキ(=誤差項)の期待値\n\nに分解して考察する際にLaw of Total Varianceを使用したりします．\n\n\n歪度と尖度\n\\(X_1\\sim N(1, 1)\\) と \\(X_2\\sim\\operatorname{Exponential}(1)\\) を２つの分布を考える．どちらも平均，分散共に 1 で一致していますが以下のように分布の形状は大きく異なります．\n\n\nCode\nimport numpy as np\nimport plotly.express as px\nimport polars as pl\n\nnp.random.seed(42)\nn = 1000\n\nx1 = np.random.normal(loc=1, scale=1, size=n)\nx2 = np.random.exponential(scale=1, size=n)\n\ndf = pl.DataFrame({\"normal_dist\": x1, \"exp_dist\": x2})\n\npx.histogram(df, histnorm=\"probability density\",\n             opacity=0.8, barmode=\"overlay\",\n             title='Exp(1) vs Normal(1, 1): same mean and variance')\n\n\n                                                \n\n\n平均や分散(locationとscale)によって確率分布の様子はある程度わかりますが，locationとscaleが同じにもかかわらず上記の指数分布は \\(N(1, 1)\\) に対して，右の裾が長い分布になっています．分布の非対称性や尖りの程度を理解するにあたって尖度と歪度を用いることがあります．\n\nDef: 歪度(skewness)と尖度(kurtosis) \n確率変数 \\(X\\) について，歪度と尖度は以下のように定義される\n\\[\n\\begin{gather*}\n\\operatorname{skewness} = \\mathbb E\\left[\\left(\\frac{X - \\mathbb E[X]}{\\sqrt{\\operatorname{Var}(X)}}\\right)^3\\right]\\\\\n\\operatorname{kurtosis} = \\mathbb E\\left[\\left(\\frac{X - \\mathbb E[X]}{\\sqrt{\\operatorname{Var}(X)}}\\right)^4\\right]\n\\end{gather*}\n\\]\n\n\n📘 REMARKS \n\nskewnessはpositiveならば右の裾が長く，negativeならば左に裾が長い\nkurtosisは大きいほど，鋭いピークと長く太い裾をもった分布になる\nskewness,kurtosisともに標準化してから3rd-moment, 4th-momentを計算しているので，non-zeroの \\(a, b\\) を定数としたとき，\\(aX + b\\) と変数変換を行っても，計算結果は変わらない = 尺度の変換関して不変\n\n\n\nExample 3.10 : 一様分布の歪度と尖度 \n\\(X\\sim\\operatorname{Unif}(0, 1)\\) としたとき，一様分布はlocationから左右対称の分布なので計算することなく\n\\[\n\\operatorname{skewness} = 0\n\\]\nとわかる．一方，尖度は\n\\[\n\\begin{align*}\n\\operatorname{kurtosis}\n    &= \\frac{1}{\\sigma^4}\\int_0^1 \\left(x - \\frac{1}{2}\\right)^4\\mathrm{d}x\\\\\n    &= 144 \\times \\frac{1}{5}\\left[\\left(x - \\frac{1}{2}\\right)^5\\right]^1_0\\\\\n    &= \\frac{9}{5}\n\\end{align*}\n\\]\n標準正規分布の尖度を基準にして\n\\[\n\\operatorname{kurtosis} = \\frac{9}{5}-3=-\\frac{6}{5}\n\\]\nと表現する場合もある",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>期待値</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/moment_generating_function.html",
    "href": "posts/statistics101/moment_generating_function.html",
    "title": "4  母関数",
    "section": "",
    "text": "確率母関数\n\\(p(x)\\geq 0, \\sum p(k)=1\\) より \\(\\vert s\\vert \\leq 1\\) の範囲で \\(\\sum_{k=0}^\\infty s^kp(k)\\) の収束が保証されるので， 「\\(\\vert s\\vert\\leq 1\\) となる \\(s\\) に対して」という条件が付きます．\n▶  PGFと離散確率関数\nPGFの定義より \\(p(0) = G_X(0), p(1) = G_X^\\prime(0), p(2) = \\frac{1}{2!}G_X^{\\prime\\prime}(0)\\) となります．一般に，\n\\[\np(k) = \\frac{1}{k!}\\frac{\\mathrm{d}^k}{\\mathrm{s}^k}G_X(s)\\bigg|_{s=0}\n\\]\nまた，\\(G_X(s) = \\sum_{k=0}^\\infty s^kp(k)\\) より，\n\\[\nG_X(1) = 1\n\\]\nになることも分かる．このように確率母関数が与えられれば，\\(G_X(s)\\) を \\(k\\) 回微分し，\\(s=0\\) と置くことで，\\(p(k)\\) を再現することができることから，非負整数上の確率分布とその確率母関数は１対１に対応していることがわかります．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>母関数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/moment_generating_function.html#確率母関数",
    "href": "posts/statistics101/moment_generating_function.html#確率母関数",
    "title": "4  母関数",
    "section": "",
    "text": "Def: 確率母関数(probability generating function) \n確率変数 \\(X\\) の標本空間を非負の整数全体 \\(\\mathcal{X} = \\{0, 1, 2, \\cdots\\}\\) とし， \\(p(k = \\Pr(X=k))\\) とする． \\(\\vert s\\vert\\leq 1\\) となる \\(s\\) に対して，\n\\[\nG_X(s) = \\mathbb E[s^x] = \\sum_{k=0}^\\infty s^kp(k)\n\\]\nを確率母関数という．\n\n\n\n\n\n\n\n\n\nExample 4.1 : PGFから確率関数を計算する \n確率変数 \\(X\\) について，PGFが \\(G_X(s) = \\frac{s}{5}(2 + 3s^2)\\) と与えられてるとします． このとき，\n\\[\n\\begin{align*}\nG_X(0) &= \\Pr(X=0) = 0\\\\\nG_X^\\prime(0) &= \\Pr(X=1) = \\frac{2}{5}\\\\\n\\frac{1}{2!}G_X^{\\prime\\prime}(0) &= \\Pr(X=2) = 0\\\\\n\\frac{1}{3!}G_X^{\\prime\\prime\\prime}(0) &= \\Pr(X=3) = \\frac{3}{5}\\\\\nG_X^{(r)}(0) &= 0 \\quad\\forall r\\geq 4\n\\end{align*}\n\\]\n従って，\n\\[\nX = \\bigg\\{\\begin{array}{c}\n    1 &\\text{with probability} \\frac{2}{5}\\\\\n    3 &\\text{with probability} \\frac{3}{5}\n\\end{array}\n\\]\nとなることがわかります\n\n\n\nTheorem 4.1 : PGFと階乗モーメント \n確率変数 \\(X\\) について，PGFが \\(G_X(s)\\) と与えられているとき，\n\n\\[\n\\begin{align*}\n&\\mathbb E[X] = G_X^\\prime(1)\\tag{a}\\\\\n&\\mathbb E[X(X-1)\\cdots(X-k)] = G_X^{(k)}(1)\\tag{b}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n ▶  (a)の証明\n\\[\n\\begin{align*}\nG^\\prime_X(s) = \\sum_{x=0}^\\infty x s^{x-1}p(x)\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\nG^\\prime_X(1)\n    &= \\sum_{x=0}^\\infty x p(x)\\\\\n    &= \\mathbb E[X]\n\\end{align*}\n\\]\n ▶  (b)の証明\n\\[\nG^{(k)}_X(s) = \\sum_{x=0}^\\infty x(x-1)\\cdots(x-k) s^{x-k}p(x)\n\\]\n従って，\n\\[\nG^{(k)}_X(1) = \\mathbb E[X(X-1)\\cdots(X-k)]\n\\]\n\n\n\n\n\nTheorem 4.2 : 独立な確率変数の和についてのPGF \n互いに独立な確率変数 \\(X_1, \\cdots, X_n\\) について，\\(Y = \\sum_{i=1}^nX_i\\) と定義する．このとき，\n\\[\nG_Y(s) = \\prod_{i=1}^nG_{X_i}(s)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nG_Y(s)\n    &= \\mathbb E[s^{(X_1+\\cdots+X_n)}]\\\\\n    &= \\mathbb E[s^{X_1}\\cdots s^{X_n}]\\\\\n    &= \\mathbb E[s^{X_1}]\\cdots \\mathbb E[s^{X_n}] \\because\\text{ 互いに独立}\\\\\n    &=\\prod_{i=1}^nG_{X_i}(s)\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 4.3 \n確率変数列 \\(X_1, X_2, \\cdots \\overset{\\mathrm{iid}}{\\sim}  F\\) とし，各確率変数 \\(X_i\\) の確率母関数を \\(G_X(s)\\) で表すとする． \\(\\{X_i\\}_{i=1}\\) と互いに独立な確率変数 \\(N\\) を考え，新たに以下の形で確率変数 \\(T_N\\) を定義する:\n\\[\nT_N = X_1 + \\cdots + X_N\n\\]\nこのとき，\\(T_N\\) についての確率母関数は\n\\[\nG_{T_N}(s) = G_N(G_X(s))\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nG_{T_N}(s)\n&= \\mathbb{E}(s^{T_N})\\\\\n&= \\mathbb{E}\\left( s^{X_1 + \\cdots + X_N} \\right)\\\\\n&= \\mathbb{E}_N \\left\\{ \\mathbb{E} \\left( s^{X_1 + \\cdots + X_N} \\mid N \\right) \\right\\} \\quad \\text{(conditional expectation)}\\\\\n&= \\mathbb{E}_N \\left\\{ \\mathbb{E} \\left( s^{X_1} \\cdots s^{X_N} \\mid N \\right) \\right\\}\\\\\n&= \\mathbb{E}_N \\left\\{ \\mathbb{E} \\left( s^{X_1} \\cdots s^{X_N} \\right) \\right\\} \\quad (X_i \\text{と} N \\text{は互いに独立})\\\\\n&= \\mathbb{E}_N \\left\\{ \\mathbb{E} \\left( s^{X_1} \\right) \\cdots \\mathbb{E} \\left( s^{X_N} \\right) \\right\\} \\quad (X_i \\text{は互いに独立})\\\\\n&= \\mathbb{E}_N \\left\\{ \\left( G_X(s) \\right)^N \\right\\}\\\\\n&= G_N\\left( G_X(s) \\right) \\quad (G_N\\text{の定義より})\n\\end{align*}\n\\]\n\n\n\n\nExample 4.2 : ATMの引き出し金額 \nとあるATMでお金を引き出す人の一日あたりの合計人数 \\(N\\) という確率変数を考える． ATMで各個人が引き出す金額 \\(X_i\\) は 互いに独立に 平均 \\(\\mu\\), 分散 \\(\\sigma^2\\), PGF \\(G_X(s)\\) の同一分布に従うとします．\nこのとき，一日あたりのATM引き出し金額 \\(T_N = X_1 + \\cdots + X_N\\) について，\n\\[\n\\begin{align*}\n\\mathbb E[T_n]\n    &= G_N^\\prime(G_X(1))\\cdot G_X^\\prime(1)\\\\\n    &= G_N^\\prime(1) \\cdot \\mathbb E[X_i] \\quad\\because G_X(1) = 1\\\\\n    &= \\mathbb E[N] \\cdot \\mathbb E[X_i]\n\\end{align*}\n\\]\nと期待値を表すことができる．\n\n\n📘 REMARKS \n積率母関数と確率母関数について以下の関係が知られています:\n\\[\n\\begin{align*}\nG_X(t) &= M_X(\\log(t))\\\\\nM_X(t) &= G_X(\\exp(t))\n\\end{align*}\n\\]\n以上より，一方が求まっていれば，それをもとにもう一方を求めることができることがわかります．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>母関数</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/order_statistic.html",
    "href": "posts/statistics101/order_statistic.html",
    "title": "5  順序統計量",
    "section": "",
    "text": "順序統計量\n▶  \\(X_{(i)}\\) の累積密度関数と確率密度関数\n\\(F\\) を密度関数 \\(f\\) を持つ連続分布として，\\(X_{(i)} \\leq x\\) となる事象を考える．この事象は \\(X_1, \\cdots, X_n\\) のなかで \\(x\\) 以下となるものの個数が \\(i\\) 個以上であるという事象と同地なので\n\\[\nB_k = \\{X_1, \\cdots, X_n\\text{のうち}k\\text{個が}x\\text{以下}\\}\n\\]\nと事象 \\(B_k\\) を設定すると，\n\\[\n\\Pr(X_{(i)}\\leq x) = \\sum_{k=i}^n \\Pr(B_k)\n\\]\nそれぞれの \\(X_j\\) について，独立に成功確率 \\(p = F(x)\\) のベルヌーイ試行と考えることができるので\n\\[\nF_{X_{(i)}}(X) = \\sum_{k=i}^n {}_nC_{k} p^k (1 - p)^{n-k}, \\ \\ p=F(x)\n\\]\nとなることがわかります．また，この式を \\(x\\) で微分することで \\(X_{(i)}\\) の確率密度関数がわかるので, \\(p(k, m)\\) を \\(\\operatorname{Binom}(m, p)\\) の確率関数とすると\n\\[\n\\begin{align*}\n&\\frac{\\mathrm{d}}{\\mathrm{d}p} {}_nC_{k} p^k (1 - p)^{n-k}\\\\\n&= \\frac{n!}{(k-1)!(n-k)!}p^{k-1}(1-p)^{n-k} - \\frac{n!}{k!(n-k-1)!}p^{k-1}(1-p)^{n-k-1}\\\\\n&= n (p(k-1, n-1) - p(k. n-1))\n\\end{align*}\n\\]\n従って，\\(p(n, n-1) =0\\) とすると，\n\\[\n\\begin{align*}\nf_{X_{(i)}}(x) &= nf(x)\\sum_{k=i}^n(p(k-1, n-1) - p(k. n-1))\\\\\n               &= nf(x) p(i-1, n-1)\\\\\n               &= \\frac{n!}{(i-1)!(n-i)!}f(x)F(x)^{i-1}(1 - F(x))^{n-i}\n\\end{align*}\n\\]\n▶  最大値の分布関数\n最大値の分布関数 \\(\\Pr(X_{(n)} \\leq x)\\) は\n\\[\n\\max_i X_i \\leq x \\Leftrightarrow X_i \\leq x, \\  \\ i = 1, \\cdots, n\n\\]\nなので，\n\\[\n\\begin{align*}\n&\\Pr(\\max_i X_i \\leq x )= F(x)^n\\\\\n&f_{X_{(n)}}(x) = nf(x)F(x)^{n-1}\n\\end{align*}\n\\]\n▶  最小値の分布関数\n最小値の分布関数 \\(\\Pr(X_{(1)} \\leq x) = \\Pr(\\min(X_i)\\leq x)\\) は\n\\[\n\\min_i X_i &gt; x \\Leftrightarrow X_i &gt; x, \\  \\ i = 1, \\cdots, n\n\\]\nより\n\\[\n\\begin{align*}\n&\\Pr(\\min_i X_i \\leq x )= 1 - (1 - F(x))^n\\\\\n&f_{X_{(1)}}(x) = nf(x)(1 - F(x))^{n-1}\n\\end{align*}\n\\]\nCode\nimport numpy as np\nfrom scipy.stats import beta\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nN = 10\nITER = 10000\nI = 9\nNBINS = 100\na, b = 10, 1\nnp.random.seed(42)\n\n\ndef random_sampling_from_unif(index, size):\n    return sorted(np.random.uniform(0, 1, size))[index]\n\n\nx = np.array(list(map(lambda x: random_sampling_from_unif(I, N), range(ITER))))\nbeta_x = np.linspace(beta.ppf(0.01, a, b), beta.ppf(0.99, a, b), NBINS)\n\n# plot\nnewnames = {\"0\": \"sample maximum\"}\nfig = px.histogram(x, histnorm=\"probability density\", nbins=NBINS, title=\"maximum value distribution of 10 rvs from Unif(0, 1)\")\nfig.for_each_trace(\n    lambda t: t.update(\n        name=newnames[t.name],\n        hovertemplate=t.hovertemplate.replace(t.name, newnames[t.name]),\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=beta_x, y=beta.pdf(beta_x, a, b), mode=\"lines\", name=\"beta(10, 1) pdf\"\n    ),\n    row=1,\n    col=1,\n)\nfig.show()",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>順序統計量</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/order_statistic.html#順序統計量",
    "href": "posts/statistics101/order_statistic.html#順序統計量",
    "title": "5  順序統計量",
    "section": "",
    "text": "Def: 順序統計量 \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} F\\) とするとき，これkらの確率変数の値を小さい順に並び替えたものを\n\\[\nX_{(1)}\\leq\\cdots\\leq X_{(i)}\\leq\\cdots \\leq X_{(n)}\n\\]\nと表し，順序統計量(order statistic)という．\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5.1 一様分布に従う確率変数列の順序統計量とベータ分布 \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} \\operatorname{Unif}(0, 1)\\) とするとき，第 \\(i\\) 順序統計量の確率密度関数と分布関数はそれぞれ以下のようになる\n\\[\n\\begin{align*}\nf_{X_{(i)}} &= \\frac{n!}{(i-1)!(n-i)!} x^{i-1}(1 - x)^{n-i}\\\\\n            &= \\frac{x^{i-1}(1 - x)^{n-i}}{\\operatorname{B}(i, n-i+1)}\\\\\nF_{X_{(i)}} &= \\sum_{k=i}^n \\frac{n!}{(n-k)!k!} x^k (1 - x)^{n-k}\\\\\n            &= \\frac{n!}{(i-1)!(n-i)!}\\int^x_0t^{i-1}(1 - t)^{n-i}\\mathrm{d}t\n\\end{align*}\n\\]\n従って，\\(X_{(i)}\\) はベータ分布 \\(\\operatorname{Beta}(i, n-i+1)\\) に従うことがわかる．ここから \\(\\operatorname{Beta}(1, 1)\\) が \\(\\operatorname{Unif}(0, 1)\\) に等しくなることもわかる．",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>順序統計量</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/order_statistic.html#最大値と最小値の同時密度関数",
    "href": "posts/statistics101/order_statistic.html#最大値と最小値の同時密度関数",
    "title": "5  順序統計量",
    "section": "最大値と最小値の同時密度関数",
    "text": "最大値と最小値の同時密度関数\n\n\nTheorem 5.1 最大値と最小値の同時密度関数 \n\\(X_1, \\cdots, X_n \\overset{\\mathrm{iid}}{\\sim} F\\) とするとき，\\(x&lt;y\\) とすると\n\\[\n\\begin{align*}\nF_{X_{(1)},X_{(n)}}(x, y) &= \\Pr(X_{(1)} \\leq x,X_{(n)}\\leq y)\\\\\n                          &= [F(y)]^n - [F(y) - F(x)]^n\n\\end{align*}\n\\]\n同時確率密度関数は\n\\[\nf_{X_{(1)},X_{(n)}}(x, y) = n(n-1)[F(y) - F(x)]^{n-2}f(x)f(y)\n\\]\n\n\n\\(x &lt; y\\) の条件のもとで，\n\\[\nF_{X_{(1)},X_{(n)}}(x, y) = \\Pr(X_{(1)} \\leq x,X_{(n)}\\leq y)\n\\]\nについてまず考える．確率事象の排他性より\n\\[\n\\Pr(X_{(n)} \\leq y) = \\Pr(X_{(1)}\\leq x, X_{(n)}\\leq y) + \\Pr(X_{(1)}&gt; x, X_{(n)}\\leq y)\n\\]\nここから，\\(\\Pr(X_{(1)}\\leq x, X_{(n)}\\leq y) = \\Pr(X_{(n)} \\leq y) - \\Pr(X_{(1)}&gt; x, X_{(n)}\\leq y)\\) を得る．\n\\[\n\\begin{align*}\n&\\Pr(X_{(1)}&gt; x, X_{(n)}\\leq y)\\\\\n&= \\Pr(x &lt; X_{(1)} \\leq y, \\cdots, x &lt; X_{(i)} \\leq y, \\cdots, x&lt; X_{(n)}\\leq y)\\\\\n&= [F(y) - F(x)]^n \\because{\\operatorname{i.i.d}}\n\\end{align*}\n\\]\n\\(\\Pr(X_{(n)}\\leq y) = [F(y)]^n\\) であるのは上で確認したので，従って，\n\\[\n\\begin{align*}\nF_{X_{(1)},X_{(n)}}(x, y) &= \\Pr(X_{(1)} \\leq x,X_{(n)}\\leq y)\\\\\n                          &= [F(y)]^n - [F(y) - F(x)]^n\n\\end{align*}\n\\]\n同時確率密度関数は\n\\[\n\\begin{align*}\nf_{X_{(1)},X_{(n)}}(x, y) &= \\frac{\\mathrm{d}}{\\mathrm{d}x}\\frac{\\mathrm{d}}{\\mathrm{d}x}\\{[F(y)]^n - [F(y) - F(x)]^n\\}\\\\\n&= \\frac{\\mathrm{d}}{\\mathrm{d}x}\\{n[F_y]^{n-1}f(y) - n[F(y) - F(x)]^{n-1}f(y)\\}\\\\\n&= n(n-1)[F(y) - F(x)]^{n-2}f(y)f(x)\n\\end{align*}\n\\]\n ▶  A heuristic approach for calculating the joint pdf\n\\(\\min X_i = x, \\max X_i =y\\), と決まっており, それ以外の値は \\((x, y)\\) 区間に収まっていれば何でも良いので， index通りに順番があるならば\n\\[\nf(x)[F(y) - F(x)]^{n-2}f(y)\n\\]\nまた，この並べ方は \\(n!\\) 通り存在するが，\\(X_{(1)}, X_{(n)}\\) 以外の並び方には区別はいらないので\n\\[\n\\frac{n!}{(n-2)!}f(x)[F(y) - F(x)]^{n-2}f(y) = n(n-1)f(x)[F(y) - F(x)]^{n-2}f(y)\n\\]\n\n📘 REMARKS \n並び替えの考えで同様に \\((X_{(1)}, \\cdots, X_{(n)})\\) の同時確率密度関数は\n\\[\nf_{X_{(1)}, \\cdots, X_{(n)}}(x_1, \\cdots, x_n) = n!f(x_1)f(x_2)\\cdots f(x_n)\n\\]\nであることがわかる．\n\\(X_{(i)}, X_{(j)}\\) の同時確率密度関数は\n\\[\nf_{X_{(i)}, X_{(j)}}(x, y)=\\frac{n!}{(i-1)!(j-i-1)!}(n-j)!f(x)f(y)F(x)^{i-1}[F(y)-F(x)]^{j-i-1}[1 - F(y)]^{n-j}\n\\]",
    "crumbs": [
      "統計学入門",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>順序統計量</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/discrete_uniform_dist.html",
    "href": "posts/probability_distribution/discrete_uniform_dist.html",
    "title": "6  離散一様分布",
    "section": "",
    "text": "離散一様分布の性質\nフェアなサイコロをふったときに出る目を確率変数 \\(X\\) とみなしたとき，\\(X\\sim \\operatorname{DU}(6)\\) となります．\n離散一様分布のQuantile functionは，\n\\[\nQ_X(u) = \\inf\\{x\\in\\mathbb R: F_X(x) \\geq u\\}\n\\]\nなので，\\(u\\in(0, 1]\\) について\n\\[\nQ_X(u) = \\lceil np\\rceil\n\\]\nと定義されます．\n▶  確率母関数を用いたモーメントの計算\n\\[\n(1-s^n) = (1 - s)(1 + s + s^2 + \\cdots + s^{n-1})\n\\]\nより\n\\[\nG_X(s) = \\frac{s + s^2 + \\cdots + s^n}{n}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\mathbb E[X] &= G_X^\\prime(s)\\bigg\\vert_{s=0} \\\\\n             &= \\frac{n+1}{2}\\\\\n\\mathbb E[X(X-1)]\n    &= G_X^{\\prime\\prime}(s)\\bigg\\vert_{s=0}\\\\\n    &= \\frac{1}{n}\\sum_{k=1}^nk(k-1)\\\\\n    &= \\frac{(n+1)(2n+1)}{6} - \\frac{n+1}{2}\n\\end{align*}\n\\]\n▶  特性関数\n特性関数は\n\\[\n\\varphi_X(t) = \\frac{\\exp(it) - \\exp(it(n+1))}{n(1 - \\exp(it))}\n\\]\n▶  歪度\n\\[\n\\mathbb E[(X - \\mathbb E[X])^3] = (\\mathbb E[X^3] – 3\\mathbb E[X^2]E[X] + 2\\mathbb E[X]^3)\n\\]\nなので，これをまず計算します．\n\\[\n\\mathbb E[X^3] = M_X^{\\prime\\prime\\prime}(t) \\bigg\\vert_{t=0} = n\\left(\\frac{n+1}{2}\\right)^2\n\\]\nこれを用いて計算すると，\\(n - 2n-1 + n+1 = 0\\) より \\(\\operatorname{skewness} = 0\\)． 平均を中心に対称な分布なので，計算しなくても歪度は0であると判断することもできます．\n▶  エントロピー\n離散確率変数 \\(X\\) の確率関数を \\(p(x)\\) とします．このとき，エントロピーを以下のように定義します．\n\\[\n\\mathrm{H}(X) = -\\sum_x p(x) \\log(p(x)) \\qquad (\\lim_{p\\to 0}p\\log(p) = 0 \\text{とする})\n\\]\n\\(X\\sim \\operatorname{DU}(n)\\) としたとき，エントロピーは\n\\[\n\\begin{align*}\n\\mathrm{H}(X) = -\\frac{1}{n}\\sum_x \\log(\\frac{1}{n}) = \\log(n)\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>離散一様分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/discrete_uniform_dist.html#離散一様分布の性質",
    "href": "posts/probability_distribution/discrete_uniform_dist.html#離散一様分布の性質",
    "title": "6  離散一様分布",
    "section": "",
    "text": "Def: 離散一様分布 \n\\(n\\in\\mathbb N\\) に対して，確率変数 \\(X\\) が \\(S = \\{1, 2, \\cdots, n\\}\\) を標本空間し，\n\\[\n\\Pr(X = k) = \\frac{1}{n}\\quad k\\in S\n\\]\nを満たすとき，\\(X\\) は離散一様分布に従うという．(\\(X\\sim \\operatorname{DU}(n)\\))\n\n\n\n\n\n\n\n\nProperty: 累積分布関数\n\n\n\n離散一様分布の累積分布関数(分布関数)は\n\\[\nF(x) = \\Pr(X\\leq x) = \\left\\{\\begin{array}{c}\n0 & x &lt; 1\\\\\n\\frac{\\lfloor x\\rfloor}{n} & 1\\leq 1 \\leq n\\\\\n1 & x \\geq n\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\n\n\n\nTheorem 6.1 : 期待値と分散 \n離散一様分布に従う確率変数 \\(X\\sim \\operatorname{DU}(n)\\) について，\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\frac{n+1}{2}\\\\\n\\operatorname{Var}(X) &= \\frac{n^2-1}{2}\\\\\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[X] &= \\sum_{k=1}^nk\\frac{1}{n} = \\frac{n(n+1)}{2n}=\\frac{n+1}{2}\\\\\n\\end{align*}\n\\]\n分散については\n\\[\n\\begin{align*}\n\\mathbb E[X^2] &= \\sum_{k=1}^nk^2\\frac{1}{n} = \\frac{(n+1)(2n+1)}{6}\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(X) = \\frac{n^2-1}{12}\n\\]\n\n\n\n\n\nTheorem 6.2 : 確率母関数 \n\\(X\\sim \\operatorname{DU}(n)\\) について，確率母関数 \\(G_X(s)\\) は\n\\[\nG_X(s) = \\frac{s(1 - s^n)}{n(1-s)}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nG_X(s) &= \\mathbb E[s^x]\\\\\n       &= \\sum_{k=1}^n \\frac{s^x}{n}\\\\\n       &= \\frac{s(1-s^n)}{n(1-s)}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 6.3 \n\\(X\\sim \\operatorname{DU}(n)\\) について，MGF \\(M_X(t)\\) は\n\\[\nM_X(t) = \\frac{\\exp(t) - \\exp(t(n+1))}{n(1 - \\exp(t))}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\mathbb E[\\exp(tX)] = \\frac{1}{n}\\sum_{k=1}^n \\exp(tk) = \\frac{\\exp(t) - \\exp(t(n+1))}{n(1 - \\exp(t))}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>離散一様分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/discrete_uniform_dist.html#k-個のサイコロの目の合計",
    "href": "posts/probability_distribution/discrete_uniform_dist.html#k-個のサイコロの目の合計",
    "title": "6  離散一様分布",
    "section": "\\(k\\) 個のサイコロの目の合計",
    "text": "\\(k\\) 個のサイコロの目の合計\n\\(k\\) 個のフェアなサイコロの目の合計の確率分布はモーメント母関数を用いて導出することができるのでこのセクションで紹介します．\n\nDef: 離散確率変数のMGF \n確率変数 \\(X\\) が離散確率変数 \\(\\Pr(X = x_i) = p_i, i = 1, \\cdots, k\\) の場合，MGFは以下のように表現される：\n\\[\n\\begin{align*}\nM_X(t)\n    &= \\mathbb E[\\exp(tX)]\\\\\n    &= p_1\\exp(tx_1) + p_2\\exp(tx_2) + \\cdots + p_k\\exp(tx_k)\n\\end{align*}\n\\]\n\n上記の定義より，フェアなサイコロのMGFは以下のようになります\n\\[\nM_X(t) = \\frac{1}{6}\\left(\\exp(t) + \\exp(2t) + \\cdots + \\exp(6t)\\right)\n\\]\n\n\nTheorem 6.4 : 互いに独立な確率変数の和とMGF \n互いに独立な確率変数 \\(X, Y\\) について，\\(Z = X +Y\\) と確率変数を定義したとき，\n\\[\nM_Z(t) = M_X(t)M_Y(t)\n\\]\nが成立する・\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nM_Z(t)\n    &= \\mathbb E[\\exp(tZ)]\\\\\n    &= \\mathbb E[\\exp(tX + tY)]\\\\\n    &= \\mathbb E[\\exp(tX)\\exp(tY)]\\\\\n    &= \\mathbb E[\\exp(tX)]\\mathbb E[\\exp(tY)]\\\\\n    &= M_X(t)M_Y(t)\n\\end{align*}\n\\]\n\n\n\n\n\\(k\\) 個のサイコロの目の合計の確率分布\n\\(k\\) 個のフェアなサイコロを独立に投げ，それぞれの目を \\(X_1, \\cdots, X_k\\) としたとき，サイコロの目の合計 \\(Y\\) は\n\\[\nY = X_1 + \\cdots + X_k\n\\]\nこのとき，上で確認したMGFの性質より\n\\[\nM_Y(t) = \\frac{1}{6}(\\exp(t)+\\exp(2t)+\\cdots + \\exp(6t))^k\n\\]\n ▶  \\(k=2\\) の場合\n\\[\n\\begin{align*}\nM_Y(t)\n    =& \\frac{1}{6}(\\exp(2t) + 2\\exp(3t)+3\\exp(4t) + 4\\exp(5t)\\\\[4pt]\n    &+ 5\\exp(6t)+ 6\\exp(7t)+ 5\\exp(8t)+ 4\\exp(9t)+ 3\\exp(10t)\\\\[4pt]\n    &+ + 2\\exp(11t)+ \\exp(12t))\n\\end{align*}\n\\]\n各係数が確率関数に対応していることがわかります．\n ▶  \\(k=4\\) の場合\n基本的には，\\(Y = \\lfloor\\mathbb E[X_i] \\times 4\\rfloor\\) が最頻値となることを留意すると\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(k=1\\)\n\\(k=2\\)\n\\(k=3\\)\n\\(k=4\\)\n\n\nvalue\nfreq\nvalue\nfreq\nvalue\nfreq\nvalue\nfreq\n\n\n\\(1\\)\n\\(1\\)\n\\(2\\)\n\\(1\\)\n\\(3\\)\n\\(1\\)\n\\(4\\)\n\\(1\\)\n\n\n\\(2\\)\n\\(1\\)\n\\(3\\)\n\\(2\\)\n\\(4\\)\n\\(3\\)\n\\(5\\)\n\\(4\\)\n\n\n\\(3\\)\n\\(1\\)\n\\(4\\)\n\\(3\\)\n\\(5\\)\n\\(6\\)\n\\(6\\)\n\\(10\\)\n\n\n\\(4\\)\n\\(1\\)\n\\(5\\)\n\\(4\\)\n\\(6\\)\n\\(10\\)\n\\(7\\)\n\\(20\\)\n\n\n\\(5\\)\n\\(1\\)\n\\(6\\)\n\\(5\\)\n\\(7\\)\n\\(15\\)\n\\(8\\)\n\\(35\\)\n\n\n\\(6\\)\n\\(1\\)\n\\(7\\)\n\\(6\\)\n\\(8\\)\n\\(21\\)\n\\(9\\)\n\\(56\\)\n\n\n\n\n\\(8\\)\n\\(5\\)\n\\(9\\)\n\\(25\\)\n\\(10\\)\n\\(80\\)\n\n\n\n\n\\(9\\)\n\\(4\\)\n\\(10\\)\n\\(27\\)\n\\(11\\)\n\\(104\\)\n\n\n\n\n\\(10\\)\n\\(3\\)\n\\(11\\)\n\\(27\\)\n\\(12\\)\n\\(125\\)\n\n\n\n\n\\(11\\)\n\\(2\\)\n\\(12\\)\n\\(25\\)\n\\(13\\)\n\\(140\\)\n\n\n\n\n\\(12\\)\n\\(1\\)\n\\(13\\)\n\\(21\\)\n\\(14\\)\n\\(146\\)\n\n\n\n\n\n\n\\(14\\)\n\\(15\\)\n\\(15\\)\n\\(140\\)\n\n\n\n\n\n\n\\(15\\)\n\\(10\\)\n\\(16\\)\n\\(125\\)\n\n\n\n\n\n\n\\(16\\)\n\\(6\\)\n\\(17\\)\n\\(104\\)\n\n\n\n\n\n\n\\(17\\)\n\\(3\\)\n\\(18\\)\n\\(80\\)\n\n\n\n\n\n\n\\(18\\)\n\\(1\\)\n\\(19\\)\n\\(56\\)\n\n\n\n\n\n\n\n\n\\(20\\)\n\\(35\\)\n\n\n\n\n\n\n\n\n\\(21\\)\n\\(20\\)\n\n\n\n\n\n\n\n\n\\(22\\)\n\\(10\\)\n\n\n\n\n\n\n\n\n\\(23\\)\n\\(4\\)\n\n\n\n\n\n\n\n\n\\(24\\)\n\\(1\\)\n\n\n\n\n📘 REMARKS \n\n最小値と最大値の頻度は \\(1\\) となる\n最頻値はちょうど真ん中の値となる(\\(Y = \\lfloor\\mathbb E[X_i] \\times 4\\rfloor\\))\n頻度 \\(f\\) は最頻値に達するまで，以下のようにrecursiveに計算できる\n\n\\[\nf_k(y) = f_k(y-1) + f_{k-1}(y-1) - f_{k-1}(y-7)\n\\]\n上記において，\\(f_{k}(y)\\) が存在しない場合は \\(0\\) と扱う",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>離散一様分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/binomial_dist.html",
    "href": "posts/probability_distribution/binomial_dist.html",
    "title": "7  二項分布",
    "section": "",
    "text": "二項分布の性質\n▶  確率母関数の計算\n確率変数 \\(X\\sim\\operatorname{Binom}(n, p)\\) としたとき，\n\\[\n\\begin{align*}\nG_X(s)\n    &= \\sum_{x=0}^ns^x \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)p^x(1-p)^{n-x}\\\\\n    &= \\sum_{x=0}^n \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)(sp)^x(1-p)^{n-x}\\\\\n    &= (sp + 1-p)^n\\\\[5pt]\n    &= [(s-1)p + 1]^n\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>二項分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/poisson_dist.html",
    "href": "posts/probability_distribution/poisson_dist.html",
    "title": "8  ポワソン分布",
    "section": "",
    "text": "ポワソン分布の性質\n▶  確率母関数の計算\n確率変数 \\(X\\sim\\operatorname{Poisson}(\\lambda)\\) としたとき，PGF \\(G_X(s)\\) は\n\\[\n\\begin{align*}\nG_X(s)\n    &= \\sum_{x=0}^\\infty s^x\\frac{\\lambda^x}{x!}\\exp(-\\lambda)\\\\\n    &= \\exp(-\\lambda)\\sum_{x=0}^\\infty \\frac{(s\\lambda)^x}{x!}\\\\\n    &= \\exp(-\\lambda)\\exp(s\\lambda)\\\\\n    &= \\exp((s-1)\\lambda)\n\\end{align*}\n\\]\n期待値は\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= G_X^\\prime(1)\\\\\n    &= \\lambda \\exp((1-1)\\lambda)\\\\\n    &= \\lambda\n\\end{align*}\n\\]\n分散は \\(\\operatorname{Var}(X) = \\mathbb E[X(X-1)] + \\mathbb E[X](1 - \\mathbb E[X])\\) なので\n\\[\n\\begin{align*}\n\\mathbb E[X(X-1)]\n    &= G_X^{\\prime\\prime}(1)\\\\\n    &= \\lambda \\lambda\\exp((1-1)\\lambda)\\\\\n    &= \\lambda^2\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\operatorname{Var}(X)\n    &= \\lambda^2 + \\lambda(1-\\lambda)\\\\\n    &= \\lambda\n\\end{align*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ポワソン分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/poisson_dist.html#ポワソン分布の性質",
    "href": "posts/probability_distribution/poisson_dist.html#ポワソン分布の性質",
    "title": "8  ポワソン分布",
    "section": "",
    "text": "Theorem 8.1 : ポワソン分布のの再生性 \n互いに独立な確率変数 \\(X\\sim\\operatorname{Poisson}(\\lambda_x), Y\\sim\\operatorname{Poisson}(\\lambda_y)\\) について, \\(X + Y\\) もパラメーター \\(\\lambda_x + \\lambda_y\\) のポワソン分布に従う．つまり，\n\\[\nX + Y \\sim \\operatorname{Poisson}(\\lambda_x + \\lambda_y)\n\\]\n\n\n\n\n\n\n\n\nProof: PGFを用いた証明\n\n\n\n\n\n\\[\n\\begin{align*}\nG_{X+Y}(s)\n    &= G_{X}(s)G_{Y}(s)\\\\\n    &= \\exp(-\\lambda_x)\\exp(s\\lambda_x)\\exp(-\\lambda_y)\\exp(s\\lambda_y)\\\\\n    &= \\exp(-(\\lambda_x+\\lambda_y))\\exp(s(\\lambda_x+\\lambda_y))\n\\end{align*}\n\\]\nこれは，パラメーター \\(\\lambda_x + \\lambda_y\\) のポワソン分布のPGFに一致する．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ポワソン分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/geometric_dist.html",
    "href": "posts/probability_distribution/geometric_dist.html",
    "title": "9  幾何分布",
    "section": "",
    "text": "幾何分布の性質\n幾何分布は，時間を \\(0, 1, 2, \\cdots\\) と離散的に考えるとき，初めてイベント \\(S\\) が起こるまでの時間の長さを表す確率分布 = 離散的な待ち時間分布とみなすことができます．一回あたりのイベント \\(S\\) の生起確率が \\(p\\) であるとすると，ちょうど１回の \\(S\\) を得るまで平均的に\n\\[\n\\frac{1}{p}\n\\]\nの時間がかかるので，生起までの待ち時間として \\(\\frac{1}{p}-1 = \\frac{1-p}{p}\\) と直感的に理解することができます．\n▶  累積分布関数\n幾何分布の累積分布関数は以下のように表すことができます:\n\\(x \\geq 0\\) のもとで\n\\[\n\\begin{align*}\n\\Pr(X\\leq x)\n    &= \\sum_{k=0}^x \\Pr(X=k)\\\\\n    &= \\sum_{k=0}^{\\lfloor x\\rfloor} pq^k\\\\\n    &= p\\frac{1-q^{\\lfloor x\\rfloor+1}}{1- q}\\\\\n    &= 1-q^{\\lfloor x\\rfloor+1}\n\\end{align*}\n\\]\nCode\nimport numpy as np\nfrom scipy.stats import geom\nfrom plotly import express as px\n\np = 0.4\nx = np.arange(geom.ppf(0.01, p), geom.ppf(0.99995, p))\n\n# note that scipy follows p(x) = (1-p)^{x-1}p \nrv = geom(p)\n\nfig = px.line(\n    x=x-1,\n    y=rv.cdf(x),\n    title=\"geometric distribution with p=0.4\",\n    labels={\"x\": \"x\", \"y\": \"probability\"},\n    markers='x'\n)\nfig.show()\n▶  幾何分布の期待値\n\\(q=1-p\\)とおくと\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_{x=0}^\\infty xq^xp\\\\\n    &= p\\sum_{x=1}^\\infty xq^x\n\\end{align*}\n\\]\nここで，\\(S = \\sum_{x=1}^\\infty xq^x\\) について考えると， \\[\n\\begin{align*}\nS &= q + 2q^2 + 3q^3 + \\cdots\\\\\nqS &= q^2 + 2q^3 + \\cdots\\\\\n(1-q)S& = q + q^2 + q^3 + \\cdots\n\\end{align*}\n\\]\n従って，\\(S = \\frac{q}{(1-q)^2}\\) を得る．以上より\n\\[\n\\mathbb E[X] = p \\frac{q}{(1-q)^2} = \\frac{1-p}{p}\n\\]\n▶  分布関数を用いた幾何分布の期待値の導出\n離散確率変数 \\(X\\) について，\n\\[\n\\mathbb E[X] = \\sum_{x=0}^\\infty \\Pr(X&gt; x)\n\\]\nが知られている(参考:期待値 &gt; Example 3.4)ので\n\\[\n\\begin{align*}\n\\Pr(X&gt; x)\n    &= 1 - \\Pr(X\\leq x)\\\\\n    &= 1 - (1 - q^{x+1})\\\\\n    &= q^{x+1}\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_{x=0}^\\infty q^{x+1}\\\\\n    &= \\frac{q}{1-q}\\\\\n    &= \\frac{1-p}{p}\n\\end{align*}\n\\]\n▶  確率の公理を用いた幾何分布の期待値の導出\n\\[\n\\sum_{x=0}^\\infty q^xp = 1\n\\]\nに留意すると\n\\[\n\\begin{align*}\n\\sum_{x=0}^\\infty xq^xp - q\\sum_{x=0}^\\infty xq^xp\n    &= \\sum_{x=0}^\\infty q^xp - p\\\\\n    &= 1-p\n\\end{align*}\n\\]\n従って，\\(\\mathbb E[X] - q\\mathbb E[X] = 1-p\\) を得る．これを整理すると\n\\[\n\\mathbb E[X] = \\frac{1-p}{p}\n\\]\n▶  幾何分布の分散\n\\[\n\\begin{align*}\n\\mathbb E[X^2]\n    &= \\sum_{x=0}^\\infty x^2q^xp\\\\\n    &= p\\sum_{x=1}^\\infty x^2q^x\n\\end{align*}\n\\]\nここで，\\(S = \\sum_{x=1}^\\infty x^2q^x\\) について考えると， \\[\n\\begin{align*}\nS &= 1^2q + 2^2q^2 + 3^2q^3 + \\cdots\\\\\nqS &= 1^2q^2 + 2^2q^3 + \\cdots\\\\\n(1-q)S& = 1^2q + (2^2-1^2)q^2 + (3^2-2^2)q^3 + \\cdots\\\\\n      &= \\sum_{x=1}^\\infty [x^2 - (x-1)^2]q^x\\\\\n      &= \\sum_{x=1}^\\infty [2x-1]q^x\\\\\n      &= \\frac{2-2p}{p^2} - \\frac{1-p}{p}\\\\\n      &= \\frac{(1-p)(2-p)}{p^2}\n\\end{align*}\n\\]\n\\(\\mathbb E[X^2] = pS = (1-q)S\\) より，\n\\[\n\\operatorname{Var}(X) = \\frac{(1-p)(2-p)}{p^2} - \\frac{(1-p)^2}{p^2} = \\frac{(1-p)}{p^2}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/geometric_dist.html#幾何分布の性質",
    "href": "posts/probability_distribution/geometric_dist.html#幾何分布の性質",
    "title": "9  幾何分布",
    "section": "",
    "text": "Def: 幾何分布 \n標本空間 \\(\\mathcal{X} = \\{0, 1, 2, \\cdots\\}\\) をもつ確率変数 \\(X\\) の確率関数が\n\\[\n\\Pr(X=k) = (1-p)^{k}p \\qquad (0&lt;p&lt;1)\n\\]\nのとき，\\(X\\) はパラメータ \\(p\\) の幾何分布に従うという．つまり，\n\\[\nX\\sim\\operatorname{Geo}(p)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n幾何分布の母関数\n\n\nTheorem 9.1 : 幾何分布の確率母関数 \n\\(X\\sim\\operatorname{Geo}(p)\\) のとき，確率母関数は\n\\[\nG_X(s) = \\frac{p}{1-sq}\n\\]\nと表される\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n確率変数 \\(X\\sim\\operatorname{Geo}(p)\\) としたとき，確率関数は \\(x = 0, 1, 2, \\cdots\\) について\n\\[\n\\begin{align*}\n\\Pr(X=x)\n    &= p(1-p)^x\\\\\n    &=pq^x \\qquad \\text{where } q = 1-p\n\\end{align*}\n\\]\nと表されるので\n\\[\n\\begin{align*}\nG_X(s)\n    &= \\sum_{x=0}^\\infty s^xpq^x\\\\\n    &= p\\sum_{x=0}^\\infty (sq)^x\\\\\n    &= \\frac{p}{1-sq} \\quad \\text{for all} s \\text{ such that } \\vert qs\\vert &lt; 1\n\\end{align*}\n\\]\n従って，\n\\[\nG_X(s) = \\frac{p}{1-sq} \\text{ for } \\vert s\\vert &lt; \\frac{1}{q}\n\\]\nとなります．PGFより期待値は\n\\[\n\\begin{align*}\nG_X^\\prime(s)\n    &= q\\frac{p}{(1-sq)^2}\\\\\n\\Rightarrow \\mathbb E[X] &= \\frac{q}{p} = \\frac{1-p}{p}   \n\\end{align*}\n\\]\n分散は\n\\[\n\\begin{align*}\nG_X^{\\prime\\prime}(s)\n        &= 2q^2\\frac{p}{(1-sq)^3}\\\\\n\\Rightarrow \\operatorname{Var}(X) &= \\frac{2q^2}{p^2} + \\frac{q}{p}(1-\\frac{q}{p})\\\\\n        &= \\frac{q}{p}\\left(\\frac{q}{p}+1\\right)\\\\\n        &=\\frac{1-p}{p^2}\n\\end{align*}\n\\]\n\n\n\n\nExample 9.1 \n初めての成功までに要した回数という形で幾何分布を変更した場合を考えます．つまり， \\(\\mathcal{X} = \\{1, 2, \\cdots\\}\\) と標本空間が表され，確率関数は\n\\[\n\\Pr(X = x) = pq^{x-1}\n\\]\nこの場合の確率母関数は\n\\[\n\\begin{align*}\nG_X(s)\n    &= \\sum_{x=1}^\\infty s^xpq^{x-1}\\\\\n    &= q^{-1}p\\sum_{x=1}^\\infty (sq)^x\\\\\n    &= \\frac{p}{q}\\frac{sq}{1-sq} \\quad \\text{for all} s \\text{ such that } \\vert qs\\vert &lt; 1\n\\end{align*}\n\\]\n期待値を求めてみると\n\\[\n\\begin{align*}\nG_X^\\prime(s)\n    &= \\frac{p}{q}\\frac{q}{1-sq} + p\\frac{sq}{(1-sq)^2}\n\\end{align*}\n\\]\n従って，\n\\[\n\\mathbb E[X] = G_X^\\prime(0) = \\frac{1}{p}\n\\]\n\n\n\nTheorem 9.2 : 積率母関数 \n\\(X\\sim\\operatorname{Geo}(p)\\) のとき，積率母関数は\n\\[\nM_X(t) = \\frac{p}{1 - q\\exp(t)}, \\qquad t &lt; -\\log q\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[\\exp(tX)]\n    &= \\sum_{x=0}^\\infty pq^x\\exp(tx)\\\\\n    &= p\\sum_{x=0}^\\infty q^x\\exp(tx)\\\\\n    &= p \\frac{1}{1 - q\\exp(t)}\n\\end{align*}\n\\]\nただし，収束するためには \\(q\\exp(t) &lt; 1\\)，すなわち \\(t &lt; -\\log(q)\\) が条件となります．\n\n\n\n\n\nTheorem 9.3 : 特性関数 \n\\(X\\sim\\operatorname{Geo}(p)\\) のとき，特性関数は\n\\[\n\\varphi(t) = \\frac{p}{1 - q\\exp(it)}, \\qquad t \\in \\mathbb R\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\mathbb E[\\exp(itX)]\n    &= \\sum_{x=0}^\\infty pq^x\\exp(itx)\\\\\n    &= p\\sum_{x=0}^\\infty q^x\\exp(itx)\\\\\n    &= p \\frac{1}{1 - q\\exp(it)}\n\\end{align*}\n\\]\n\\(\\vert q\\exp(it)\\vert &lt; 1\\) であるので，\\(t\\in \\mathbb R\\) で上記の式は収束します．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/hypergeometric.html",
    "href": "posts/probability_distribution/hypergeometric.html",
    "title": "10  超幾何分布",
    "section": "",
    "text": "超幾何分布の性質\nツボに \\(K\\) 個の赤玉と \\(N-K\\) 個の白玉，つまり合計 \\(N\\) 個の玉が入っている中から，\\(n\\) 個の玉をランダムに 非復元(without replacement)で抽出するとする．このとき取り出した赤玉の個数を \\(X\\) としたとき，この \\(X\\) は超幾何分布 \\(\\operatorname{Hypergeometric}(N, K, n)\\) に従います．\n▶  確率関数の合計が1になることの証明\n恒等式\n\\[\n(1 + t)^N = (1 + t)^{K}(1 + t)^{N-K}\n\\]\nを考える．RHSを展開し，\\(t^n\\) の係数 \\(\\beta_n\\) を見てみると\n\\[\n\\begin{align*}\n\\beta_n = \\sum_{\\max(n+K-N, 0)}^{\\min(K, n)} {}_KC_{x} \\times {}_{N-K}C_{n-x}\n\\end{align*}\n\\]\n一方，LHSでみると\n\\[\n\\beta_n = {}_NC_n\n\\]\n従って，\n\\[\n\\begin{gather*}\n\\sum_{\\max(n+K-N, 0)}^{\\min(K, n)} {}_KC_{x} \\times {}_{N-K}C_{n-x} = {}_NC_n\\\\\n\\Rightarrow \\sum_{\\max(n+K-N, 0)}^{\\min(K, n)}\\Pr(X=k) = 1\n\\end{gather*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>超幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/hypergeometric.html#超幾何分布の性質",
    "href": "posts/probability_distribution/hypergeometric.html#超幾何分布の性質",
    "title": "10  超幾何分布",
    "section": "",
    "text": "Def: 超幾何分布 \nParameters \\((N, K, n)\\) の超幾何分布に従う確率変数 \\(X\\) について，その確率関数は\n\\[\n\\begin{gather*}\n\\Pr(X = x) = \\frac{{}_KC_x \\cdot {}_{N-K}C_{n-x}}{{}_NC_n}\\\\\n\\text{where} \\max\\{0, n+K-N\\} \\leq x \\leq \\min\\{n, K\\}\n\\end{gather*}\n\\]\nまた \\(\\max\\{0, n+K-N\\} \\leq x \\leq \\min\\{n, K\\}\\) の範囲外の \\(x\\) については \\(\\Pr(X = x) = 0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 10.1 期待値 \n確率変数 \\(X \\sim \\operatorname{Hypergeometric}(N, K, n)\\) について\n\\[\n\\mathbb E[X] = n\\frac{K}{N}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\left(\\begin{array}{c}n\\\\k\\end{array}\\right)= \\frac{n}{k} \\left(\\begin{array}{c}n-1\\\\ k-1\\end{array}\\right)\n\\]\nという関係式をもちいると\n\\[\n\\begin{align*}\n\\mathbb E[X]\n    &= \\sum_x\\frac{x \\left(\\begin{array}{c}K\\\\ x\\end{array}\\right)\\left(\\begin{array}{c}N-K\\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N\\\\ n\\end{array}\\right)}\\\\\n    &= \\frac{nK}{N}\\sum_x\\frac{ \\left(\\begin{array}{c}K-1\\\\ x-1\\end{array}\\right)\\left(\\begin{array}{c}N-K\\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N-1\\\\ n-1\\end{array}\\right)}\\\\\n    &= \\frac{nK}{N}\\sum_x\\frac{ \\left(\\begin{array}{c}K-1\\\\ x-1\\end{array}\\right)\\left(\\begin{array}{c}N-1-(K-1)\\\\ n-1 - (x-1)\\end{array}\\right)}{\\left(\\begin{array}{c}N-1\\\\ n-1\\end{array}\\right)}\n\\end{align*}\n\\]\n最後の式変形は，ツボに \\(K-1\\) 個の赤玉と \\(N-1 - (K-1)\\) 個の白玉，つまり合計 \\(N-1\\) 個の玉が入っている中から \\(n-1\\) 個のボールを選ぶ場合の確率関数と同じなので\n\\[\n\\begin{align*}\n\\sum_x\\frac{ \\left(\\begin{array}{c}K-1\\\\ x-1\\end{array}\\right)\\left(\\begin{array}{c}N-1-(K-1)\\\\ n-1 - (x-1)\\end{array}\\right)}{\\left(\\begin{array}{c}N-1\\\\ n-1\\end{array}\\right)} = 1\n\\end{align*}\n\\]\n従って，\\(\\displaystyle\\mathbb E[X] = \\frac{nK}{N}\\) を得る．\n\n\n\n\n\nTheorem 10.2 分散 \n確率変数 \\(X \\sim \\operatorname{Hypergeometric}(N, K, n)\\) について\n\\[\n\\operatorname{Var}(X) = n\\frac{K}{N}\\frac{N-K}{N}\\frac{N-n}{(N-1)}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\operatorname{Var}(X) = \\mathbb E[X(X-1)] + E[X](1 - E[X])\n\\]\nなので，\\(\\mathbb E[X(X-1)]\\) がわかれば良い．\n\\[\n\\begin{align*}\n\\mathbb E[X(X-1)]\n    &= \\sum_{x}\\frac{x(x-1)\\left(\\begin{array}{c}K \\\\ x\\end{array}\\right)\\left(\\begin{array}{c}N-K \\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N \\\\ n\\end{array}\\right)}\\\\\n    &= \\frac{n(n-1)K(K-1)}{N(N-1)}\\sum_{x}\\frac{\\left(\\begin{array}{c}K-2 \\\\ x-2\\end{array}\\right)\\left(\\begin{array}{c}(N-2) - (K-2) \\\\ (n-2)-(x-2)\\end{array}\\right)}{\\left(\\begin{array}{c}N -2\\\\ n - 2\\end{array}\\right)}\\\\\n    &= \\frac{n(n-1)K(K-1)}{N(N-1)}\\sum_{l=x-2}\\frac{\\left(\\begin{array}{c}K-2 \\\\ l\\end{array}\\right)\\left(\\begin{array}{c}(N-2) - (K-2) \\\\ (n-2)-l\\end{array}\\right)}{\\left(\\begin{array}{c}N -2\\\\ n - 2\\end{array}\\right)}\\\\\n    &= \\frac{n(n-1)K(K-1)}{N(N-1)}\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(X) = n\\frac{K}{N}\\frac{N-K}{N}\\frac{N-n}{N-1}\n\\]\n\n\n\n\n📘 REMARKS \n\n\\(\\frac{N-n}{N-1}\\) は有限母集団修正と呼ばれる\n\n\n\n超幾何分布の極限と二項分布\n確率変数 \\(X \\sim \\operatorname{Hypergeometric}(N, K, n)\\) について, \\(\\frac{K}{N} = p \\text{ as } N, K \\to\\infty\\) が極限において 成立するとします．\n\\[\n\\begin{align*}\n&\\Pr(X = x)\\\\\n    &= \\frac{\\left(\\begin{array}{c}K\\\\ x\\end{array}\\right)\\left(\\begin{array}{c}N-K\\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N\\\\n\\end{array}\\right)}\\\\\n    &= \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)\\frac{K!}{(K-x)!}\\frac{(N-k)!}{(N-K-n+x)!}\\frac{(N-n)!}{N}\\\\\n    &= \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)\\frac{K(K-1)\\cdots(K-x+1)}{N(N-1)\\cdots(N-x+1)}\\frac{(N-K)\\cdots(N-K-(n-x)+1)}{(N-x)\\cdots(N-x+1)}\n\\end{align*}\n\\]\nこのとき，\\(p = K/N\\) とすると，極限において\n\\[\n\\begin{gather*}\n\\frac{K(K-1)\\cdots(K-x+1)}{N(N-1)\\cdots(N-x+1)} \\approx p^x\\\\\n\\frac{(N-K)\\cdots(N-K-(n-x)+1)}{(N-x)\\cdots(N-x+1)}\\approx (1-p)^{n-x}\n\\end{gather*}\n\\]\n以上より\n\\[\n\\lim_{N,K\\to\\infty}\\Pr(X=x) = \\left(\\begin{array}{c}n\\\\ x\\end{array}\\right)p^x(1-p)^{n-x}\n\\]\n\n\n有限母集団からの非復元抽出と有限母集団修正\n有限母集団からの復元抽出は，i.i.d.確率変数が観測されるが，非復元抽出の場合は i.i.d.となりません． 大きさ \\(N\\) の有限母集団を考え，\\(X_i\\) を標本として抽出された観測値とします．なお，有限母集団に属する各個体の個体値を，\n\\[\na_1, a_2, \\cdots, a_N\n\\]\nとします．サイズ \\(n\\) の非復元抽出は，任意の互いに異なる \\(i_1, \\cdots, i_n\\) について，以下のように確率が定義される標本抽出方法です：\n\\[\n\\begin{align*}\n\\Pr(X_1 = a_{i_1}, \\cdots, X_1 = a_{i_n}) = \\frac{1}{N(N-1)\\cdots(N-n+1)}\n\\end{align*}\n\\]\n ▶  有限母集団の平均と分散\n有限母集団の平均と分散は\n\\[\n\\begin{gather*}\n\\mu = \\frac{1}{N}\\sum_{i=1}^Na_i\\\\\n\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N(a_i - \\mu)^2\n\\end{gather*}\n\\]\nと定義できます．\n ▶  標本平均の期待値と分散\n\\[\n\\begin{align*}\n\\mathbb E[\\overline{{X}}]\n    &= \\frac{1}{n}\\sum_{i=1}^n\\mathbb E[X_i]\\\\\n    &= \\mu\\\\\n\\\\\n\\operatorname{Var}(\\overline{{X}})\n    &= \\frac{1}{n^2}\\operatorname{Var}(\\sum_{i=1}^nX_i)\\\\\n    &=\\frac{1}{n^2}\\left[\\sum_{i=1}^n\\operatorname{Var}(X_i) + \\sum_{i\\neq j}\\operatorname{Cov}(X_i, X_j)\\right]\\\\\n    &=\\frac{1}{n^2}\\left[n\\operatorname{Var}(X_1) + n(n-1)\\operatorname{Cov}(X_1, X_2)\\right]\n\\end{align*}\n\\]\nここで，\n\\[\n\\begin{align*}\n&\\operatorname{Cov}(X_1, X_2)\\\\\n    &= \\mathbb E[X_1X_2] - \\mathbb E[X_1]\\mathbb E[X_2]\\\\\n    &= \\frac{1}{N(N-1)}\\sum_{i\\neq j}a_ia_j - \\left(\\frac{1}{N}\\sum_{i=1}^Na_i\\right)^2\\\\\n    &= \\frac{1}{N(N-1)}\\left[(\\sum_{i=1}^Na_i)^2 - \\sum_{i=1}^Na_i^2\\right]- \\left(\\frac{1}{N}\\sum_{i=1}^Na_i\\right)^2\\\\\n    &= \\frac{(\\sum_{i=1}^Na_i)^2}{N^2(N-1)} - \\frac{\\sum_{i=1}^Na_i^2}{N(N-1)}\\\\\n    &= -\\frac{1}{N(N-1)}\\left(\\sum_{i=1}^Na_i^2 - \\frac{1}{N}(\\sum_{i=1}^Na_i)^2\\right)\\\\\n    &= -\\frac{1}{N(N-1)}\\sum_{i=1}^N(a_i - \\overline(a))^2\\\\\n    &= -\\frac{\\sigma^2}{N-1}\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Var}(\\overline{{X}}) = \\frac{\\sigma^2}{n}\\frac{N-n}{N-1}\n\\]\n\n📘 REMARKS \nサイズ \\(N\\) の有限母集団からサイズ \\(n\\) の非復元無作為抽出を実施する場合，\n\\[\n\\{a_{i_1}, \\cdots, a_{i_n}\\}\n\\]\nという要素の重複を許した多重集合を一度抽出し，そこから改めて１個ずつ順に無作為に抜き出すという方法でも同じ抽出方法となります． ここから，\\(X_i\\) の周辺分布は \\(X_1\\) の周辺分布と同じになることが分かるし，また，\\((X_i, X_j)\\) の２次元同時分布は \\((X_1, X_2)\\) の２次元同時分布と同じであることがわかります．\n\n ▶  別解: 非復元抽出における \\(\\operatorname{Cov}(X_1, X_2)\\) の求め方\n\\[\n\\sum_{i=1}^N X_i = \\sum_{i=1}^N a_i\n\\]\nと定数であるので，\\(\\operatorname{Var}(\\sum_{i=1}^N X_i) = 0\\) となる．\n\\[\n\\begin{align*}\n\\operatorname{Var}(\\sum_{i=1}^N X_i)\n    &= \\sum_{i=1}^N \\operatorname{Var}(X_1) + \\sum_{i\\neq j}\\operatorname{Cov}(X_1, X_2)\\\\\n    &= N\\sigma^2 + N(N-1)\\operatorname{Cov}(X_1, X_2) = 0\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{Cov}(X_1, X_2) = -\\frac{\\sigma^2}{N-1}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\nExample 10.1 : 有限母集団修正 \n\n\n\nID\n身長\n\n\n\n\n1\n171.0\n\n\n2\n167.3\n\n\n3\n170.6\n\n\n4\n178.7\n\n\n5\n162.3\n\n\n\n上記のような５観測単位からなる有限母集団を考える．このとき，３人を非復元抽出でサンプリングしたとき， その標本平均の平均と分散は以下のようになる\n\nimport numpy as np\nfrom itertools import combinations as comb\n\nfinite_population = np.array([171.0, 167.3, 170.6, 178.7, 162.3])\nmeans_of_comb = list(\n    map(lambda x: np.mean(finite_population[np.array(x)]), comb(range(0, 5), 3))\n)\n\nprint(\"mean: {:.2f}, var: {:.2f}\".format(np.mean(means_of_comb), np.var(means_of_comb)))\n\nmean: 169.98, var: 4.79\n\n\n有限母集団修正を考えずに計算してみると\n\nbiased_var = np.var(finite_population) / 3\ncorrection = (5 - 3) / (5 - 1)\n\nprint(\n    \"biased-var: {:.2f}, corrected-var:{:.2f}\".format(\n        biased_var, correction * biased_var\n    )\n)\n\nbiased-var: 9.58, corrected-var:4.79\n\n\n以上のように，標本平均の分散について，有限母集団修正により正しい値が得られることが分かる．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>超幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/hypergeometric.html#多変量超幾何分布",
    "href": "posts/probability_distribution/hypergeometric.html#多変量超幾何分布",
    "title": "10  超幾何分布",
    "section": "多変量超幾何分布",
    "text": "多変量超幾何分布\n\nDef: 多変量超幾何分布 \n各個体が \\(c_1, c_2, \\cdots, c_k\\) のいずれかに所属するようなクラスサイズ \\(N\\) 有限母集団を考える(各 \\(c_i\\) のサイズは \\(C_i\\) とする)．つまり，\n\\[\n\\sum_{j=1}^kC_j = N\n\\]\nこの有限母集団から，サイズ \\(n\\) の非復元無作為抽出をする場合，その同時確率関数は\n\\[\n\\begin{gather*}\n\\Pr(X_1=x_1, \\cdots, X_k=x_k) = \\frac{\\left(\\begin{array}{c}C_1\\\\x_1 \\end{array}\\right)\\left(\\begin{array}{c}C_2\\\\x_2 \\end{array}\\right)\\cdots\\left(\\begin{array}{c}C_k\\\\ x_k \\end{array}\\right)}\n{\\left(\\begin{array}{c}N\\\\n \\end{array}\\right)}\\\\\n\\text{where } \\sum_{i=1}^kx_i = n\n\\end{gather*}\n\\]\nとなる．このとき，\\(k\\) 次元確率変数ベクトル \\(X\\) は \\(\\operatorname{Multi-hypergeometric}(N, (C_1, \\cdots, C_k), n)\\) に従う．\n\n ▶  周辺確率分布\n多変量超幾何分布の \\(X_i\\) についての周辺確率分布は，\\(X_i\\) 以外のグループをまとめてシンプルな超幾何分布とみなして考えることができるので\n\\[\n\\begin{gather*}\n\\Pr(X_i=x) = \\frac{\\left(\\begin{array}{c}C_i\\\\ x\\end{array}\\right)\\left(\\begin{array}{c}N-C_i\\\\ n-x\\end{array}\\right)}{\\left(\\begin{array}{c}N\\\\ n\\end{array}\\right)}\\\\\n\\text{where } \\max\\{0, n+C_i-N\\} \\leq x \\leq \\min\\{n, C_i\\}\n\\end{gather*}\n\\]\n ▶  期待値と分散\n\\[\n\\begin{gather*}\n\\mathbb E[X_i] = n\\frac{C_i}{N}\\\\\n\\operatorname{Var}(X_i) = n\\frac{C_i}{N}\\frac{N-C_i}{N}\\frac{N-n}{N-1}\\\\\n\\operatorname{Cov}(X_i, X_j) = -n\\frac{N-n}{N-1}\\frac{C_i}{N}\\frac{C_j}{N}\n\\end{gather*}\n\\]",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>超幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/hypergeometric.html#references",
    "href": "posts/probability_distribution/hypergeometric.html#references",
    "title": "10  超幾何分布",
    "section": "References",
    "text": "References\n\nLibreTexts Statistics &gt; The Multivariate Hypergeometric Distribution",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>超幾何分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/normal_dist.html",
    "href": "posts/probability_distribution/normal_dist.html",
    "title": "11  正規分布",
    "section": "",
    "text": "正規分布の性質\n\\(X\\sim N(\\mu, \\sigma^2)\\) について，標準化変換(standardization)\n\\[\nZ = \\frac{X-\\mu}{\\sigma}\n\\]\nを行うと，変数変換の公式より\n\\[\nf_Z(z) = \\sigma f_X(\\sigma z + \\mu)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\n\\]\nとなります．\\(N(0, 1)\\) のことを特に標準正規分布とよび，そのpdfを \\(\\phi(z)\\), CDFを \\(\\Phi(z)\\) で表します．\n\\(f_X(x)\\) の形状から，location parameter \\(\\mu\\) を中心に対称であることが分かる．つまり, \\(\\phi(z)\\) は \\(z=0\\) で対称であり\n\\[\n\\begin{gather*}\n\\Phi(0) = \\frac{1}{2}\\\\\n\\Phi(-z) = 1 - \\Phi(z)\n\\end{gather*}\n\\]\nがわかる．\n▶  \\(\\sigma\\) 範囲\n\\(X\\sim N(\\mu, \\sigma^2)\\) という確率分布を考えたとき，シグマ範囲の目安として以下が知られてます\n\\[\n\\begin{gather*}\n\\Pr(\\vert X - \\mu\\vert &gt; \\sigma) \\approx \\frac{1}{3}\\\\\n\\Pr(\\vert X - \\mu\\vert &gt; 2\\sigma) \\approx \\frac{1}{20}\\\\\n\\Pr(\\vert X - \\mu\\vert &gt; 3\\sigma) \\approx \\frac{3}{1000}\\\\\n\\Pr(\\vert X - \\mu\\vert &gt; 4\\sigma) \\approx \\frac{1}{10000}\n\\end{gather*}\n\\]\n大体の目安として, \\(3\\sigma\\) 範囲はいわゆる「千三つ」であることは覚えといて損はないと思います．\n▶  二項分布の極限分布としての正規分布\n\\(n\\) を正の整数として，\\(Y_n \\sim \\operatorname{Binom}(n, 1/2)\\) とし，\n\\[\nX_n = \\frac{Y_n - n/2}{\\sqrt{n}/2}\n\\]\nという確率変数を考えます．\\(X_n \\in (x - 2/\\sqrt{n}, x]\\) となるような確率を考えてみると\n\\[\n\\begin{align*}\n&\\Pr(x - 2/\\sqrt{n} &lt; X_n \\leq x)\\\\\n    &= \\Pr\\left(\\frac{\\sqrt{n}}{2}x + \\frac{n}{2} - 1 &lt; Y_n \\leq \\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\right)\\\\\n    &= \\Pr\\bigg(\\bigg\\lfloor Y_n = \\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rfloor\\bigg)\\\\\n    &= \\frac{n!}{\\bigg\\lfloor \\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rfloor ! \\bigg\\lceil -\\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rceil !}\\left(\\frac{1}{2}\\right)^n\n\\end{align*}\n\\]\nここでスターリングの公式より十分大きい正の整数 \\(m\\) について\n\\[\nm! \\approx \\sqrt{2\\pi m} m^m\\exp(-m)\n\\]\nと近似できるので\n\\[\n\\begin{align*}\n&\\lim_{n\\to\\infty}\\Pr(x - 2/\\sqrt{n} &lt; X_n \\leq x)\\\\\n    &= \\lim_{n\\to\\infty}\\frac{n!}{\\bigg\\lfloor \\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rfloor ! \\bigg\\lceil -\\frac{\\sqrt{n}}{2}x + \\frac{n}{2}\\bigg\\rceil !}\\left(\\frac{1}{2}\\right)^n\\\\\n    &= \\lim_{n\\to\\infty} \\frac{1}{\\sqrt{2\\pi\\left(\\frac{n}{4}-\\frac{x^2}{4}\\right)}\\left(1 - \\frac{x^2}{n}\\right)^{\\frac{n}{2}}\\left(1 + \\frac{x}{\\sqrt{n}}\\right)^{\\frac{\\sqrt{n}}{2}x}\\left(1 - \\frac{x}{\\sqrt{n}}\\right)^{-\\frac{\\sqrt{n}}{2}x}}\\left(\\frac{1}{2}\\right)^n\\\\\n    &= \\frac{1}{\\sqrt{2\\pi}\\exp(-x^2/2)\\exp(x^2/2)\\exp(x^2/2)}\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\exp(-x^2/2)\\\\[8pt]\n    &= \\phi(x)\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n▶  標準正規分布のn次モーメントについて\n標準正規分布について，\\(\\phi(z)\\) が偶関数であることから，\\(n\\) が奇数のときは\n\\[\n\\mathbb E[X^n] = 0\n\\]\nであることはすぐに分かります．一方，\\(l\\) を自然数として, \\(n=2l\\) と表せるときは\n\\[\n\\begin{align*}\n\\mathbb E[X^n] &= \\frac{1}{\\sqrt{2\\pi}}\\int^{\\infty}_{-\\infty}x^n\\exp\\bigg(-\\frac{x^2}{2}\\bigg)\\mathrm{d}x\\\\[3pt]\n               &= \\frac{2}{\\sqrt{2\\pi}}\\int^{\\infty}_{0}x^{2l}\\exp\\bigg(-\\frac{x^2}{2}\\bigg)\\mathrm{d}x\\\\[3pt]\n\\end{align*}\n\\]\n\\(x^2/2 = u\\) と変数変換を行うと，\\(\\frac{\\mathrm{d}x}{\\mathrm{d}u}=\\frac{1}{\\sqrt{2u}}\\) より\n\\[\n\\begin{align*}\n\\mathbb E[X^n] &= \\frac{2^{l+1}}{\\sqrt{2\\pi}}\\int^{\\infty}_{0}u^{l}\\exp(-u)\\frac{1}{\\sqrt{2u}}\\mathrm{d}u\\\\[3pt]\n               &= \\frac{2^l}{\\sqrt{\\pi}}\\int^{\\infty}_{0}u^{l-\\frac{1}{2}}\\exp(-u)\\mathrm{d}u\\\\[3pt]\n               &= \\frac{2^l}{\\sqrt{\\pi}}\\int^{\\infty}_{0}u^{l+\\frac{1}{2}-1}\\exp(-u)\\mathrm{d}u\\\\[3pt]\n               &= \\frac{2^l}{\\sqrt{\\pi}}\\Gamma\\bigg(l+\\frac{1}{2}\\bigg)\n\\end{align*}\n\\]\n\\(\\Gamma(1/2) = \\sqrt{\\pi}\\) であることに留意すると\n\\[\n\\begin{align*}\n\\Gamma\\left(\\frac{3}{2}\\right) &= \\frac{1}{2}\\Gamma\\left(\\frac{1}{2}\\right)\\\\\n\\Gamma\\left(\\frac{5}{2}\\right) &= \\frac{3}{2}\\Gamma\\left(\\frac{3}{2}\\right)\\\\\n                               &= \\frac{1 \\times 3}{2^2}\\sqrt{\\pi}\n\\end{align*}\n\\]\nになるので\n\\[\n\\begin{align*}\n\\mathbb E[X^n]\n    &= 1\\times 3\\times 5\\times\\cdots\\times (2l-1)\\\\\n    &= \\prod_{i=1}^l(2i-1)           \n\\end{align*}\n\\]\nまたはこれを変形して，\n\\[\n\\frac{1}{2l!}\\mathbb E[X^{2l}] = \\prod^l_{k=1}\\frac{1}{2k}\n\\]\nと表すこともできます．",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>正規分布</span>"
    ]
  },
  {
    "objectID": "posts/probability_distribution/normal_dist.html#正規分布の性質",
    "href": "posts/probability_distribution/normal_dist.html#正規分布の性質",
    "title": "11  正規分布",
    "section": "",
    "text": "Def: 正規分布 \n確率変数 \\(X\\) が平均と分散 \\(\\mu, \\sigma^2\\) をもつ正規分布に従う，つまり \\(X \\sim N(\\mu, \\sigma^2)\\) のとき，\\(X\\) の確率密度関数 \\(f_X(x)\\) は\n\\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}, \\quad -\\infty&lt;x&lt;\\infty\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 11.1 \n\\[\n\\int^\\infty_{-\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\mathrm{d}x = 1\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(z = \\frac{x-\\mu}{\\sigma}\\) と変数変換をすると\n\\[\n\\int^\\infty_{-\\infty}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\\mathrm{d}z = 1\n\\]\nが示せれば良い．\n\\[\nI = \\int^\\infty_{-\\infty}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\\mathrm{d}z\n\\]\nとおくと，\n\\[\n\\begin{align*}\nI^2 &= \\left(\\int^\\infty_{-\\infty}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\\mathrm{d}z\\right)^2\\\\\n    &= \\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}\\exp\\left\\{-\\frac{a^2 + b^2}{2}\\right\\}\\mathrm{d}a\\mathrm{d}b\n\\end{align*}\n\\]\nここで，\\(a=r\\cos\\theta, b= r\\sin\\theta\\) と極座標変換を行う. ヤコビアン \\(J\\) は\n\\[\n\\begin{align*}\n\\vert J\\vert\n    &= \\bigg\\vert \\frac{\\partial a}{\\partial r}\\frac{\\partial b}{\\partial \\theta} - \\frac{\\partial a}{\\partial \\theta}\\frac{\\partial b}{\\partial r}\\bigg\\vert\\\\\n    &= r\n\\end{align*}\n\\]\nより \\[\n\\begin{align*}\nI^2 &= \\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}\\exp\\left\\{-\\frac{a^2 + b^2}{2}\\right\\}\\mathrm{d}a\\mathrm{d}b\\\\\n    &= \\int^\\infty_{0}\\int^{2\\pi}_{0}\\exp\\left\\{-\\frac{r^2}{2}\\right\\}r \\mathrm{d}\\theta\\mathrm{d}r\\\\\n    &= 2\\pi \\int^\\infty_{0}\\exp\\left\\{-\\frac{r^2}{2}\\right\\}r \\mathrm{d}r\\\\\n    &= 2\\pi \\left[\\exp\\left\\{-\\frac{r^2}{2}\\right\\}\\right]^0_\\infty\\\\\n    &= 2\\pi\n\\end{align*}\n\\]\n以上より, \\(I = \\sqrt{2\\pi}\\) を得る．\n\\[\n\\begin{align*}\n\\int^\\infty_{-\\infty}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{z^2}{2}\\right\\}\\mathrm{d}z\n    &= \\frac{1}{\\sqrt{2\\pi}}I\\\\\n    &= 1\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 11.2 \n標準正規分布の確率密度関数を \\(\\phi(x)\\) とするとき，\n\\[\n\\begin{gather*}\n\\int_\\mathbb R x\\phi(x) \\mathrm{d}x = 0\\\\\n\\int_\\mathbb R x^2\\phi(x) \\mathrm{d}x = 1\\\\\n\\end{gather*}\n\\]\nが成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(\\phi(x)\\) が偶関数，\\(x\\) が奇関数より, \\(x\\phi(x)\\) は奇関数になる．従って，\n\\[\n\\int_\\mathbb R x\\phi(x) \\mathrm{d}x = 0\n\\]\n2次モーメントについては\n\\[\n\\begin{align*}\n\\int_\\mathbb R x^2\\phi(x) \\mathrm{d}x\n    &=  2\\int_0^\\infty x^2\\phi(x) \\mathrm{d}x\\\\\n    &= 2[-x\\exp(-x^2/2)]^\\infty_0 + 2\\int^\\infty_0 \\phi(x) \\mathrm{d}x\\\\\n    &= 0 + 2 \\times \\frac{1}{2}\\\\\n    &= 1\n\\end{align*}\n\\]\n\n\n\n\nExample 11.1 : 標準正規分布の4次モーメントの導出 \n\\(X\\sim N(0, 1)\\) の4次モーメントについて，ガンマ関数 \\(\\Gamma(1/2) = \\sqrt{\\pi}\\) を用いて以下のように計算できます．\n\\[\n\\begin{align*}\n\\mathbb E[X^4]\n    &= \\frac{1}{\\sqrt{2\\pi}}\\int^\\infty_{-\\infty}x^4\\exp\\left(-\\frac{x^2}{2}\\right)\\mathrm{d}x\\\\\n    &= \\frac{2}{\\sqrt{2\\pi}}\\int^\\infty_{0}x^4\\exp\\left(-\\frac{x^2}{2}\\right)\\mathrm{d}x \\quad\\because \\text{偶関数より}\n\\end{align*}\n\\]\nここで, \\(x^2 = u\\) という変数変換を行う．\n\\[\n\\begin{align*}\n\\frac{2}{\\sqrt{2\\pi}}\\int^\\infty_{0}x^4\\exp\\left(-\\frac{x^2}{2}\\right)\\mathrm{d}x\n    &= \\frac{2}{\\sqrt{2\\pi}}\\int^\\infty_{0}u^2 \\exp\\left(-\\frac{u}{2}\\right)\\frac{u^{-1/2}}{2}\\mathrm{d}u\\\\\n    &= \\frac{1}{\\sqrt{2\\pi}}\\int^\\infty_{0}u^{\\frac{5}{2}-1} \\exp\\left(-\\frac{u}{2}\\right)\\mathrm{d}u\\\\\n    &= \\frac{1}{\\sqrt{2\\pi}} \\Gamma\\left(\\frac{5}{2}\\right)\\left(\\frac{1}{2}\\right)^{-5/2}\\\\\n    &= \\frac{\\sqrt{\\pi} \\frac{1}{2}\\frac{3}{2}}{\\sqrt{2\\pi}}2^{5/2}\\\\\\\\\n    &= 3\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMGFと特性関数\n\n\nTheorem 11.3 : 標準正規分布の積率母関数と特性関数 \n\\(X\\sim N(0, 1)\\) としたとき，積率母関数 \\(M_Z(t)\\) 及び，特性関数 \\(\\varphi_Z(t)\\) は以下のようになる\n\\[\n\\begin{align*}\nM_Z(t) &= \\exp(t^2/2)\\\\\n\\varphi_Z(t)&= \\exp(-t^2/2)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\nM_Z(t)\n    &= \\mathbb E[\\exp(tZ)]\\\\\n    &= \\frac{1}{\\sqrt{2\\pi}}\\int^\\infty_{-\\infty}\\exp(tz-z^2/2)\\mathrm{d}z\\\\\n    &= \\exp(t^2/2)\\frac{1}{\\sqrt{2\\pi}}\\int^\\infty_{-\\infty}\\exp(-(t-z)^2/2)\\mathrm{d}z\\\\\n    &= \\exp(t^2/2)\n\\end{align*}\n\\]\n特性関数は \\(\\varphi(t) = M_Z(it)\\) より \\(\\varphi(t) = \\exp(-t^2/2)\\) とわかるが，以下のように計算することもできる．\n\\[\n\\begin{align*}\n\\varphi_Z(t)\n    &= \\mathbb E[\\exp(itZ)]\\\\[5pt]\n    &= \\mathbb E[\\cos(tZ) + i\\sin(tZ)]\\\\[5pt]\n    &= \\mathbb E[\\cos(tZ)] + i\\mathbb E[\\sin(tZ)]\\\\[5pt]\n    &= \\int^\\infty_{-\\infty}\\cos(tz)\\phi(z)\\mathrm{d}z + i\\int^\\infty_{-\\infty}\\sin(tz)\\phi(z)\\mathrm{d}z\\\\[5pt]\n    &= \\int^\\infty_{-\\infty}\\cos(tz)\\phi(z)\\mathrm{d}z \\quad \\because\\text{奇関数}\n\\end{align*}\n\\]\n次に，\\(t\\) について \\(\\varphi_Z(t)\\) を微分すると\n\\[\n\\begin{align*}\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\varphi_Z(t)\n    &= \\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb E[\\exp(itZ)]\\\\[5pt]\n    &= \\mathbb E\\left[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\exp(itZ)\\right] \\because\\text{期待値と微分の順序交換性}\\\\[5pt]\n    &= i\\mathbb E\\left[Z\\exp(itZ)\\right]\\\\\n    &= -\\int^\\infty_{-\\infty}z\\sin(tz)\\phi(z)\\mathrm{d}z\n\\end{align*}\n\\]\nこのとき，\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{z}}\\phi(z) = -z\\phi(z)\n\\]\nであるので\n\\[\n\\begin{align*}\n-\\int^\\infty_{-\\infty}z\\sin(tz)\\phi(z)\\mathrm{d}z\n    &= [\\sin(tz)\\phi(z)]^\\infty_{-\\infty} - \\int^\\infty_{-\\infty}t\\cos(tz)\\phi(z)\\mathrm{d}z\\\\\n    &= - \\int^\\infty_{-\\infty}t\\cos(tz)\\phi(z)\\mathrm{d}z\\\\\n    &= -t\\varphi_Z(t)\n\\end{align*}\n\\]\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\varphi_Z(t)=-t\\varphi_Z(t)\n\\]\n\\(\\varphi_Z(0) = 1\\) より，\n\\[\n\\varphi_Z(t) = \\exp(-t^2/2)\n\\]\n\n\n\n\n\nTheorem 11.4 : MGF of non-standard normal distribution \n\\(X \\sim N(\\mu, \\sigma^2)\\) の積率母関数および特性関数は\n\\[\n\\begin{align*}\nM_X(t) &= \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2t^2\\right)\\\\\n\\varphi_X(t) &= \\exp\\left(i\\mu t-\\frac{t^2\\sigma^2}{2}\\right)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(Z\\sim N(0, 1)\\) とすると，\\(Z = \\sigma Z + \\mu\\) と表せるので，\n\\[\n\\begin{align*}\nM_X(t) &= \\mathbb E[\\exp(t(\\sigma Z + \\mu))]\\\\[5pt]\n       &= \\exp(t\\mu) \\mathbb E[\\exp(t\\sigma Z)]\\\\[5pt]\n       &= \\exp(t\\mu)\\exp\\left(\\frac{1}{2}t^2\\sigma^2\\right)\\\\\n       &=  \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2t^2\\right)\n\\end{align*}\n\\]\n同様に\n\\[\n\\begin{align*}\n\\varphi_X(t)\n    &= \\mathbb E[\\exp(it(\\sigma z + \\mu))]\\\\[5pt]\n    &= \\exp(it\\mu) \\mathbb E[\\exp(it\\sigma z)]\\\\[5pt]\n    &= \\exp\\left(i\\mu t-\\frac{t^2\\sigma^2}{2}\\right)\n\\end{align*}\n\\]\n\n\n\n\n\nTheorem 11.5 \n\\(X\\sim N(\\mu, \\sigma^2)\\) とする．定数 \\(a, b\\) に対して\n\\[\nY = aX + b\n\\]\nとしたとき，\\(Y\\sim N(a\\mu + b, a^2\\sigma^2)\\) となる．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(N(a\\mu + b, a^2\\sigma^2)\\) に従う確率変数の特性関数は\n\\[\n\\varphi(t) = \\exp\\left[(a\\mu + b)it - \\frac{a^2\\sigma^2t^2 }{2}\\right]\n\\]\nなので，\\(Y\\) の特性関数がこれと一致することを示せば良い．\n\\[\n\\begin{align*}\n\\varphi_Y(t)\n    &= \\mathbb E\\left[\\exp(itY)\\right]\\\\[5pt]\n    &= \\mathbb E\\left[\\exp(it(aX + b))\\right]\\\\[5pt]\n    &= \\exp(itb)\\exp\\left(i a\\mu t - \\frac{a^2\\sigma^2t^2}{2}\\right)\\\\\n    &=  \\exp\\left[(a\\mu + b)it - \\frac{a^2\\sigma^2t^2 }{2}\\right]\n\\end{align*}\n\\]\n\n\n\n\n\n正規分布の再生性\n\nDef: 確率分布の再生性 \n確率分布 \\(F\\) について，2 つの独立な確率変数 \\(X, Y\\) が \\(F\\) に従うとする．このとき，\n\\[\n\\begin{align*}\nZ &= X + Y\\\\\nZ& \\sim F\n\\end{align*}\n\\]\nが成立するとき，確率分布 \\(F\\) は再生性をもつという．\n\n二項分布，負の二項分布，ポアソン分布，正規分布などは，再生性を持つことがしられています．\n\n\nTheorem 11.6 : 正規分布の再生性 \n正規分布は，location parameter, scale parameter両方について再生性を持つ．つまり，\n\\[\n\\begin{align*}\n&X \\sim N(\\mu_x, \\sigma^2_x), Y \\sim N(\\mu_y, \\sigma^2_y)\\\\\n\\Rightarrow& X +Y \\sim N(\\mu_x+\\mu_y, \\sigma^2_x + \\sigma^2_y)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof: MGFを用いた証明\n\n\n\n\n\n確率変数 \\(X, Y\\) は独立なので\n\\[\n\\begin{align*}\nM_{X+Y}(t)\n    &= M_{X}(t) M_{Y}(t)\\\\\n    &= \\exp\\left(\\mu_xt + \\frac{\\sigma^2_xt}{2}\\right)\\exp\\left(\\mu_yt + \\frac{\\sigma^2_yt}{2}\\right)\\\\\n    &= \\exp\\left((\\mu_x + \\mu_y)t + \\frac{(\\sigma^2_x + \\sigma^2_y)t}{2}\\right)\n\\end{align*}\n\\]\n\\(X+Y\\) のMGFが \\(N(\\mu_x + \\mu_y, \\sigma^2_x + \\sigma^2_y)\\) のMGFと一致するので\n\\[\nX+Y\\sim N(\\mu_x + \\mu_y, \\sigma^2_x + \\sigma^2_y)\n\\]\n\n\n\n\n\n\n\n\n\nProof: 畳み込みを用いた証明\n\n\n\n\n\n確率変数 \\(X, Y\\) のそれぞれの密度関数を \\(f_X(x), f_Y(y)\\) で表したとき，\\(Z =X+Y\\) の確率密度関数 \\(p(z)\\) は畳み込みにより以下のように表せます．\n\\[\np(z) = \\int^\\infty_{-\\infty} f_X(x)f_Y(z-x)\\mathrm{d}x\n\\]\n従って，\n\\[\n\\begin{align*}\np(z)\n    &= \\frac{1}{2\\pi\\sigma_X\\sigma_Y}\\int^\\infty_{-\\infty}\\exp\\left(-\\frac{(x -\\mu_x)^2}{2\\sigma_X^2}-\\frac{((z-x) -\\mu_y)^2}{2\\sigma_Y^2}\\right)\\mathrm{d}x\n\\end{align*}\n\\]\nここで，最終項の指数部分について，\\(x\\) についてまとめると\n\\[\n\\begin{align*}\n&-\\frac{(x -\\mu_x)^2}{2\\sigma_X^2}-\\frac{((z-x) -\\mu_y)^2}{2\\sigma_Y^2}\\\\\n    &= -\\frac{\\sigma_x^2 + \\sigma_y^2}{2\\sigma_x^2\\sigma_y^2}\\left(x - \\frac{z\\sigma_x^2 -\\mu_x\\sigma_y^2 + \\mu_y\\sigma_x^2}{\\sigma_x^2 + \\sigma_y^2}\\right)^2 - \\frac{(z - (\\mu_x+\\mu_y))^2}{2(\\sigma_x^2 + \\sigma_y^2)}\n\\end{align*}\n\\]\nここでガウス積分より，\n\\[\n\\int^\\infty_{-\\infty}\\exp\\left(-\\frac{\\sigma_x^2 + \\sigma_y^2}{2\\sigma_x^2\\sigma_y^2}\\left(x - C\\right)^2 \\right)\\mathrm{d}x = \\sqrt{\\frac{2\\pi\\sigma_x^2\\sigma_y^2}{\\sigma_x^2 + \\sigma_y^2}}\n\\]\n以上より，\n\\[\np(z) = \\frac{1}{\\sqrt{2\\pi\\sigma_x^2\\sigma^2_y}}\\exp\\left(-\\frac{(z - (\\mu_x+\\mu_y))^2}{2(\\sigma_x^2 + \\sigma_y^2)}\\right)\n\\]\nこれは，\\(N(\\mu_x + \\mu_y, \\sigma^2_x + \\sigma^2_y)\\) の確率密度関数と一致するので，\n\\[\nX+Y\\sim N(\\mu_x + \\mu_y, \\sigma^2_x + \\sigma^2_y)\n\\]\n\n\n\n\n\nDifferential Entropy\n\nDef: 微分エントロピー \n連続確率変数 \\(X\\) について，確率密度関数が \\(p(x)\\) で与えられているとする．このとき，微分エントロピーは以下の形で定義される\n\\[\n\\mathrm{H}(X) = - \\int_{\\mathcal{X}} p(x) \\log_b p(x) \\, \\mathrm{d}x\n\\]\nなお，\\(b\\) は通常 \\(2, e\\) が用いられる\n\n平均 \\(\\mu\\), 分散 \\(\\sigma^2\\) をもつ確率分布のうち，正規分布は微分エントロピーを最大にする分布として知られています．\n ▶  \\(N(\\mu, \\sigma^2)\\) の微分エントロピー\n\\(N(\\mu, \\sigma^2)\\) の確率密度関数を \\(f(x)\\) として，微分エントロピーの定義より\n\\[\n\\begin{align*}\n\\mathrm{H}\n    &= - \\int_{-\\infty}^\\infty f(x) \\log_2 f(x) \\, \\mathrm{d}x\\\\\n    &= - \\int_{-\\infty}^\\infty f(x) \\log_2 \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\right) \\, \\mathrm{d}x\\\\\n    &= \\frac{\\log_2(2\\pi\\sigma^2)}{2}\\int_{-\\infty}^\\infty f(x)\\mathrm{d}x + \\frac{\\log_2 e}{2\\sigma^2} \\int_{-\\infty}^\\infty (x-\\mu)^2f(x)\\mathrm{d}x\\\\\n    &= \\frac{1}{2}\\log_2(2\\pi e\\sigma^2)\n\\end{align*}\n\\]\nなお，自然対数で表現する場合，\\(\\mathrm{H} = \\frac{1}{2}[1 + \\ln(2\\pi\\sigma^2)]\\)\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\nTheorem 11.7 \n平均 \\(\\mu\\), 分散 \\(\\sigma^2\\) をもつ確率分布のうち，正規分布は微分エントロピーを最大にする分布である．\n\n\n\n\n\n\n\n\nProof: ラグランジュの未定係数法\n\n\n\n\n\n（注意: 制約付き最大化問題を解くにあたって，ラグランジュの未定係数法が使用できると仮定してます）\n ▶  汎関数の定義\n\\[\n\\begin{align*}\n&F(p(x), \\lambda_1, \\lambda_2, \\lambda_3)\\\\\n&= -\\int^\\infty_{-\\infty}p(x)\\ln(p(x))\\,\\mathrm{d}x + \\lambda_1\\left[\\int^\\infty_{-\\infty}p(x)\\,\\mathrm{d}x-1\\right]\\\\\n&\\qquad+\\lambda_2\\int^\\infty_{-\\infty}(x-\\mu)p(x)\\,\\mathrm{d}x + \\lambda_3\\left[\\int^\\infty_{-\\infty}(x-\\mu)^2p(x)\\,\\mathrm{d}x-\\sigma^2\\right]\\\\\n\\end{align*}\n\\]\n ▶  極値条件の計算\n\n\\[\n\\begin{align*}\n\\frac{\\partial F}{\\partial p(x)} &= -\\int^\\infty_{-\\infty}(1 + \\ln(p(x)) - \\lambda_1 - \\lambda_2(x-\\mu) - \\lambda_3(x-\\mu)^2)\\,\\mathrm{d}x = 0\\tag{A}\\\\\n\\frac{\\partial F}{\\partial \\lambda_1} &= \\int^\\infty_{-\\infty}p(x)\\,\\mathrm{d}x-1 = 0\\tag{B}\\\\\n\\frac{\\partial F}{\\partial \\lambda_2} &= \\int^\\infty_{-\\infty}(x-\\mu)p(x)\\,\\mathrm{d}x = 0\\tag{C}\\\\\n\\frac{\\partial F}{\\partial \\lambda_3} &= \\int^\\infty_{-\\infty}(x-\\mu)^2p(x)\\,\\mathrm{d}x-\\sigma^2= 0\\tag{D}\n\\end{align*}\n\\]\n\n ▶  条件(A)の整理\n条件 \\(\\mathrm{(A)}\\) より以下を得る\n\n\\[\np(x) = \\exp(-1+\\lambda_1 + \\lambda_2(x-\\mu)+ \\lambda_3(x-\\mu)^2)\\tag{E}\n\\]\n\nなお扱いやすいように \\(z = x - \\mu\\) として以下の形で表す．\n\n\\[\np(x) = \\exp(-1+\\lambda_1 + \\lambda_2z+ \\lambda_3z^2)\\tag{E'}\n\\]\n\n ▶  \\(\\lambda_1\\) の消去\n\\(\\mathrm{(E')}\\) を \\(\\mathrm{(B)}\\) に代入すると，\\(\\frac{\\,\\mathrm{d}x}{\\,\\mathrm{d}z}=1\\) より\n\\[\n\\begin{align*}\n&\\int^\\infty_{-\\infty}\\exp(-1 + \\lambda_1 + + \\lambda_2z+ \\lambda_3z^2)\\,\\mathrm{d}z\\\\\n&=\\exp(-1+\\lambda_1)\\exp\\left(-\\frac{\\lambda^2}{4\\lambda_3}\\right) \\int^\\infty_{-\\infty}\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\,\\mathrm{d}z\\\\\n&=1\n\\end{align*}\n\\]\nこのとき，等号が成立するためには \\(\\lambda_3 &lt; 0\\) が必要であることが分かる．また，ガウス積分より\n\n\\[\n\\int^\\infty_{-\\infty}\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\,\\mathrm{d}z = \\frac{\\sqrt{\\pi}}{\\sqrt{-\\lambda_3}}\\tag{F}\n\\]\n\n従って，\n\n\\[\np(x) =  \\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\tag{G}\n\\]\n\n ▶  \\(\\lambda_2\\) の消去\n\\(\\mathrm{(G)}\\) と \\(\\mathrm{(C)}\\) より\n\n\\[\n\\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\int^\\infty_{-\\infty}z\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\,\\mathrm{d}z = 0\\tag{H}\n\\]\n\n\\(\\mathrm{(H)} = 0\\) が成立するためには，\\(\\exp\\left(\\lambda_3\\left(z + \\frac{\\lambda_2}{2\\lambda_3}\\right)^2\\right)\\) が偶関数になる必要があるので\n\\[\n\\begin{gather*}\n\\frac{\\lambda_2}{2\\lambda_3} = 0\\\\\n\\Rightarrow \\lambda_2 = 0\n\\end{gather*}\n\\]\n従って，\n\n\\[\np(x) = \\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\exp\\left(\\lambda_3z^2\\right)\\tag{I}\n\\]\n\n ▶  \\(\\lambda_3\\) の消去\n\\(\\mathrm{(I)}, \\mathrm{(D)}\\) を整理すると\n\\[\n\\begin{align*}\n\\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\int^\\infty_{-\\infty} z^2\\exp\\left(\\lambda_3z^2\\right)\\,\\mathrm{d}z = \\sigma^2\n\\end{align*}\n\\]\n\\(\\int^\\infty_{-\\infty}x^2\\exp(-ax^2)\\mathrm{d}x = \\frac{\\sqrt{\\pi}}{2a\\sqrt{a}}\\) より\n\\[\n\\begin{align*}\n\\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\int^\\infty_{-\\infty} z^2\\exp\\left(\\lambda_3z^2\\right)\\,\\mathrm{d}z\n    &= \\frac{\\sqrt{-\\lambda_3}}{\\sqrt{\\pi}}\\frac{\\sqrt{\\pi}}{-2\\lambda_3\\sqrt{-\\lambda_3}}\\\\\n    &= \\frac{1}{-2\\lambda_3}\n\\end{align*}\n\\]\n従って，\n\\[\n\\lambda_3 = -\\frac{1}{2\\sigma^2}\n\\]\n以上より, 微分エントロピーを最大化する \\(p^*(x)\\) は\n\\[\np^*(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\nとなり，正規分布となることが分かる．\n\n\n\n\n📘 REMARKS \n\n\\(\\lambda_2 = 0\\) より location paramter \\(\\mu\\) を変化させても，微分エントロピーは限界的には増えないことが分かります\n\\(\\lambda_3 &lt; 0\\) より scale paramter \\(\\sigma^2\\) を増大させると，微分エントロピーは限界的に増大することも分かります",
    "crumbs": [
      "確率分布",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>正規分布</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_101/fisher_exact_test.html",
    "href": "posts/statistical_hypothesis_test_101/fisher_exact_test.html",
    "title": "12  Fisher’s exact test",
    "section": "",
    "text": "\\(2\\times 2\\)クロスセル表とFisher’s exact test\n各グループの合計という周辺の値が固定されていると考えたとき，(Treated, Positive)の人数という確率変数が従う分布は超幾何分布とみなすことができる． つまり，\nとしたとき，\\(X_{11}\\)の確率は\n\\[\n\\begin{align*}\n\\Pr(X_{11}=x) &= \\frac{{}_{x_{1\\cdot}}C_{x}\\times {}_{x_{2\\cdot}}C_{x_{\\cdot 1} - x} }{{}_{N}C_{x_{\\cdot 1}}}\\\\\n              &= \\frac{x_{\\cdot 1}!x_{\\cdot 2}!x_{2\\cdot}!x_{2\\cdot}!}{x_{11}!x_{12}!x_{21}!x_{22}!N!}\n\\end{align*}\n\\]\nこのとき，\\(x\\) の範囲は \\(\\max(0, x_{1\\cdot} - x_{2\\cdot}) \\leq x \\leq \\min(x_{1\\cdot}, x_{\\cdot 1})\\) になる．\n▶  Null hypothesis vs Alternative hypothesis\n上記の問題設定におけるFisher’s exact testにおける検定仮説設定例はと，両側検定ならば\n\\(H_0\\)の仮定の下では，\\(X_{11}\\)は超幾何分布(hypergeometric distribution)に従うはずなので，この仮定に基づいてP値を計算します．両側検定でのP値の計算方法例として\n\\[\n\\begin{align*}\n\\text{p-value} = \\sum_{x} \\Pr(X_{11}={x}) \\text{ s.t } \\{x \\vert \\Pr(X_{11}={x}) \\leq \\Pr(X_{11}={x_{11}})\\}\n\\end{align*}\n\\]",
    "crumbs": [
      "統計的仮説検定",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_101/fisher_exact_test.html#times-2クロスセル表とfishers-exact-test",
    "href": "posts/statistical_hypothesis_test_101/fisher_exact_test.html#times-2クロスセル表とfishers-exact-test",
    "title": "12  Fisher’s exact test",
    "section": "References",
    "text": "問題設定 \nある医薬品試験のRCTにて，５０人の患者を無作為にtreatedとプラセボ(control)に分けて，一定期間後の健康状態(Positive vs Negative)を確認したところ 以下のような結果になった．\n\n\n\n\n\n \n\n\n\n\n\n\nTreated\n\n\n\n\nControl\n\n\n\n\n合計\n\n\n\n\n\n\nPositive\n\n\n\n\n21\n\n\n\n\n15\n\n\n\n\n36\n\n\n\n\n\n\nNegative\n\n\n\n\n4\n\n\n\n\n10\n\n\n\n\n14\n\n\n\n\n\n\n合計\n\n\n\n\n25\n\n\n\n\n25\n\n\n\n\n50\n\n\n\n\n\nこのとき，プラセボとグループと医薬品投入グループ間で健康状態分布が異なるかどうか検定したい．\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nTreated\n\n\n\n\nControl\n\n\n\n\n合計\n\n\n\n\n\n\nPositive\n\n\n\n\n\\(X_{11}\\)\n\n\n\n\n\\(X_{12}\\)\n\n\n\n\n\\(x_{1\\cdot}\\)\n\n\n\n\n\n\nNegative\n\n\n\n\n\\(X_{21}\\)\n\n\n\n\n\\(X_{22}\\)\n\n\n\n\n\\(x_{2\\cdot}\\)\n\n\n\n\n\n\n合計\n\n\n\n\n\\(x_{\\cdot 1}\\)\n\n\n\n\n\\(x_{\\cdot 2}\\)\n\n\n\n\n\\(N\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0\\): 処置(Treatment)と一定期間後の健康状態(主要評価項目)は独立\n\\(H_1\\): 処置(Treatment)と一定期間後の健康状態(主要評価項目)は独立ではない \\(\\Rightarrow \\Pr(\\text{Positive}\\vert \\text{Treated})\\neq \\Pr(\\text{Positive}\\vert \\text{Control})\\)\n\n\n\n\n\n\nCode\nimport math\nimport numpy as np\nimport polars as pl\nimport plotly.express as px\n\n\ndef compute_prob(\n    x: int, positive: int, negative: int, treated: int, denom: int\n) -&gt; np.float64:\n    return math.comb(positive, x) * math.comb(negative, treated - x) / denom\n\n\nDENOM = math.comb(50, 25)\nX_DOMAIN = np.arange(11, 25)\n\nprob = list(\n    map(\n        lambda x: compute_prob(x, positive=36, negative=14, treated=25, denom=DENOM),\n        X_DOMAIN,\n    )\n)\n\n# create polars.DataFrame\ndf = pl.DataFrame({\"x\": X_DOMAIN, \"prob\": prob})\n\n# plotly\nfig = px.bar(df, x=\"x\", y=\"prob\", title=\"Null hypothesis下における確率分布\")\nfig.update_layout(\n    xaxis_title=\"TreatedにおけるPositiveの人数\", yaxis_title=\"probability\"\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\nexact p-valueの計算\n ▶  片側検定\n\\(X_{11} \\geq x\\) となる場合のp-valueをscipy.stats.fisher_exactで計算すると以下のようになります．\n\nfrom scipy.stats import fisher_exact\ntable = np.array([[21, 15], [4, 10]])\nres_greater = fisher_exact(table, alternative='greater')\nprint(\"scipy-p-value: {:.6f}\".format(res_greater.pvalue))\n\nscipy-p-value: 0.056829\n\n\n一方，上で計算したprobabilityに則って上側確率を見てみると\n\nprint(\"self-computed-pvalue: {:.6f}\".format(df.filter((pl.col(\"x\") &gt;= 21))['prob'].sum()))\n\nself-computed-pvalue: 0.056824\n\n\nと数値計算誤差を無視してしまえば大まかに一致することが確認できます．\n ▶  両側検定\n両側検定におけるp-valueは\n\\[\n\\begin{align*}\n\\text{p-value} = \\sum_{x} \\Pr(X_{11}={x}) \\text{ s.t } \\{x \\vert \\Pr(X_{11}={x}) \\leq \\Pr(X_{11}={x_{11}})\\}\n\\end{align*}\n\\]\nなので\n\nthreshold = df.filter((pl.col(\"x\") == 21))[\"prob\"].to_numpy()[0]\nres_twosided = fisher_exact(table, alternative=\"two-sided\")\nmyres_twosided = df.filter((pl.col(\"prob\") &lt;= threshold))[\"prob\"].sum()\nprint(\"\"\"scipy-p-value: {:.6f},self-computed-pvalue: {:.6f}\n      \"\"\".format(res_twosided.pvalue, myres_twosided))\n\nscipy-p-value: 0.113657,self-computed-pvalue: 0.113652\n      \n\n\nどちらの計算でもおよそ \\(11.37\\%\\) であることが確認できます．\n\n📘 REMARKS \n\n組み合わせの数が大きすぎ，exact p-valueの計算が難しい場合はMonte Carlo法を用いて計算します\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTreatment\nT\nT\nT\nT\nT\nC\nC\nC\nC\nC\n\n\nPromoted\n1\n1\n1\n1\n0\n1\n1\n0\n0\n0\n\n\n\nというTreatedのうち４人がPromotedされたデータが得られた場合，TreatedかつPromotedの人数を \\(X\\) としたとき， \\(\\Pr(X \\geq 4)\\) のついて計算参する場合は\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTreatment\nT\nT\nT\nT\nT\nC\nC\nC\nC\nC\n\n\nPromoted\n1\n0\n1\n0\n0\n1\n1\n1\n1\n1\n\n\n\nのように２行目についてPermutationをランダムに \\(Y\\) 回実施してサンプリングから \\(\\Pr(X \\geq 4)\\) を計算します（上の例では \\(X = 3\\)）となっている．このように計算されたp-valueはfisher’s exact p-valueのMonte Carlo approximationと呼んだりします．\n\n\n\nOdds ratio\nscipy.stats.fisher_exactではpvalueのほかにstatisticという返り値をもっています．\n\nprint(res_twosided.statistic)\n\n3.5\n\n\nこの 3.5 はいわゆるodds ratioで\n\\[\n3.5 = \\frac{21 / 4}{15 / 10}\n\\]\nで計算されます．\n\nDef: Odds \n確率事象 \\(A\\) についての odds は以下のように計算される\n\\[\n\\begin{align*}\n\\text{odds}(A) = \\frac{\\Pr(A)}{1 - \\Pr(A)} = \\frac{\\Pr(A)}{\\Pr(A^c)}\n\\end{align*}\n\\]\n\noddsを用いることで表現がシンプルになるケースとしてフェアな賭けにおいける倍率の計算が上げられます． 例として，確率事象 A に対して1円を賭ける状況を考えます．確率事象 A が発生しなかったら1円を失い，確率事象 A が発生したら1円はキープ & x 円のリターンを得られるとします．\nこのとき，この賭けがフェアであるためには，期待利得が0であることが必要ですが，以下のように \\(x = \\text{odds}(A^c)\\) とリターンが設定されているとフェアな賭けになります．\n\\[\n\\begin{align*}\n&\\text{expected return} = x \\times \\Pr(A) + (-1) \\times \\Pr(A^c)\\\\\n&\\Rightarrow x = \\frac{\\Pr(A^c)}{\\Pr(A)} \\because \\text{exptected return should be 0}\\\\\n& \\Rightarrow x = \\text{odds}(A^c) = 1/\\text{odds}(A)\n\\end{align*}\n\\]\n\nDef: Odds ratio \nとある母集団にたいして，とある疾患の発症を抑制すると謳っている新薬を考えます．\n\n疾患が発症したならPositive, 発症しなかったらNegative\n新薬を処方されたらTreated, されなかったらControl\n\nとして，母集団の各組み合わせに対する事前割当確率が以下のようなクロスセルで定義されているとします．\n\n\n\n\n\n \n\n\n\n\n\n\nTreated\n\n\n\n\nControl\n\n\n\n\n\n\nPositive\n\n\n\n\n\\(p_{11}\\)\n\n\n\n\n\\(p_{12}\\)\n\n\n\n\n\n\nNegative\n\n\n\n\n\\(p_{21}\\)\n\n\n\n\n\\(p_{22}\\)\n\n\n\n\n\nこのとき，treated/control間の疾患発症のodds ratioは\n\\[\n\\text{odds ratio} = \\frac{p_{11}p_{22}}{p_{21}p_{12}}\n\\]\nで表現される．\n\n\n仮に Treated, Control両方のグループで疾患発症がレアなイベントだとすると \\(1- \\Pr(\\text{Positive} \\vert \\text{Treated}), 1-\\Pr(\\text{Positive} \\vert \\text{Control})\\) はともに十分小さくなり，\n\\[\n\\text{odds}(\\text{Positive}\\vert\\text{Treated}) \\approx Pr(\\text{Positive} \\vert \\text{Treated})\n\\]\nとみなせるので\n\\[\n\\frac{\\text{odds}(\\text{Positive}\\vert\\text{Treated})}{\\text{odds}(\\text{Positive}\\vert\\text{Control})} \\approx \\frac{Pr(\\text{Positive} \\vert \\text{Treated})}{Pr(\\text{Positive} \\vert \\text{Control})}\n\\]\nOdds ratioが0.7だとすると，Treated は Controlにくらべ 30% ほど疾患発症確率が低いという解釈に繋がります．\n\nOdds ratioの推定と信頼区間\n\n\n\n\n\n \n\n\n\n\n\n\nTreated\n\n\n\n\nControl\n\n\n\n\n合計\n\n\n\n\n\n\nPositive\n\n\n\n\n\\(x_{11}\\)\n\n\n\n\n\\(x_{12}\\)\n\n\n\n\n\\(x_{1\\cdot}\\)\n\n\n\n\n\n\nNegative\n\n\n\n\n\\(x_{21}\\)\n\n\n\n\n\\(x_{22}\\)\n\n\n\n\n\\(x_{2\\cdot}\\)\n\n\n\n\n\n\n合計\n\n\n\n\n\\(x_{\\cdot 1}\\)\n\n\n\n\n\\(x_{\\cdot 2}\\)\n\n\n\n\n\\(N\\)\n\n\n\n\n\n上記のようなデータについて，prior odds ratio \\(\\theta\\) の推定は\n\\[\n\\hat\\theta = \\frac{\\hat p_{11}\\hat p_{22}}{\\hat p_{21}\\hat p_{12}} \\  \\ \\text{where } \\hat p_{ij} = \\frac{x_{ij}}{N}\n\\]\n従って，\n\\[\n\\hat\\theta = \\frac{x_{11}x_{22}}{x_{21}x_{12}}\n\\]\n ▶  Confidence Intervalの計算\nConfidence Intervalは，実務では \\(\\log(\\theta)\\) を用いたCLTとdelta methodによる近似で計算されます．\n\\[\n\\begin{align*}\n\\mathbf p = (p_{11},p_{12},p_{21},p_{22})\n\\end{align*}\n\\]\nとしたとき，もともとのテーブルはクラス4の多項分布とみなせるので \\(\\mathbf p\\) についての共分散行列 \\(\\Sigma\\) は\n\\[\n\\begin{align*}\n\\Sigma = \\frac{1}{n}\\left(\n    \\begin{array}{cccc}\n    (1-p_{11}) p_{11} & -p_{11} p_{12} & -p_{11} p_{21} & -p_{11} p_{22} \\\\\n     -p_{11} p_{12} & \\left(1-p_{12}\\right) p_{12} & -p_{12} p_{21} & -p_{12} p_{22} \\\\\n     -p_{11} p_{21} & -p_{12} p_{21} & \\left(1-p_{21}\\right) p_{21} & -p_{21} p_{22} \\\\\n     -p_{11} p_{22} & -p_{12} p_{22} & -p_{21} p_{22} & (1-p_{22}) p_{22}\n    \\end{array}\n\\right)\n\\end{align*}\n\\]\nまた，\\(\\log(\\theta) = \\log(p_{11}) - \\log(p_{12}) - \\log(p_{21}) + \\log(p_{22})\\) についての分散はdelta methodを用いて\n\\[\n\\begin{align*}\n&\\operatorname{Var}(\\log(\\mathrm{OR})) = (\\nabla f \\Sigma )\\times \\nabla f^T\\\\\n&\\nabla f = \\left(\\frac{1}{p_{11}},-\\frac{1}{p_{12}},-\\frac{1}{p_{21}},\\frac{1}{p_{22}}\\right)\n\\end{align*}\n\\]\nと表せます．これを推定値 \\(\\hat p_{ij}\\) を用いて計算すると\n\\[\n\\begin{align*}\n&\\widehat{\\operatorname{Var}(\\log(\\operatorname{OR})}=\\frac{1}{x_{11}}+\\frac{1}{x_{12}}+\\frac{1}{x_{21}}+\\frac{1}{x_{22}}\\\\\n&\\widehat{\\operatorname{SE}(\\log(\\operatorname{OR})}=\\sqrt{\\frac{1}{x_{11}}+\\frac{1}{x_{12}}+\\frac{1}{x_{21}}+\\frac{1}{x_{22}}}\n\\end{align*}\n\\]\n\\(\\mathbf p\\) は \\(N\\) が十分大きいときCLTより正規分布に近似できると考えられるので \\(\\log(\\hat\\theta)\\) についてのConfidence Intervalは\n\\[\n\\text{CI(log odds ratio)} = \\widehat{\\log(\\operatorname{OR})}\\pm z_{1-\\alpha/2}\\times \\widehat{\\operatorname{SE}(\\log(\\operatorname{OR})}\n\\]\nまた，odds ratioのConfidence Intervalは対数を再度変換すれば良いので\n\\[\n\\text{CI(odds ratio)} = \\exp(\\widehat{\\log(\\operatorname{OR})}\\pm z_{1-\\alpha/2}\\times \\widehat{\\operatorname{SE}(\\log(\\operatorname{OR})})\n\\]\nと計算できる．\n\n📘 REMARKS \n\n上記の方法でのConfidence intervalは \\(\\log(\\hat\\theta)\\) 自体の推定分散ではなく，CLTを用いているのであくまで分散についての極限分布を用いている\n\\(p_{21}\\) や \\(p_{12}\\) 自体は0になり得ることを考えると，\\(\\hat\\theta\\) や \\(\\log(\\hat\\theta)\\) が存在しないことも考えられる\n\n\n\n\nReferences\n\nPennState STAT 504 &gt; 4.5 - Fisher’s Exact Test",
    "crumbs": [
      "統計的仮説検定",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_101/fisher_exact_test.html#references",
    "href": "posts/statistical_hypothesis_test_101/fisher_exact_test.html#references",
    "title": "12  Fisher’s exact test",
    "section": "",
    "text": "PennState STAT 504 &gt; 4.5 - Fisher’s Exact Test",
    "crumbs": [
      "統計的仮説検定",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html",
    "title": "13  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "",
    "text": "Fisher流検定の考え方\n２標本問題を考えたとき，２標本の平均の差がそのバラツキの大きさ（＝標準誤差）と比べて大きければ大きいほど 「母集団に差があり」のエビデンス力が高いという考えがFisher流検定となります．P値自体はサンプルサイズに依存すると留意していましたが， Fisher流ではP値が小さいほどエビデンス力が高いという解釈になります．さらに，\nというモノサシの提案をFisherはしました．これが現在の有意水準(significance level) 5% という慣習の由来であると言われています．\nなお，FisherはP値を統計家がデータの解析結果を「報告」するときのモノサシとしての提案にとどまっており， 効果があったか否かの「判定」は，統計家だけでなく関連専門家が参加するグループ討議によって，報告されたP値，分析対象，サンプルサイズ等を吟味して総合的に「判定」すべきであると考えてます．",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#fisher流検定の考え方",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#fisher流検定の考え方",
    "title": "13  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "",
    "text": "平均の差が標準誤差の２倍未満であれば平均の差はバラツキによる差であって考慮に値しない，\n2倍以上の差があるとき，偶然のみに支配されたバラツキに比べると指標の値が相対的に大きいと言える→初めて科学的に意味のある差であるか否かを検討する対象になりうる（正規分布を仮定したとき，約5％水準に相当）",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#neyman-pearson流検定の考え方",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#neyman-pearson流検定の考え方",
    "title": "13  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "Neyman-Pearson流検定の考え方",
    "text": "Neyman-Pearson流検定の考え方\nNeyman-Pearson流は統計的検定について\n\n\\(H_0: \\theta \\in \\Theta_0\\), Null hypothesis\n\\(H_1: \\theta \\not\\in \\Theta_0\\), Alternative hypothesis\n\nの２つを設定し，観察されたデータに基づいてどちらの仮説がより妥当な仮説であるかを判定する問題という統計的判定問題を考えました． 統計的判定問題のおける判定の誤りについて，\n\nType I Error: \\(H_0\\) が正しいのに誤って \\(H_0\\) を棄却するエラー\nType II Error: \\(H_1\\) が正しいのに誤って \\(H_0\\) を採択するエラー\n\nの２種類があるとし，Type I Errorの確率を \\(\\alpha\\) に抑えた上で，Type II Errorの確率 \\(\\beta\\) を最小にする制約付き最小化問題 として統計的判定問題を定式化しました．\n\n\n\n\n\n \n\n\n\n\nTruth\n\n\n\n\n\n\n\\(H_0\\)\n\n\n\n\n\\(H_1\\)\n\n\n\n\n\n\n\n\n\n\n\n検定結果\n\n\n\n\n\\(H_0\\)\n\n\n\n\n正しい(\\(1- \\alpha\\))\n\n\n\n\nType II Error(\\(\\beta\\))\n\n\n\n\n\n\n\\(H_1\\)\n\n\n\n\nType I Error(有意水準: \\(\\alpha\\))\n\n\n\n\n正しい（検出力: \\(1 - \\beta\\)）\n\n\n\n\n\n検定問題に対応する 検定統計量 \\(T\\), \\(H_0\\) の棄却域を \\(R\\) で表すとそれぞれ以下のように表現されます\n\nType I Error rate, \\(\\alpha = \\Pr(T \\in R \\vert H_0)\\)\nType II Error rate: \\(\\beta = \\Pr(T \\not\\in R \\vert H_1)\\)\n\nFisher流ではP値の大きさがエビデンス力という意味を持つことに対して，Neyman-Pearson流では\n\n事前に定められた有意水準 \\(\\alpha\\) をP値が下回るなら効果ありとの判定\nそうでないなら，効果なしとの判定\n\\(P = 0.00001\\) だろうが \\(P = 0.049\\) だろうがP値の水準自体には意味を求めない\n\nという違いがあります．\n\n📘 REMARKS \nNeyman-Pearson流では， \\(\\alpha, \\beta\\) を用いて統計的に効果があると言えるか？という統計的判定問題として仮説検定を定式化しましたが，\n\n統計的検定は決定するための方法ではなく，結果を報告するための方法である by F.Mostelller(1916-2006)\n\nと理解するにとどめたほうが良いとされています．",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#探索的リサーチと検証的リサーチ",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#探索的リサーチと検証的リサーチ",
    "title": "13  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "探索的リサーチと検証的リサーチ",
    "text": "探索的リサーチと検証的リサーチ\n特定の疾患をターゲットとして行われる医薬品の開発過程（詳細はこちら）を例にすると，\n\n候補化学物質について，発がん性試験，変異原性試験，薬効薬理研究など様々な試験をラットや細胞に対して探索的に実施\n健常なヒトを対象に臨床第I相試験として，安全性や薬物動態などを探索的に研究\n当該疾患の患者を対象に第II相臨床試験として，病気の程度によってどのような効き目を発揮するのか（有効性）、副作用はどの程度か（安全性）、またどのような使い方（投与量・間隔・期間など）をしたらよいか、を研究\n第III相臨床試験として，医薬品の有効性と安全性をRCTで検証\n\n第III相臨床試験においては，TreatmentのEffect Sizeの想定と十分なサンプルサイズを確保した上でRCTを実施，そして得られたデータに基づいて統計的意味における効果の有無を検証しています． このようなリサーチを検証的リサーチといいます．\n一方，それまでの動物試験，非臨床試験，臨床第I相試験，臨床第II相試験では，Effect Sizeやサンプルサイズが事前に統計的に設定される場合は少なく，あくまで 次の分析ステップに進む値するエビデンス収集や仮説立案という目的で実施されるリサーチです．このような分析を探索的リサーチと呼びます．\n\n検証的リサーチにおける仮説検定手順\nとあるPopulationを対象に実施するTreatmentの効果をRCTで仮説検定検証する場合，基本的には次のような一連の手順で実施します．\n\nTable: 検証的リサーチ手順\n\n\n\n\n\n\n手順\n説明\n\n\n\n\n手順(1)\n主要評価項目 \\(\\delta\\) を定義し，期待される水準 \\(\\delta_0\\) を見積もる\n\n\n手順(2)\n\\(H_0: \\delta = 0, H_1: \\delta \\neq 0\\) のようにHypothesesを言語化する\n\n\n手順(3)\n有意水準 \\(\\alpha\\), 検出力 \\(1 - \\beta\\) を定める\n\n\n手順(4)\n有意水準 \\(\\alpha\\), 検出力 \\(1 - \\beta\\) のもとで \\(\\delta_0\\) を検出するための必要サンプルサイズを計算する\n\n\n手順(5)\nPopulationからランダムにEntityをサンプリングして，手順(4)のサンプルサイズを満たすようにEntityをランダム or 層化ランダムでtreated/controlに割り当てる\n\n\n手順(6)\ntreated, controlのバランスチェック\n\n\n手順(7)\ntreated, controlがともに実験から逸脱しない形でそれぞれ処置を受けることを観察(= プロトコル遵守の確保)\n\n\n手順(8)\ntreated, controlのデータを収集し，Attritionなどの対応を実施した上で，主要評価項目, 検定統計量を計算\n\n\n手順(9)\n手順(8)で計算された検定統計量を元に，統計的検定を実施し，P値が有意水準 \\(\\alpha\\) 以下ならば効果があると統計的判断を下し，それ以外の場合では \\(H_0\\) が棄却できなかったとする\n\n\n\n上記の手順に則って，\\(H_0\\) が棄却された場合，少なくとも \\(\\delta \\geq \\delta_0\\) なのだろうという統計的判断がなされます．\n\n\n探索的リサーチと仮説検定\n探索的リサーチでは，多くの場合，サンプルサイズや特徴量バランスがコントロールできない観察データを対象に分析し， また次のリサーチに進むための仮説構築や検討に値する特徴量スクリーニングを目的とすることが多いです．このとき検定を実施するとしても，有意水準，検出力，Effect Sizeを想定した Neyman-Pearson流の検定の実施は難しく，偶然のバラツキにしては差が大きそうというインサイトを得ることを目的としたFisher流仮説検定の用い方となります．\nただ，P値に基づいて推論を行うのではなく，平均の差やハザード比などの指標や信頼区間，またその分野のドメイン知識を考慮した上で， 総合的に結果を解釈→仮説の構築をすることが重要です．",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#appendix-新薬誕生までのプロセス",
    "href": "posts/statistical_hypothesis_test_201/fisher_vs_neyman_pearson.html#appendix-新薬誕生までのプロセス",
    "title": "13  Fisher流検定 vs Neyman-Pearson流検定",
    "section": "Appendix: 新薬誕生までのプロセス",
    "text": "Appendix: 新薬誕生までのプロセス\n\n\n\n出典: 治験の３つのステップ，群馬大学医学部附属病院 先端医療開発センター臨床研究推進部\n\n\n\nTable: 各工程における分析目的\n\n\n\n\n\n\n工程\n説明\n\n\n\n\n新規物質の探索・創製\n薬になりそうな新しい物質を探したり，作り出したりすること\n\n\n物理的化学的研究\n新規物質の構造や物理的・化学的な性状などを調べること\n\n\n薬効薬理研究\nどのような効果があるか，どのようなメカニズムで効果を現すのかなどを調べること\n\n\n薬物動態研究\nどのように，体内に吸収され，臓器などに分布し，代謝されて排泄されるかなどを調べること\n\n\n一般薬理研究\nどのような部位にどんな作用を及ぼすかなど，薬効薬理作用以外の安全性に関する作用を調べること\n\n\n一般毒性研究\n投与期間を短・中・長期などに分けて，毒性（安全性）を広く調べること\n\n\n特殊毒性研究\n発がん性や胎児への影響がないかなど，特別な毒性（安全性）を調べること\n\n\n臨床第I相試験（臨床薬理試験）\n少数の健康成人などについて，主に安全性や薬物動態などを調べる試験\n\n\n臨床第II相試験（探索的試験）\n比較的少数の患者さんについて，有効性と安全性などを調べる試験\n\n\n臨床第III相試験（検証的試験）\n多数の患者さんについて，標準的な「くすり」などと比較して有効性と安全性を確認する試験\n\n\n製造販売後調査\n製造販売後に多くの患者さんに使用されたときの安全性や有効性などの情報を集め，それを分析・ 評価して医療関係者などに伝えること",
    "crumbs": [
      "統計的仮説検定の実践",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fisher流検定 vs Neyman-Pearson流検定</span>"
    ]
  },
  {
    "objectID": "posts/ExplanatoryDataAnalysis/gini_coefficient.html",
    "href": "posts/ExplanatoryDataAnalysis/gini_coefficient.html",
    "title": "14  ジニ係数",
    "section": "",
    "text": "ジニ係数とローレンツ曲線\nPythonで形状パラメター 2.5 のパレート分布の乱数と \\(\\operatorname{Unif}(0, 1)\\) を20個ずつ発生させ作画したのが以下\nCode\nimport numpy as np\nfrom scipy.stats import pareto\nimport polars as pl\nimport plotly.express as px\n\nnp.random.seed(42)\n\nSAMPLESIZE = 20\nshape_parameter = 1\nx = pareto.rvs(shape_parameter, size=SAMPLESIZE)\nx2 = np.random.uniform(0, 1, SAMPLESIZE)\n\n# Compute ratio\nrelative_freq = np.arange(0, SAMPLESIZE + 1) / SAMPLESIZE\nrelative_cumulative_pareto = np.insert(np.cumsum(sorted(x)) / np.sum(x), 0, 0)\nrelative_cumulative_uniform = np.insert(np.cumsum(sorted(x2)) / np.sum(x2), 0, 0)\n\n# create dataframe\ndf = pl.DataFrame(\n    {\n        \"relative_freq\": relative_freq,\n        \"pareto\": relative_cumulative_pareto,\n        \"uniform\": relative_cumulative_uniform,\n    }\n)\n\n\n# compute gini\ndef compute_gini(relative_freq, relative_cumulative):\n    a = relative_freq[1:-1] - relative_cumulative[1:-1]\n    b = relative_freq[2:] - relative_freq[:-2]\n    return np.sum(a * b)\n\n\n# plot\npareto_gini = compute_gini(relative_freq, relative_cumulative_pareto)\nuniform_gini = compute_gini(relative_freq, relative_cumulative_uniform)\nfig = px.line(\n    df,\n    x=\"relative_freq\",\n    y=[\"pareto\", \"uniform\"],\n    color_discrete_sequence=[\"blue\", \"red\"],\n    markers=\"x\",\n    title=\"\"\"pareto gini coefficient: {:.2f}, uniform gini coeffient:  {:.2f}\"\"\".format(\n        pareto_gini, uniform_gini\n    ),\n)\n\nfig.update_yaxes(\n    scaleanchor=\"x\",\n    scaleratio=1,\n)\nBASE_SIZE = 600\nfig.update_layout(\n    autosize=True,\n    width=BASE_SIZE,\n    height=BASE_SIZE,\n    shapes=[\n        dict(\n            type=\"line\",\n            line_dash=\"dot\",\n            yref=\"y\",\n            y0=0,\n            y1=1,\n            xref=\"x\",\n            x0=0,\n            x1=1,\n            line=dict(color=\"gray\"),\n            label=dict(text=\"完全平等線\", textposition=\"middle center\"),\n        )\n    ],\n)\n\nfig.show()\n上記のfigureにおける45度線は完全平等線と呼ばれる線です．この赤線とローレンツ曲線で囲まれたエリアの面積の２倍がジニ係数に相当します．ジニ係数は赤線と青線のエリアを三角形と台形に分けてそれぞれを計算し，その合計の２倍で計算することができます．\n上記のようにsampleジニ係数は台形の面積の２倍で計算しますが，母集団ジニ係数と比較して小さめに計算されます． サンプルサイズが十分大きい場合は無視できる程度ですが，度数分布表に基づく計算の場合やsmall sampleの場合は 過小方向バイアスの修正が必要になる場合があります．\nまた，ジニ係数の導出式より，MAD(Mean absolute difference)と sample meanの相対比にジニ係数が比例することがわかります．\n\\[\n\\begin{align*}\n\\operatorname{Gini} &= \\frac{1}{2n^2\\overline{x}}\\sum_{i=1}^n\\sum_{j=1}^n \\vert x_i - x_j\\vert\\\\\n                    &= \\frac{1}{2}\\frac{\\sum_{i=1}^n\\sum_{j=1}^n \\vert x_i - x_j\\vert}{n^2}\\frac{1}{\\overline x}\\\\\n                    &\\propto \\frac{\\operatorname{MAD}}{\\operatorname{sample mean}}\n\\end{align*}\n\\]\n▶  ジニ係数の特徴",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ジニ係数</span>"
    ]
  },
  {
    "objectID": "posts/ExplanatoryDataAnalysis/gini_coefficient.html#ジニ係数とローレンツ曲線",
    "href": "posts/ExplanatoryDataAnalysis/gini_coefficient.html#ジニ係数とローレンツ曲線",
    "title": "14  ジニ係数",
    "section": "",
    "text": "Def: ローレンツ曲線 \nデータ \\(X = \\{x_1, \\cdots, x_n\\}\\) について，以下のような順序統計量をとる\n\\[\nx_{[1]} \\leq \\cdots \\leq x_{[i]} \\leq \\cdots \\leq x_{[n]}\n\\]\nこのとき，相対度数 \\(r_i\\) と累積比率 \\(I_i\\) をそれぞれ以下のように定義する：\n\\[\n\\begin{align*}\nr_i & = \\frac{i}{n}\\\\\nI_i &= \\frac{\\sum_{j=1}^i x_{[j]}}{\\sum_{j=1}^n x_{[j]}}\n\\end{align*}\n\\]\n点 \\((0,0), (r_1, I_1), \\cdots , (r_n, I_n)\\) を区分的に直線で結んで得られる曲線がローレンツ曲線である．\n\n\n\n\n\nDef: ジニ係数 \n\nジニ係数はデータの偏りを示す指標\n\n ▶  相対度数に基づくジニ係数\n\\(a_i\\) を度数表に基づく累積相対頻度， \\(b_i\\) を累積相対階級値としたとき\n\\[\n\\begin{align*}\n\\operatorname{Gini} = \\sum_{i=1}^{k-1} (a_i - b_i)(a_{i+1} - a_{i-1}) \\  \\ \\text{where } a_0 = 0, b_0 = 0\n\\end{align*}\n\\]\n ▶  データに基づくジニ係数\n\\[\n\\begin{align*}\n\\operatorname{Gini}\n    &= \\frac{2}{n}\\sum_{n-1}^{i=1}\\left(\\frac{i}{n} - \\frac{\\sum_{j=1}^i x_{[j]}}{n\\overline{x}}\\right) \\\\\n    &= \\frac{1}{2n^2\\overline{x}}\\sum_{i=1}^n\\sum_{j=1}^n \\vert x_i - x_j\\vert\n\\end{align*}\n\\]\n\n\n\n\n\n\nローレンツ曲線，ジニ係数ともに分布の尺度パラメーターに依存しない\n1点に集中する分布の場合，ローレンツ曲線は完全平等線と一致し，ジニ係数は0となる\nエントロピーと異なり，一様分布の場合にジニ係数が最大をとるとかではない\n\n\n📘 REMARKS \n\nデータの偏りや集中性を見る指標としてジニ係数は使用されますが，他にもエントロピーという指標がある\n\nカテゴリー別に分類されたデータにおいて，各カテゴリーの総体頻度を \\(\\hat p_i = f_i/n\\) としたとき\n\\[\n\\begin{align*}\nH(\\mathbf p) = -\\sum \\hat p_i \\log(\\hat p_i)\n\\end{align*}\n\\]\nHが大きいほどデータは一様になり，集中性があるほど指標は小さくなる．\n\n\n無限母集団におけるローレンツ曲線\n定義域 \\((0, \\infty)\\), 期待値 \\(\\mu\\) を持つ連続確率変数 \\(X\\) について，累積分布関数を \\(F(x)\\), 確率密度関数 \\(f(x)\\) とおきます．このとき，\\(z = F(x)\\) としたときのローレンツ曲線は\n\\[\n\\begin{align*}\n&F^{-1}(z) = x \\  \\ \\text{ (inverse function of cdf)}\\\\\n&L(z) =\\frac{\\int_0^{F^{-1}(z)} t f(t) \\mathrm{d}t}{\\mathbb E[X]}\n\\end{align*}\n\\]\nとして, \\((z, L(z))\\) で作られる曲線となります．ジニ係数は\n\\[\n\\begin{align*}\n\\operatorname{gini} &= 2\\int_0^1 (u - L(u))\\mathrm{d}u\\\\\n                    &= 1 - 2\\int_0^1 L(u)\\mathrm{d}u\n\\end{align*}\n\\]",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>ジニ係数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/complex_number.html",
    "href": "posts/mathematical_appendix/complex_number.html",
    "title": "15  複素数",
    "section": "",
    "text": "複素数の性質\n複素数 \\(z = a + bi\\) の実数の組 \\((a, b)\\) について，\n\\[\n\\begin{align*}\na &= \\text{Re}\\, z\\\\\nb &= \\text{Im}\\, z\n\\end{align*}\n\\]\nと表したりします．それぞれ，real part, imaginary partの略と理解できます．\n▶  複素数の加減乗除\n\\(z_1 = a + bi, z_2 = c + di\\) としたとき，四則演算は以下のように計算されます\n\\[\n\\begin{align*}\nz_1 + z_2 &= (a + c) + (b + d)i\\\\\nz_1 - z_2 &= (a - c) + (b - d)i\\\\\nz_1z_2    &= ac + (ad + bc)i + bdi^2\\\\\n          &= (ac - bd) + (ad + bc)i \\quad \\because i^2 =-1\\\\\n\\frac{z_1}{z_2}\n          &= \\frac{a + bi}{c + di}\\\\\n          &= \\frac{a + bi}{c + di}\\frac{c - di}{c - di}\\\\\n          &= \\frac{(a + bi)(c - di)}{c^2+d^2}\\\\\n          &= \\frac{(ac + bd) + (bc - ad)i}{c^2+d^2}\n\\end{align*}\n\\]\n共役複素数は，複素数平面上で実軸に関して対称移動させたものと解釈することができます．また，定義より自明ですが 複素数と共役複素数について，和と積が実数になるという特徴もあります．\n定義より，\\((a, b) = (0, 0)\\) のときは，\\(\\vert z\\vert =0\\)．逆に \\(\\vert z\\vert =0\\) ならば，\n\\[\na^2 + b^2 =0\n\\]\nを満たす実数の組は \\((0, 0)\\) 歯科存在しないので，\\(z = 0\\) となります．",
    "crumbs": [
      "Mathematical Appendix",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>複素数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/complex_number.html#複素数の性質",
    "href": "posts/mathematical_appendix/complex_number.html#複素数の性質",
    "title": "15  複素数",
    "section": "",
    "text": "Def: 複素数 \n２つの実数 \\(a, b\\) を用いて\n\\[\nz = a + bi\n\\]\nと表される数 \\(z\\) を複素数という．\\(a\\) を上の複素数の実部，\\(b\\) を虚部と呼ぶ． 複素数全体の集合は一般に \\(\\mathbb C\\) と表される．\n\n\n\n\n\n\n\n\nDef: 共役複素数 \n複素数 \\(z = a+bi\\) に対し，\n\\[\n\\bar z = a - bi\n\\]\nを \\(z\\) の共役複素数（きょうやくふくそすう）と呼ぶ．\n\n\n\n\nTheorem 15.1 \n２つの複素数 \\(z_1, z_2\\) について，次が成り立つ\n\\[\n\\begin{align*}\n\\overline{z_1 + z_2} &= \\bar z_1 + \\bar z_2\\\\\n\\overline{z_1z_2} &= \\bar z_1\\bar z_2\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(z_1 = a_1 + b_1i, z_2 = a_2 + b_2i\\) としたとき，\n\\[\n\\begin{align*}\n\\overline{z_1 + z_2}\n    &= \\overline{(a_1 + a_2) + (b_1 +b_2)i}\\\\\n    &= (a_1 + a_2) - (b_1 +b_2)i\\\\\n    &= a_1 - b_1i + a_2 - b_2i\\\\\n    &= \\bar z_1 + \\bar z_2\n\\end{align*}\n\\]\n積については\n\\[\n\\begin{align*}\n\\overline{z_1z_2}\n    &= \\overline{(a_1a_2 - b_1b_2) + (a_1b_2 + a_2b_1)i}\\\\\n    &= (a_1a_2 - b_1b_2) - (a_1b_2 + a_2b_1)i\\\\\n    &= a_1a_2 - (a_1b_2 + a_2b_1)i + b_1b_2i^2\\\\\n    &= (a_1 - b_1i)(a_2 - b_2i)\\\\\n    &= \\bar z_1\\bar z_2\n\\end{align*}\n\\]\n\n\n\n\nDef: 複素数の絶対値 \n複素数 \\(z = a + bi\\) の絶対値 \\(\\vert z\\vert\\) は次のように計算される：\n\\[\n\\vert z \\vert = \\sqrt{a^2 + b^2}\n\\]",
    "crumbs": [
      "Mathematical Appendix",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>複素数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/complex_number.html#複素平面",
    "href": "posts/mathematical_appendix/complex_number.html#複素平面",
    "title": "15  複素数",
    "section": "複素平面",
    "text": "複素平面\n複素数 \\(z = x + yi\\) が与えられたとき，２つの実数の組 \\((x, y)\\) が与えられた状況とも解釈できます． また，２つの実数の組 \\((x, y)\\) が与えられた状況とは，\\(xy\\)-平面上の点を与えられる状況とも考えることができます． つまり，複素数が与えられたとき，\n\n実部を \\(x\\) 軸\n虚部を \\(y\\) 軸\n\nとする \\(xy\\)-平面を考えることができます．\\(z = 1 + \\sqrt{3}\\) を描いてみると，\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npi = np.pi\n\n\n# Set parameters\nr = 2\ntheta = pi/3\nx = r * np.cos(theta)\nx_range = np.linspace(0, x, 1000)\ntheta_range = np.linspace(0, theta, 1000)\n\n# Plot\nfig = plt.figure(figsize=(6, 6))\nax = plt.subplot(111, projection='polar')\n\nax.plot((0, theta), (0, r), marker='o', color='b')          # Plot r\nax.plot(np.zeros(x_range.shape), x_range, color='b')       # Plot x\nax.plot(theta_range, x / np.cos(theta_range), color='b')        # Plot y\nax.plot(theta_range, np.full(theta_range.shape, 0.1), color='r')  # Plot theta\n\nax.margins(0) # Let the plot starts at origin\n\nax.set_title(\"Trigonometry of complex numbers\", va='bottom',\n    fontsize='x-large')\n\nax.set_rmax(2)\nax.set_rticks((0.5, 1, 1.5, 2))  # Less radial ticks\nax.set_rlabel_position(-88.5)    # Get radial labels away from plotted line\n\nax.text(theta, r+0.01 , r'$z = x + iy = 1 + \\sqrt{3}\\, i$')   # Label z\nax.text(theta+0.2, 1 , r'$\\vert z\\vert = 2$')                             # Label r\nax.text(0-0.2, 0.5, '$x = 1$')                            # Label x\nax.text(0.5, 1.2, r'$y = \\sqrt{3}$')                      # Label y\nax.text(0.25, 0.15, r'$\\text{arg } $z$ = 60^o$')                   # Label theta\n\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n複素数が \\(xy\\)-平面上の点で表すことができるということは，原点からの長さと角度 \\((r, \\theta)\\) によって点を表現できることになります． このとき，以下のように表します:\n\\[\n\\begin{align*}\nr & = \\vert z\\vert\\\\\n\\theta &= \\operatorname{arg}z = \\arctan \\left(\\frac{y}{x}\\right)\n\\end{align*}\n\\]\nこのことから次の定義が導けます．\n\nDef: 複素数の極形式 \n複素平面上の点 \\(z = x + yi\\) は極座標を用いて次のように表せる:\n\\[\n\\begin{align*}\n&z = r(\\cos \\theta + i\\sin\\theta)\\\\\n&\\text{where } r = \\vert z\\vert, \\theta = \\operatorname{arg} z\n\\end{align*}\n\\]\n\n ▶  複素数の積と複素平面",
    "crumbs": [
      "Mathematical Appendix",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>複素数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/beta_function.html",
    "href": "posts/mathematical_appendix/beta_function.html",
    "title": "16  ベータ関数",
    "section": "",
    "text": "ベータ関数の性質",
    "crumbs": [
      "Mathematical Appendix",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ベータ関数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/beta_function.html#ベータ関数の性質",
    "href": "posts/mathematical_appendix/beta_function.html#ベータ関数の性質",
    "title": "16  ベータ関数",
    "section": "",
    "text": "Def: ベータ関数 \n実数 \\(a, b\\) について，ベータ関数 \\(\\operatorname{B}(a, b)\\) は以下のように定義される．\n\\[\n\\operatorname{B}(a, b) = \\int^1_0 x^{a-1}(1-x)^{b-1}\\mathrm{d}x\n\\]\nまた，ガンマ関数を用いて以下のように表せる\n\\[\n\\operatorname{B}(a, b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\n\\]\n\n\n\n\n\n\n\nProof: ベータ関数とガンマ関数の関係\n\n\n\n\n\nガンマ関数の定義を用いて，\n\\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b) &= \\int^\\infty_0 x^{a-1}\\exp(-x)\\mathrm{d}x \\int^\\infty_0 y^{b-1}\\exp(-y)\\mathrm{d}y \\\\\n                   &= \\int^\\infty_0 \\int^\\infty_0 x^{a-1}y^{b-1}\\exp(-x-y)\\mathrm{d}x \\mathrm{d}y \\\\\n\\end{align*}\n\\]\nここで, \\(x + y = u, x/(x + y) = v\\) という変数変換を考える．つまり，\n\\[\n\\begin{align*}\nx = uv, y = u(1-v)\n\\end{align*}\n\\]\nこのときのヤコビアンは\n\\[\n\\begin{align*}\n\\vert\\operatorname{det} J \\vert= u\n\\end{align*}\n\\]\n従って，\n\\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b) &=\\int^\\infty_0 \\int^\\infty_0 (uv)^{a-1}[u(1-v)]^{b-1}\\exp(-u) u\\mathrm{d}x \\mathrm{d}y \\\\\n                   &= \\int^\\infty_0 \\int^\\infty_0 u^{a+b-1}\\exp(-u) v^{a-1}(1-v)^{b-1}\\mathrm{d}x \\mathrm{d}y \\\\\n                   &= \\int^\\infty_0 \\int^1_0 u^{a+b-1}\\exp(-u) v^{a-1}(1-v)^{b-1}\\mathrm{d}v \\mathrm{d}u \\\\\n                   &= \\int^\\infty_0 u^{a+b-1}\\exp(-u) \\mathrm{d}u \\int^1_0 v^{a-1}(1-v)^{b-1}\\mathrm{d}v \\\\\n                   &= \\operatorname{\\Gamma}(a+b)\\operatorname{B}(a, b)\n\\end{align*}\n\\]\nつまり，\n\\[\n\\operatorname{B}(a, b) = \\frac{\\operatorname{\\Gamma}(a+b)}{\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b)}\n\\]\n\n\n\n\n\n\n\n\n\nProof: 座標変換\n\n\n\n\n\nガンマ関数は \\(z^2 = x\\) という変数変換を用いると以下のように変換できる\n\\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a) &= \\int^\\infty_0 x^{a-1}\\exp(-x)\\mathrm{d}x \\\\\n          &= \\int^\\infty_0 z^{2a-2}\\exp(-z^2)\\frac{\\mathrm{d}x}{\\mathrm{d}z}\\mathrm{d}z \\\\\n          &= 2\\int^\\infty_0 z^{2a-1}\\exp(-z^2)\\mathrm{d}z\n\\end{align*}\n\\]\nこれを用いて\n\\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b) &= 4\\int^\\infty_0 x^{2a-1}\\exp(-x^2)\\mathrm{d}x \\int^\\infty_0 y^{2b-1}\\exp(-y^2)\\mathrm{d}y \\\\\n                   &= 4\\int^\\infty_0 \\int^\\infty_0 x^{2a-1}y^{2b-1}\\exp(-(x^2+y^2))\\mathrm{d}x \\mathrm{d}y\n\\end{align*}\n\\]\nここで, \\(x = r\\cos\\theta, y=r\\sin\\theta\\) という変数変換を行う．このときのヤコビアンは\n\\[\n\\vert \\operatorname{det} J \\vert = r\n\\]\nよって， \\[\n\\begin{align*}\n\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b) &= 4\\int^\\infty_0 \\int^\\infty_0 x^{2a-1}y^{2b-1}\\exp(-(x^2+y^2))\\mathrm{d}x \\mathrm{d}y\\\\\n                   &= 4\\int^{\\pi/2}_0 \\int^\\infty_0 r^{2a+2b-2} \\cos^{2a-1}\\theta \\sin^{2b-1}\\theta \\exp(-r^2) r \\mathrm{d}r \\mathrm{d}\\theta\\\\\n                   &= \\left(2\\int^{\\pi/2}_0 \\cos^{2a-1}\\theta \\sin^{2b-1}\\theta \\mathrm{d}\\theta\\right) \\times \\left(\\int^\\infty_0r^{2(a+b)-1}\\exp(-r^2)\\mathrm{d}r \\right)\\\\\n                   &= \\operatorname{B}(a, b) \\times \\operatorname{\\Gamma}(a+b)\n\\end{align*}\n\\]\n従って，\n\\[\n\\operatorname{B}(a, b) = \\frac{\\operatorname{\\Gamma}(a+b)}{\\operatorname{\\Gamma}(a)\\operatorname{\\Gamma}(b)}\n\\]\n\n\n\n\n\nTheorem 16.1 三角関数とベータ関数の関係 \n正の実数 \\(a, b\\) について，以下が成立する\n\\[\n\\operatorname{B}(a, b) = 2\\int^{\\pi/2}_0\\cos^{2a-1}\\theta\\sin^{2b-1}\\theta \\mathrm{d}\\theta\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nベータ関数について \\(x = \\cos^2\\theta\\) を用いた置換積分で以下のように示すことができます．\n\\[\n\\begin{align*}\n\\operatorname{B}(a, b) &= \\int^1_0 x^{a-1}(1-x)^{b-1} \\mathrm{d}x\\\\\n                       &= \\int^0_{\\pi/2} (\\cos^2\\theta)^{a-1}(1 - \\cos^2\\theta)^{b-1} \\cdot (-2\\cos\\theta\\sin\\theta) \\mathrm{d}\\theta\\\\\n                       &= 2\\int^{\\pi/2}_0 \\cos^{2a-1}\\theta \\sin^{2b-1}\\theta\\mathrm{d}\\theta\n\\end{align*}\n\\]\nなお, \\(\\sin^2\\theta = 1 - \\cos^2\\theta\\) 及び \\(\\displaystyle\\frac{\\mathrm{d}\\cos^2\\theta}{\\mathrm{d}\\theta} = -2\\cos\\theta\\sin\\theta\\) を用いている．\n\n\n\n\n\nTheorem 16.2 引数の交換性 \n正の実数 \\(a, b\\) について，以下が成立する\n\\[\n\\operatorname{B}(a, b) = \\operatorname{B}(b, a)\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{align*}\n\\operatorname{B}(a, b) &= \\int^1_0 x^{a-1}(1-x)^{b-1} \\mathrm{d}x\\\\\n                       &= \\int^0_1 (1-z)^{a-1}z^{b-1} \\frac{\\mathrm{d}x}{\\mathrm{d}z}\\mathrm{d}z\\\\\n                       &= \\int^1_0 (1-z)^{a-1}z^{b-1} \\mathrm{d}z\\\\\n                       &= \\operatorname{B}(b, a)\n\\end{align*}\n\\]",
    "crumbs": [
      "Mathematical Appendix",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>ベータ関数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/gamma_function.html",
    "href": "posts/mathematical_appendix/gamma_function.html",
    "title": "17  ガンマ関数",
    "section": "",
    "text": "ガンマ関数の性質\nガンマ関数は，階乗関数を正の実数に拡張したものです． \\(n\\in\\mathbb N\\) について階乗関数は\n\\[\nn! = (n-1)!\\cdot n\n\\]\nと表されますが，ガンマ関数も\n\\[\n\\Gamma(z) = \\Gamma(z-1)\\cdot (z-1)\n\\]\nという性質があります．\nガンマ関数は，正の整数 \\(x\\) に対して，\\(y=x!\\) という平面上の点 \\((x, y)\\) を結ぶsmooth curveに対応します．\nCode\nfrom plotly import express as px\nimport numpy as np\nfrom scipy.special import gamma, factorial\n\nx = np.linspace(0, 6*1.05, 200)\nk = np.arange(1, 7)\n\nfig = px.line(x=x, y=gamma(x), title='Gamma function')\nfig.add_traces(\n    list(px.scatter(x=k, y=factorial(k-1)).select_traces())\n)\nfig.show()\n▶  \\(\\Gamma(n) = (n-1)!\\) の確認\nまず，\\(\\gamma(1) = 1\\) を確認します．\n\\[\n\\begin{align*}\n\\Gamma(1)\n    &= \\int^\\infty_0 x^{1-1}\\exp(-x)\\mathrm{d}x\\\\\n    &= \\int^\\infty_0 \\exp(-x)\\mathrm{d}x\\\\\n    &= \\left[-\\exp(-x)\\right]^\\infty_0\\\\\n    &= 1 = 0!\n\\end{align*}\n\\]\n続いて, \\(z\\in\\mathbb R_{++}\\) について，\\(\\Gamma(z) = \\Gamma(z-1)\\cdot (z-1)\\) が成立することを確認します．\n\\[\n\\begin{align*}\n\\Gamma(z+1)\n    &= \\int^\\infty_0 x^{z+1-1}\\exp(-x)\\mathrm{d}x\\\\\n    &= \\int^\\infty_0 x^{z}\\exp(-x)\\mathrm{d}x\\\\\n    &= [-x^z\\exp(-x)]^\\infty_0 + z\\int^\\infty_0 x^{z-1}\\exp(-x)\\mathrm{d}x\\\\\n    &= z\\Gamma(z)\n\\end{align*}\n\\]\nなお，\n\\[\n\\lim_{x\\to\\infty}-x^z\\exp(-x)=0\n\\]\nはロピタルの定理を用いています．",
    "crumbs": [
      "Mathematical Appendix",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>ガンマ関数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/gamma_function.html#ガンマ関数の性質",
    "href": "posts/mathematical_appendix/gamma_function.html#ガンマ関数の性質",
    "title": "17  ガンマ関数",
    "section": "",
    "text": "Def: ガンマ関数 \n関数 \\(\\Gamma: \\mathbb R_{++}\\to\\mathbb R_{++}\\) を以下のように定義する：\n\\[\n\\Gamma(z) = \\int_0^\\infty x^{z-1}\\exp(-x)\\mathrm{d}x\n\\]\nこれをガンマ関数と呼ぶ．\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 17.1 \n\\(a &gt; 0\\) という定数をについて\n\\[\n\\int^\\infty_0t^{x-1}\\exp(-at)\\mathrm{d}t = \\frac{\\Gamma(x)}{a^x}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(u = at\\) という変数変換を考える．\\(\\frac{\\mathrm{d}t}{\\mathrm{d}u}=\\frac{1}{a}\\) であるので，\n\\[\n\\begin{align*}\n\\int^\\infty_0t^{x-1}\\exp(-at)\\mathrm{d}t\n    &= \\int^\\infty_0\\left(\\frac{u}{a}\\right)^{x-1}\\exp(-u)\\frac{1}{a}\\mathrm{d}u\\\\\n    &= \\frac{1}{a^x}\\int^\\infty_0u^{x-1}\\exp(-u)\\mathrm{d}u\\\\\n    &= \\frac{\\Gamma(x)}{a^x}\n\\end{align*}\n\\]\n\n\n\n\nExample 17.1 \n\\[\n\\int^\\infty_{-\\infty}x^2\\exp(-ax^2)\\mathrm{d}x\n\\]\nも上述の定理を使うと簡単に計算できます．\\(x^2 = u\\) という変数変換を念頭に以下，\n\\[\n\\begin{align*}\n\\int^\\infty_{-\\infty}x^2\\exp(-ax^2)\\mathrm{d}x\n    &= 2\\int^\\infty_{0}x^2\\exp(-ax^2)\\mathrm{d}x \\quad\\because \\text{偶関数}\\\\\n    &= 2\\int^\\infty_{0}u\\exp(-au)\\times \\frac{1}{2\\sqrt{u}}\\mathrm{d} u\\\\\n    &= \\int^\\infty_{0}u^{\\frac{3}{2}-1}\\exp(-au)\\mathrm{d} u\\\\\n    &= \\frac{\\Gamma(3/2)}{a^{\\frac{3}{2}}}\\\\\n    &=\\frac{\\sqrt{\\pi}}{2a\\sqrt{a}}\n\\end{align*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\n\nTheorem 17.2 \nガンマ関数について，以下の等式が成り立つ\n\\[\n\\Gamma(s) = 2\\int_0^\\infty t^{2s-1}\\exp(-t^2)\\mathrm{d}t\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nガンマ関数について \\(t = u^2\\) という変数変換を行うと以下のように導けます．\n\\[\n\\begin{align*}\n\\Gamma(s)\n    &= \\int_0^\\infty t^{s-1}\\exp(-t)\\mathrm{d}t\\\\\n    &= \\int_0^\\infty u^{2s-2}\\exp(-u^2)\\times 2u \\mathrm{d}u\\\\\n    &= 2\\int_0^\\infty u^{2s-1}\\exp(-u^2)\\mathrm{d}u\\\\\n\\end{align*}\n\\]\n\n\n\n\nガンマ関数の有名な公式\n\n\nTheorem 17.3 \n\\[\n\\Gamma\\bigg(\\frac{1}{2}\\bigg) = \\sqrt{\\pi}\n\\]\n\n\n\n\n\n\n\n\nProof: ガウス積分を用いる場合\n\n\n\n\n\n\\(t = u^2\\) という変数変換を考える．\n\\[\n\\begin{align*}\n\\int^\\infty_0t^{-1/2}\\exp(-t)\\mathrm{d}t\n    &= \\int^\\infty_0u^{-1}\\exp(-u^2)2u\\mathrm{d}u\\\\[3pt]\n    &= 2\\int^\\infty_0\\exp(-u^2)\\mathrm{d}u\\\\[3pt]\n    &= \\int^\\infty_{-\\infty}\\exp(-u^2)\\mathrm{d}u\\\\[3pt]\n    &= \\int^\\infty_{-\\infty}\\exp\\bigg(-\\frac{u^2}{2\\times(\\frac{1}{\\sqrt 2})^2}\\bigg)\\mathrm{d}u\\\\[3pt]\n    &= \\sqrt{2\\pi}\\frac{1}{\\sqrt 2}\\\\\n    &= \\sqrt{\\pi}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nProof: 極座標変換を用いる場合\n\n\n\n\n\n\\[\n\\begin{align*}\n\\Gamma\\bigg(\\frac{1}{2}\\bigg)^2 &= \\bigg(\\int^\\infty_0t^{-1/2}\\exp(-t)\\mathrm{d}t\\bigg)^2\\\\[3pt]\n                                &= \\int^\\infty_0x^{-1/2}\\exp(-x)\\mathrm{d}x\\int^\\infty_0y^{-1/2}\\exp(-y)\\mathrm{d}y\n\\end{align*}\n\\]\nここで，\\(x=u^2, y=v^2\\) と置換積分する．\n\\[\n\\begin{align*}\n\\int^\\infty_0x^{-1/2}\\exp(-x)\\mathrm{d}x\\int^\\infty_0y^{-1/2}\\exp(-y)\\mathrm{d}y\n    &= 4\\int^\\infty_0\\exp(-u^2)\\mathrm{d}u\\int^\\infty_0\\exp(-v^2)\\mathrm{d}v\\\\\n    &= 4\\int^\\infty_0\\int^\\infty_0\\exp[-(u^2+v^2)]\\mathrm{d}u\\mathrm{d}v\n\\end{align*}\n\\]\n更に，\\(u = r\\cos\\theta, v=r\\sin\\theta\\) という極座標変換を行う．\\(u, v\\geq 0\\) であることに留意すると，\\(r\\in(0, \\infty), \\theta\\in (0, \\pi/2)\\) になるので\n\\[\n\\begin{align*}\n4\\int^\\infty_0\\int^\\infty_0\\exp[-(u^2+v^2)]\\mathrm{d}u\\mathrm{d}v &= 4\\int^\\infty_0\\int^{\\pi/2}_0\\exp[-r^2]r \\mathrm{d}\\theta \\mathrm{d}r \\  \\  \\because u, v\\geq 0 \\\\[3pt]\n&= 4\\times \\frac{\\pi}{2} \\int^{\\infty}_0\\exp[-r^2]r\\mathrm{d}r\\\\[3pt]\n&= 4\\times \\frac{\\pi}{2}\\bigg[\\frac{\\exp(-r^2)}{-2}\\bigg]^\\infty_0\\\\[3pt]\n&= \\pi\n\\end{align*}\n\\]\nガンマ関数の定義より，\\(\\Gamma(1/2) &gt; 0\\) なので，\n\\[\n\\Gamma(1/2)^2 = \\pi \\Rightarrow \\Gamma(1/2) = \\sqrt{\\pi}\n\\]",
    "crumbs": [
      "Mathematical Appendix",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>ガンマ関数</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/jensen_inequality.html",
    "href": "posts/mathematical_appendix/jensen_inequality.html",
    "title": "18  Jensen’s Inequality",
    "section": "",
    "text": "Def: convex function \n区間 \\(I\\) で定義された関数 \\(f:I \\to \\mathbb R\\) がconvex(凸関数)であるとは, 任意の \\(0 &lt; t &lt; 1\\) について\n\\[\nf((1-t)x + ty) \\leq (1 - t)f(x) + t f(y)\\quad \\forall x, y \\in I, x \\neq y\n\\]\nstrictly convexであるとは\n\\[\nf((1-t)x + ty) &lt; (1 - t)f(x) + t f(y)\\quad \\forall x, y \\in I, x \\neq y\n\\]\n\n\n\nDef: concave function \n区間 \\(I\\) で定義された関数 \\(f:I \\to \\mathbb R\\) がconcave(凹関数)であるとは, 任意の \\(0 &lt; t &lt; 1\\) について\n\\[\nf((1-t)x + ty) \\geq (1 - t)f(x) + t f(y)\\quad \\forall x, y \\in I, x \\neq y\n\\]\nstrictly concaveであるとは\n\\[\nf((1-t)x + ty) &gt; (1 - t)f(x) + t f(y)\\quad \\forall x, y \\in I, x \\neq y\n\\]\n\n\n以下のような \\(\\exp(x), x^2, \\vert x\\vert\\) などが凸関数の例です.\n\n\nCode\nimport numpy as np\nimport plotly.express as px\nimport polars as pl\n\nx = np.linspace(-1, 1, 100)\nexp_x = np.exp(x)\nsquared_x = x**2\nabs_x = abs(x)\n\ndf = pl.DataFrame({\"x\": x, \"exp_x\": exp_x, \"squared_x\": squared_x, \"abs_x\": abs_x})\n\nfig = px.line(df, x=\"x\", y=[\"exp_x\", \"squared_x\", \"abs_x\"], title='example: convex fucntions')\nnewnames = {\"exp_x\": \"exp(x)\", \"squared_x\": \"x^2\", \"abs_x\": \"abs(x)\"}\nfig.for_each_trace(\n    lambda t: t.update(\n        name=newnames[t.name],\n        hovertemplate=t.hovertemplate.replace(t.name, newnames[t.name]),\n    )\n)\n\nfig.show()\n\n\n                                                \n\n\nまた, \\(\\ln(x), \\sqrt{x}\\) やconvext関数に \\(-1\\) を掛けたものは凹関数の例となります．\n\n\nCode\nx = np.linspace(0.05, 1.5, 100)\n\nln_x = np.log(x)\nsqrt_x = np.sqrt(x)\nsquared_x = -(x**2)\n\ndf = pl.DataFrame({\"x\": x, \"ln_x\": ln_x, \"sqrt_x\": sqrt_x, \"squared_x\": squared_x})\n\nfig = px.line(\n    df, x=\"x\", y=[\"ln_x\", \"sqrt_x\", \"squared_x\"], title=\"example: concave fucntions\"\n)\nnewnames = {\"ln_x\": \"log(x)\", \"sqrt_x\": \"sqrt(x)\", \"squared_x\": \"-x^2\"}\nfig.for_each_trace(\n    lambda t: t.update(\n        name=newnames[t.name],\n        hovertemplate=t.hovertemplate.replace(t.name, newnames[t.name]),\n    )\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\nTheorem 18.1 \n関数 \\(f\\) が区間 \\([a, b]\\) で連続で \\((a, b)\\) で２回微分可能とする．このとき， 関数 \\(f\\) が凸関数であることの必要十分条件は\n\\[\nf^{\\prime\\prime}(x) \\geq 0 \\quad \\forall x\\in (a, b)\n\\]\n\n\n\n\n\nTheorem 18.2 : Subgradient Inequality \n関数 \\(f\\) が区間 \\([a, b]\\) で凸関数であり，微分可能とする．このとき以下が成立する\n\\[\nf(y) \\geq f(x) + f^{\\prime}(x)(y-x) \\quad \\forall x, y\\in (a, b)\n\\]\n\n\n\n\n\nTheorem 18.3 Jensen’s Inequality \n\\(\\mathbb E[X] = \\mu &lt; \\infty\\) 及び \\(I \\subset \\mathbb R\\) をサポートとする確率変数 \\(X\\) について，\\(g:I\\to \\mathbb R\\) というconvex functionを考える．\\(g\\) が 区間 \\(I\\) で微分可能としたとき，\n\\[\n\\mathbb E[g(X)] \\geq g(\\mathbb E[X])\n\\]\n\\(g\\) がstrictly convexの場合，\\(X\\) がdegenerateであることの必要十分条件は \\(\\mathbb E[g(X)] = g(\\mathbb E[X])\\)．\n\\(g(\\cdot)\\) がconcaveの場合は，\\(\\mathbb E[g(X)] \\leq g(\\mathbb E[X])\\) が成立する．\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\(g(\\cdot)\\) はconvex functionなので,\n\\[\ng(X) \\geq g(\\mu) + g^\\prime(\\mu)(X - \\mu)\n\\]\n両辺について期待値をとると，\n\\[\n\\begin{align*}\n\\mathbb E[g(X)] &\\geq g(\\mathbb E[X]) + g^\\prime(\\mu)(\\mathbb E[X] - \\mu)\\\\\n                &= g(\\mathbb E[X])\n\\end{align*}\n\\]\n\n\n\n\nExample 18.1 \n確率変数 \\(X &gt;0\\) がnon-degenerateであるとき，Jensen’s inequalityより \\(g(x) = 1/x\\) はstrictly convexなので\n\\[\n\\mathbb E\\left[\\frac{1}{X}\\right] &gt; \\frac{1}{\\mathbb E[X]}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]\n\n\nExample 18.2 : 不偏分散と標準偏差 \n確率変数 \\(X\\) について，\\(\\operatorname{Var}(X) = \\sigma^2\\) とする．母集団に対してランダムサンプリングを実施し，そこから得られた サンプルから不偏分散 \\(s^2\\) を得たとする．つまり，\n\\[\n\\mathbb E[s^2] = \\sigma^2\n\\]\nこのとき，\\(\\mathbb E[s] \\neq \\sigma\\) となってしまう．なぜなら，\\(h(x) = \\sqrt{x}\\) としたとき， 関数 \\(h\\) はstrictly concaveのため，Jensen’s inequalityより\n\\[\n\\begin{gather*}\n\\mathbb E[h(s^2)] &lt; h(\\mathbb E[s^2])\\\\\n\\Rightarrow \\mathbb E[S] &lt; \\sigma\n\\end{gather*}\n\\]\n\\[\\tag*{\\(\\blacksquare\\)}\\]",
    "crumbs": [
      "Mathematical Appendix",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Jensen's Inequality</span>"
    ]
  },
  {
    "objectID": "posts/mathematical_appendix/Lhopitals_rule.html",
    "href": "posts/mathematical_appendix/Lhopitals_rule.html",
    "title": "19  ロピタルの定理",
    "section": "",
    "text": "Theorem 19.1 \n\\(x=a\\)に十分近い\\(x\\)について\\(f(x), g(x)\\)は微分可能とする. さらに\\(x=a\\)以外で\\(g(x)\\neq 0\\)とする\n(1). \\(\\lim_{x\\to a}f(x)=\\lim_{x\\to a}g(x)=0\\)のとき次式が成り立つ\n\\[\n\\lim_{x\\to a}\\frac{f(x)}{g(x)} = \\lim_{x\\to a}\\frac{f^\\prime(x)}{g^\\prime(x)}\n\\]\n(2). \\(\\lim_{x\\to a}\\vert f(x)\\vert= \\infty, \\lim_{x\\to a}\\vert g(x)\\vert=\\infty\\)のとき次式が成り立つ\n\\[\n\\lim_{x\\to a}\\frac{f(x)}{g(x)} = \\lim_{x\\to a}\\frac{f^\\prime(x)}{g^\\prime(x)}\n\\]\n(1), (2)で \\(a\\) を \\(\\infty, -\\infty\\) に置き換えても同様の命題が成り立つ.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n ▶  \\(a&lt;\\infty, \\lim_{x\\to a}f(x)=g(x)=0\\) の場合\n平均値の定理より\n\\[\n\\begin{align*}\n\\frac{f(x)}{g(x)} = \\frac{f(x)-f(a)}{g(x)-g(a)}= \\frac{f^\\prime(\\xi)}{g^\\prime(\\xi)}\n\\end{align*}\n\\]\nこのとき, \\(\\xi\\)は\\(a, x\\)の間の数. なお, \\(\\xi\\to a \\text{ as }x\\to a\\)なので\n\\[\n\\lim_{x\\to a}\\frac{f(x)}{g(x)} = \\frac{f^\\prime(a)}{g^\\prime(a)}\n\\]\n ▶  \\(a=\\infty, \\lim_{x\\to \\infty}f(x)=g(x)=0\\) の場合\n\\(x = \\frac{1}{t}\\)と変換し, 次の関数を考える\n\\[\n\\begin{align*}\n&h(t) = f(1/t)\\\\\n&k(t) = g(1/t)\\\\\n& \\lim_{t\\to 0} h(t) = \\lim_{t\\to 0} k(t) =  0\n\\end{align*}\n\\]\n従って,\n\\[\n\\begin{align*}\n&\\frac{h(t)}{k(t)} = \\frac{h(t)-h(0)}{k(t)-k(0)} = \\frac{h^\\prime(\\xi)}{k^\\prime(\\xi)}\\\\[3pt]\n&\\Rightarrow \\lim_{t\\to0}\\frac{h(t)}{k(t)} = \\lim_{t\\to0}\\frac{h^\\prime(t)}{k^\\prime(t)}\n\\end{align*}\n\\]\nよって,\n\\[\n\\lim_{x\\to\\infty}\\frac{f(x)}{g(x)} = \\lim_{x\\to\\infty}\\frac{f^\\prime(x)}{g^\\prime(x)}\n\\]\n\n\n\n\nExample 19.1 : ロピタルの定理の使用例 \n\\[\n\\begin{align*}\n\\lim_{x\\to\\infty}\\frac{x^k}{e^x}\n      &= \\lim_{x\\to\\infty}\\frac{kx^{k-1}}{e^x}\\\\[3pt]\n      &= \\lim_{x\\to\\infty}\\frac{k(k-1)x^{k-2}}{e^x}\\\\[3pt]\n      &= \\cdots\\\\\n      &= \\lim_{x\\to\\infty}\\frac{k!}{e^x}\\\\[3pt]\n      &= 0\n\\end{align*}\n\\]",
    "crumbs": [
      "Mathematical Appendix",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>ロピタルの定理</span>"
    ]
  },
  {
    "objectID": "posts/statistics101/chapter_header.html",
    "href": "posts/statistics101/chapter_header.html",
    "title": "統計学入門",
    "section": "",
    "text": "▶  統計学入門 のスコープ\n\n竹村彰通 (2020) をベースに，統計推測法に必要な基礎知識について勉強します\n日々のデータサイエンス分析のベースとなるような統計基礎概念や定義が勉強対象になりますが，これらについて数学的な定義とともにわかりやすい言語化ができるようになることを目的にしています\n\n ▶  記述統計と統計的推測\n\n 記述統計(descriptive statistics)\n\n調査や実験で得られたデータを整理して，その解釈を助けるような統計的分析のこと\n\n統計的推測(statistical inference)\n\n確率的な変動を多く含むデータに対して，そのDGP(= Data generating process)に何かしらの仮定を想定し，データから確率モデルの推定や検定を行う分析のこと\n\n\n\n「伏せられたトランプカードを透視することでスートを当てることができる！」という人がいたとします． この能力を試してみたところ５２枚のカードの内，４０枚を当てることができたというデータが得られました． この，40枚当てることができたというデータについて確率論的意味を判断をするというのが統計的推測です．\nこのように統計的推測とは確率モデルを想定してデータを解釈/判断する分析なので，確率論を中心とする数学的表現を用いたモデルの定式化 （=ランダムネスの法則を扱う数学理論）が必要となります．加えて，\n\n想定した確率モデルが正しそうか？\n与えられたデータと矛盾しないか？\n乖離がある場合，想定したモデルから導かれる結論はどの程度妥当すると言えるのだろうか？\n\nという分析上の判断も必要となります．そのため，統計的推測はデータ分析初心者にとっては敷居が高い手法となりますが， 一旦モデル化や仮定の妥当性についての説明がうまく行くと，手元に実際にあるデータの背後にあるメカニズムに基づいて推測が行えるようになります． 例えば，実際に観測できないPotential Outcomesの分布についての推定や将来の予測などがあります．\nこのように統計的推測とは，使いこなすのは大変ですが，使えるようになるととても強力なツールです．このノートを通じて，統計的推測の基礎やいくつかの応用分野の分析を見ながらマスターしたいなと思っています．\n ▶  測定の尺度\n\n\n\n\n\n\n\n\n種類\nmeasurement\n説明\n\n\n\n\nカテゴリカルデータ\n名義尺度(nominal scale)\nある対象が他と同一か，異なるかを表す測定例: 性別, 血液型\n\n\nカテゴリカルデータ\n順序尺度(ordinal scale)\n大小/優劣関係を表す測定例: ４段階評価の健康状態\n\n\n量的データ\n間隔尺度(interval scale)\n0が相対的な意味をもつ指標例: 気温，知能指数，標高\n\n\n量的データ\n比例尺度(ratio scale)\n0が絶対的な意味を持つ指標例: 身長，金額，時間の経過，絶対温度\n\n\n\n\n\n\n\n竹村彰通 (2020), 現代数理統計学, 学術図書出版社.",
    "crumbs": [
      "統計学入門"
    ]
  },
  {
    "objectID": "posts/probability_distribution/chapter_header.html",
    "href": "posts/probability_distribution/chapter_header.html",
    "title": "確率分布",
    "section": "",
    "text": "▶  この章のスコープ\nデータの背後にある確率モデルを構築するときには，研究対象の特徴と確率分布の特徴を踏まえて 適した分布の選択を行うことが望ましいです．例として，\n\n所得ならば正の値をとり，右に長い裾をもつので対数正規分布\n生物・人体測定値ならば正規分布\nシステムの耐久年数を表すならばガンマ分布\n捕獲再捕獲の確率ならば超幾何分布\n\nこの章では，代表的な確率分布に関して，確率分布の形状や特性値，確率分布がもっている性質について解説していきます．",
    "crumbs": [
      "確率分布"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/chapter_header.html",
    "href": "posts/statistical_hypothesis_test_201/chapter_header.html",
    "title": "統計的仮説検定の実践",
    "section": "",
    "text": "References",
    "crumbs": [
      "統計的仮説検定の実践"
    ]
  },
  {
    "objectID": "posts/statistical_hypothesis_test_201/chapter_header.html#references",
    "href": "posts/statistical_hypothesis_test_201/chapter_header.html#references",
    "title": "統計的仮説検定の実践",
    "section": "",
    "text": "柳川堯 (2018), P値: その正しい理解と適用, 近代科学社.\n\n\n永田靖 (2003), サンプルサイズの決め方, 朝倉書店.",
    "crumbs": [
      "統計的仮説検定の実践"
    ]
  },
  {
    "objectID": "posts/robust_statistics/chapter_header.html",
    "href": "posts/robust_statistics/chapter_header.html",
    "title": "Data Analysis and Outliers",
    "section": "",
    "text": "▶  Robust Statistics のスコープ\n\n藤澤洋徳 (2017) をベースに，外れ値に対する分析上の対処方法について勉強する\n\n ▶  ロバスト推定とロバスト検定\n\nDef: ロバスト推定とロバスト検定 \n\nロバスト推定: 外れ値に頑健な推定(estimation)\nロバスト検定: 外れ値の混入に頑健な検定\n\n\n\n\n\n\n藤澤洋徳 (2017), ロバスト統計 : 外れ値への対処の仕方（ISMシリーズ : 進化する統計数理 / 統計数理研究所編, 6, 近代科学社.",
    "crumbs": [
      "Data Analysis and Outliers"
    ]
  },
  {
    "objectID": "posts/econometrics101/chapter_header.html",
    "href": "posts/econometrics101/chapter_header.html",
    "title": "Econometrics Topics",
    "section": "",
    "text": "Econometrics Topics のスコープ\n\n\n\n\n\nEconometric Analysisの基本的な考え\n ▶  ceteris paribus\n\nceteris paribusとは「holding all other relevant factors fixed」を意味する概念\n\nとある確率変数 \\(X\\) の変化が別の確率変数 \\(Y\\) の変化を引き起こす（cause）とデータから主張するためには，単に同時分布（相関関係）を確認するだけでは不十分で，他の変数を固定した上(ceteris paribus)で， \\(X\\) の変化が \\(Y\\) の変化を伴うことを示す必要があります．\n ▶  Asymptotics\n\nfinite sample propertyと対になる概念で，\\(N\\to\\infty\\) に飛ばした極限分布における統計量の性質のこと\ncross section dataの場合は, unit of observations を \\(N\\to\\infty\\)\npanel data analysisの場合は，time indexを固定した上で unit of entitie sを \\(N\\to\\infty\\)\n\n ▶  説明変数(regressor)が確率的である\n\n説明変数が非確率的である例として，実験データのように説明変数 \\(\\mathbf x_i\\) の水準について分析者が事前に決定できる場合がある\n\nこの場合，error termと説明変数の相関（内生性問題）は排除できる\n\n観測データの場合は，実験データのように説明変数 \\(\\mathbf x_i\\) の水準については決定できないため，「非確率的」という仮定は通常当てはまらない\n\n説明変数が確率的である場合，非確率的のもとでは一致性を満たす推定量(例: GLS)が一致性を満たさなくなるリスクがあります．",
    "crumbs": [
      "Econometrics Topics"
    ]
  }
]