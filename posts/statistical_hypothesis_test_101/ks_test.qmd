---
title: "KSæ¤œå®š"
author: "Ryo Nakagami"
date: "2024-10-06"
date-modified: last-modified
number_sections: false
code-fold: true
comments:
    utterances:
         repo: RyoNakagami/statistics-for-regression-monkey
         label: discussion
---

## ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ãƒ»ã‚¹ãƒŸãƒ«ãƒãƒ•æ¤œå®šã®æ€§è³ª

$X_1,\cdots, X_n$ ã‚’ç‹¬ç«‹ã«åˆ†å¸ƒé–¢æ•° $F(x)$ ã«å¾“ã†ç¢ºç‡å¤‰æ•°ã¨ã—ã¾ã™ï¼ã“ã®ç¢ºç‡å¤‰æ•°ãŒç‰¹å®šã®åˆ†å¸ƒé–¢æ•°
$F_0(x)$ ã«å¾“ã†ã‹ã‚’æ¤œå®šã™ã‚‹å•é¡Œã‚’è€ƒãˆã¾ã™:

$$
H_0: F(x) = F_0(x), H_1: \exists x_0 \in \mathbb R \text{ such that }F(x) \neq F_0(x)
$$

ã“ã®å•é¡Œã®æ¤œå®šçµ±è¨ˆé‡ã¨ã—ã¦

$$
D_n = \sup_{x} \vert F_n(x) - F_0(x)\vert
$$

ã‚’ç”¨ã„ãŸæ¤œå®šã‚’ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ãƒ»ã‚¹ãƒŸãƒ«ãƒãƒ•æ¤œå®šã¨ã„ã„ã¾ã™ï¼$F_n(x)$ ã¯çµŒé¨“åˆ†å¸ƒé–¢æ•°ã§ã™ï¼

::: {#exm- .custom_problem }
<br>

ç¢ºç‡å¤‰æ•° $X \sim \operatorname{U}(0,1)$ ã‚’ $Y = -log(X)$ ã¨å¤‰æ•°å¤‰æ›ã™ã‚‹ã¨

$$
Y \sim \operatorname{Exp}(1)
$$

ã«å¾“ã„ã¾ã™ï¼25å€‹ã®æ¨™æº–ä¸€æ§˜åˆ†å¸ƒã«å¾“ã†ä¹±æ•°ã‚’ç™ºç”Ÿã•ã›ï¼Œãã‚Œã«åŸºã¥ã„ã¦KS test statisticã‚’è¨ˆç®—ã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼

```{python}
import numpy as np
from statsmodels.distributions.empirical_distribution import ECDF
from scipy import stats
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
from regmonkey_style.config import CONFIG
from regmonkey_style import stylewizard as sw

sw.set_templates("regmonkey_twoline")

np.random.seed(2222)

# DGP
N = 25
sample = -np.log(np.random.uniform(0, 1, N))
ecdf = ECDF(sample)
x = np.linspace(min(sample), max(sample), 1000)
true_cdf = stats.expon.cdf(x)
ecdf_values = ecdf(x)

# compute statistic
sup_idx, ks_statistic = np.argmax(np.abs(true_cdf - (ecdf_values))), np.max(
    np.abs(true_cdf - (ecdf_values))
)
sup_x = x[sup_idx]

fig, ax = plt.subplots()

ax.step(x, ecdf_values, label="Empirical CDF")
ax.plot(x, true_cdf, label="True CDF")
ax.annotate(
    "",
    xy=(sup_x, min(stats.expon.cdf(sup_x), ecdf(sup_x))),
    xytext=(sup_x, max(stats.expon.cdf(sup_x), ecdf(sup_x))),
    arrowprops=dict(
        arrowstyle="<->", color="black", linestyle="dashed"
    ),
)
ax.text(
    sup_x + 0.05,
    (true_cdf[sup_idx] + ecdf_values[sup_idx]) / 2,
    f"KS Statistic: {ks_statistic:.3f}",
    color="black",
    ha="left"
)
ax.set_title("KS test statistic")
ax.legend()
plt.show()
```

`scipy.stats.ks_1samp` ã§`exact`ã«ãƒ†ã‚¹ãƒˆã‚’ã—ã¦ã¿ã‚‹ã¨

```{python}
#| code-fold: false
stats.ks_1samp(sample, stats.expon(0).cdf, method='exact')
```

:::



<div class="blog-custom-border">
<strong>Def: çµŒé¨“åˆ†å¸ƒé–¢æ•°</strong> <br>

$n$ å€‹ã®å®Ÿæ•°å€¤ãƒ‡ãƒ¼ã‚¿ $D = \{x^{(i)}\in\mathbb R\vert i=1,\cdots, n\}$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãï¼ŒçµŒé¨“åˆ†å¸ƒ
$\operatorname{Emp}(D)$ ã®ç¢ºç‡é–¢æ•° $f_n(x)$ ã¯æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹

$$
\begin{gather}
f_n(x) = \frac{1}{n}\sum_{i=1}^n\delta(x - x^{(i)})\\
\text{where }\delta(\cdot) \text{ : Dirac delta function}
\end{gather}
$$ 

ç´¯ç©åˆ†å¸ƒé–¢æ•°ï¼ˆçµŒé¨“åˆ†å¸ƒé–¢æ•°ï¼‰ $F_X(x)$ ã¯æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹

$$
\begin{align*}
F_n(x)
    &= \frac{1}{n}\sum_{i=1}^n\int^x_{-\infty}\delta(z - x^{(i)})\,\mathrm{d}z\\
    &= \frac{1}{n}\mathbb 1(x^{(i)\leq x})
\end{align*}
$$

</div>

LLN(å¤§æ•°ã®æ³•å‰‡)ã‚ˆã‚Šï¼Œä»»æ„ã® $x\in\mathbb R$ ã«ã¤ã„ã¦ï¼Œ

$$
F_n(x) \to \mathbb E \mathbb 1(X_i \leq x) = P(X_i\leq x) = F(x)
$$

ãŒæˆç«‹ã—ã¾ã™ï¼æ¬¡ã«ï¼ŒKS Testã®çµ±è¨ˆé‡ãŒå¸°ç„¡ä»®èª¬ã§è¨­å®šã—ãŸ $F$ ã«ä¾å­˜ã—ãªã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼

<div class="blog-custom-border">
::: {#thm- .custom_problem }
<br>

$F(x)$ ãŒé€£ç¶šã®ã¨ãï¼Œ

$$
\sup_{x\in\mathbb R} \vert F_n(x) - F(x)\vert
$${#eq-dist-distance-sup}

ã®åˆ†å¸ƒã¯ $F$ ã«ä¾å­˜ã—ãªã„ï¼

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

ç´¯ç©åˆ†å¸ƒé–¢æ•° $F$ ã®é€†é–¢æ•°ã‚’æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™

$$
F^{-1}(y) = \min \{x: F(x)\leq y\}
$$

$F^{-1}$ ã‚’ç”¨ã„ã¦ @eq-dist-distance-sup ã‚’æ¬¡ã®ã‚ˆã†ã«å¤‰å½¢ã—ã¾ã™

$$
\begin{align*}
P(\sup_{x\in\mathbb R} \vert F_n(x) - F(x)\vert \leq t)
    = P(\sup_{y\in [0, 1]} \vert F_n(F^{-1}(y)) - y\vert \leq t)
\end{align*}
$$

çµŒé¨“åˆ†å¸ƒé–¢æ•° $F_n$ ã«ã¤ã„ã¦

$$
\begin{align*}
F_n(F^{-1}(y))
    &= \frac{1}{n}\sum \mathbb 1(X_i \leq F^{-1}(y))\\
    &= \frac{1}{n}\sum \mathbb 1(F(X_i) \leq y)
\end{align*}
$$

å¾“ã£ã¦ï¼Œ

$$
P(\sup_{y\in [0, 1]} \vert F_n(F^{-1}(y)) - y\vert \leq t) 
    = P\left(\sup_{y\in [0, 1]} \left\vert \frac{1}{n}\sum \mathbb 1(F(X_i) \leq y) - y\right\vert \leq t\right)
$$

ä»»æ„ã®åˆ†å¸ƒã®ç´¯ç©åˆ†å¸ƒé–¢æ•°ã¯æ¨™æº–ä¸€æ§˜åˆ†å¸ƒã«å¾“ã†ã®ã§ï¼Œ

$$
F(X_i) \sim \operatorname{U}(0, 1) 
$$

$U_i = F(X_i) \ \ \text{for } i \in \{1, \cdots, n\}$ ã¨ã™ã‚‹ã¨ï¼Œ

$$
P(\sup_{y\in [0, 1]} \vert F_n(F^{-1}(y)) - y\vert \leq t) 
    = P\left(\sup_{y\in [0, 1]} \left\vert \frac{1}{n}\sum \mathbb 1(U_i \leq y) - y\right\vert \leq t\right)
$$


ä»¥ä¸Šã‚ˆã‚Šï¼Œ @eq-dist-distance-sup ã®åˆ†å¸ƒã¯ $F$ ã«ä¾å­˜ã—ãªã„ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸï¼

:::

ã“ã“ã‹ã‚‰KS Testçµ±è¨ˆé‡ã‚’çµ±è¨ˆçš„ä»®èª¬æ¤œå®šã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ä¹—ã›ã‚‹ãŸã‚ã«ã¯ï¼Œå¾—ã‚‰ã‚ŒãŸçµ±è¨ˆé‡ã®å€¤ãŒ $H_0$ ã®ä¸‹ã§ã©ã®ã‚ˆã†ã«è©•ä¾¡ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã‹ï¼Ÿã‚’çŸ¥ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼
å›ºå®šã•ã‚ŒãŸ $x$ ã«ãŠã„ã¦CLTã‚ˆã‚Šä»¥ä¸‹ã®ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™

$$
\sqrt{n}(F_n(x) - F(x)) \overset{\mathrm{d}}{\to} N\big(0, F(x)(1 - F(x))\big) 
$$

ã“ã‚Œã‚’ç”¨ã„ã¦

$$
\sqrt{n}\sup \vert F_n(x) - F(x)\vert 
$$

ãŒåæŸã™ã‚‹åˆ†å¸ƒã‚’æ¬¡ã«ç¢ºèªã—ã¾ã™ï¼

<div class="blog-custom-border">
::: {#thm-ks-test-statistic-dist .custom_problem }
**: KSçµ±è¨ˆé‡ã®åæŸåˆ†å¸ƒ**
<br>

$$
P\big(\sqrt{n}\sup_{x\in\mathbb R} \vert F_n(x) - F(x)\vert \leq t\big) \overset{\mathrm{d}}{\to} H(t)
$$

$H(t)$ ã¯Kolmogorov-Smirnovåˆ†å¸ƒã§ãã®ç´¯ç©åˆ†å¸ƒé–¢æ•°ã¯ä»¥ä¸‹ã®å½¢ã«ãªã‚Šã¾ã™

$$
F_H(t) = 1 - 2\sum_{i=1}^\infty (-1)^{i-1}\exp(-2i^2t)
$$

:::

</div>

$F_0$ ã‚’å¸°ç„¡ä»®èª¬ã§è¨­å®šã—ãŸåˆ†å¸ƒã¨ã—ãŸã¨ãï¼Œååˆ†å¤§ãã„ $n$ ã®ã‚‚ã¨ã§ $F_n$ ã¯ $F$ ã«åæŸã™ã‚‹ã®ã§æ¤œå®šçµ±è¨ˆé‡ã«åŸºã¥ãåˆ¤æ–­ã¯ï¼Œååˆ†å°ã•ã„ $\delta$ ã‚’é¸ã³

$$
\sup_x \vert F_n(x) - F_0(x)\vert > \delta
$$

ã“ã‚Œã§åˆ¤æ–­ã§ãã‚Œã°è‰¯ã„ã¨ãªã‚Šã¾ã™ï¼ã“ã‚Œã«å¯¾ã—ã¦ï¼Œ$\sqrt{n}$ ã‚’ä¸¡è¾ºã«ä¹—ã˜ã‚‹ã¨

$$
D_n = \sqrt{n}\sup_x \vert F_n(x) - F_0(x)\vert > \sqrt{n}\delta
$$

ã‚‚ã— $H_0$ ãŒä¸æˆç«‹ãªã‚‰ã°

$$
D_n > \sqrt{n}\delta \to \infty \quad\text{as } n \to \infty
$$

å¾“ã£ã¦ï¼ŒDecision ruleã¯

$$
\delta =
\left\{\begin{array}{cc}
H_0 & D_n \leq c\\
H_1 & D_n > c\\
\end{array}\right.
$$

ã“ã®Decision ruleã®ã‚‚ã¨ã§ã®æœ‰æ„æ°´æº– $\alpha$ ã¯

$$
\alpha = P(\delta \neq H_0\vert H_0) = P(D_n > c \vert H_0) \approx 1 - H(c)
$$

::: {.nte- .callout-tip icon="false"}
# ğŸµ Green Tea Break

$H(c)$ ã‚’è¨ˆç®—ã™ã‚‹ã®ã¯å›°é›£ãªå ´åˆãŒå¤šã„ã®ã§ï¼Œæœ‰æ„æ°´æº– $\alpha$ ã«å¯¾å¿œã—ãŸ $c$ ã®æ•°å€¤ã¨ã—ã¦

$$
\begin{align*}
x_\alpha &= [-(1/2)\log(\alpha/2)]^{1/2} \quad \text{(æ£„å´ç‚¹)}\\
c &= x_\alpha/\sqrt{n}
\end{align*}
$$

ã¨ã—ã¦è¨ˆç®—ã•ã‚Œã¾ã™ï¼

ã¾ãŸï¼Œæ¨™æœ¬ã«ãŠã‘ã‚‹KSçµ±è¨ˆé‡ã®è¨ˆç®—ã‚‚

$$
D = \max_{1 \le i \le N} \left( F(Y_{i}) -
               \frac{i-1} {N}, \frac{i}{N} - F(Y_{i}) \right)
$$

ã¨ã—ã¦æ¨™æœ¬ã«ãŠã‘ã‚‹å„ç‚¹ã«ãŠã‘ã‚‹å¸°ç„¡ä»®èª¬ã®åˆ†å¸ƒã¨ã®è·é›¢ã®upper boundã‚’ãƒ™ãƒ¼ã‚¹ã«è¨ˆç®—ã•ã‚Œã¾ã™ï¼

:::


### ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ãƒ»ã‚¹ãƒŸãƒ«ãƒãƒ•æ¤œå®šã®æ³¨æ„ç‚¹

KS Testã¯åˆ†å¸ƒã«ã‚ˆã‚‰ãªã„æ¨æ¸¬æ³•ã¨ã„ã†ãƒ¡ãƒªãƒƒãƒˆã¯ã‚ã‚Šã¾ã™ãŒï¼Œå¸°ç„¡ä»®èª¬ã®åˆ†å¸ƒã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã—ï¼Œã¾ãŸæ¬¡ã®æ³¨æ„ç‚¹ãŒã‚ã‚Šã¾ã™ï¼

1. é€£ç¶šå‹ç¢ºç‡åˆ†å¸ƒã®æ¤œå®šã®ã¿ã—ã‹ä½¿ãˆãªã„
2. ãƒ†ãƒ¼ãƒ«éƒ¨åˆ†ã¨æ¯”ã¹åˆ†å¸ƒã®ä¸­å¿ƒä»˜è¿‘ã§ã®è·é›¢ã«å¯¾ã—ã¦sensitive = ãƒ†ãƒ¼ãƒ«åˆ†å¸ƒã®é•ã„ã«ã¯éˆæ„Ÿ
3. å¸°ç„¡ä»®èª¬ã®åˆ†å¸ƒã‚’location, shapeãªã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…¨ã¦æŒ‡å®šã—ãªã„ã¨ï¼ŒåŒã˜åˆ†å¸ƒæ—ã‚’å¸°ç„¡ä»®èª¬ã«æŒ‡å®šã—ã¦ã„ã¦ã‚‚æ£„å´ã•ã‚Œã¦ã—ã¾ã†

ã“ã®ã‚ˆã†ãªæ³¨æ„ç‚¹ã«åŠ ãˆï¼Œ Anderson-Darling testã‚„Cramer Von-Mises testã¨ã„ã£ãŸæ”¹è‰¯å‹ã®æ¤œå®šæ‰‹æ³•ãŒã‚ã‚‹ã®ã§ï¼ŒKS Testã¯ã‚ã¾ã‚Šåˆ©ç”¨ã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ï¼

## Two Sample KS Test
