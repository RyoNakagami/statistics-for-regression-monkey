---
title: "Bias and Variance"
author: "Ryo Nakagami"
date: "2024-10-09"
date-modified: last-modified
number_sections: false
code-fold: true
comments:
    utterances:
         repo: RyoNakagami/statistics-for-regression-monkey
         label: discussion
---

## Bias and Variance in Prediction

æœªçŸ¥ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æ¨å®šé‡ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸçµ±è¨ˆçš„æ¨å®šã¨ã“ã¨ãªã‚Šï¼Œæ•™å¸«ã‚ã‚Šæ©Ÿæ¢°å­¦ç¿’ã®äºˆæ¸¬ã§ã¯
$x$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã« $y$ ã‚’è‰¯ãäºˆæ¸¬ã§ãã‚‹é–¢æ•° $f$ 

$$
\begin{align*}
&y = f(x) + \epsilon\\
&\text{where }  \mathbb E[\epsilon \vert x] = 0
\end{align*}
$$

ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ï¼ã“ã®é–¢æ•°ã®å­¦ç¿’ã‚’ç›®æŒ‡ã—ã¦ï¼Œ

$$
S = \{x^{(i)}, y^{(i)}\}_{i=1}^n
$$

ã‚’training datasetã¨ã—ã¦ï¼Œ$\hat f_n$ ã¨ã„ã†é–¢æ•°ã‚’æ§‹ç¯‰ã—ãŸã¨ã—ã¾ã™ï¼æ¯é›†å›£ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
ã•ã‚ŒãŸ $S$ ã«ã¯ãƒã‚¤ã‚º $\epsilon_i$ ãŒå«ã¾ã‚Œã¦ãŠã‚Šï¼Œ$S$ ã‚’ç”¨ã„ã¦å­¦ç¿’ã—ãŸ $\hat f_n$ ã¯ç¢ºç‡çš„ã«å¤‰å‹•ã™ã‚‹å´é¢ãŒã‚ã‚Šã¾ã™ï¼ãã®ä¸Šã§åŒã˜æ¯é›†å›£ã‹ã‚‰i.i.dã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸæœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\{x^{*}, y^{*}\}$ ã«ãŠã„ã¦ã‚‚ $x$ ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ç”¨ã„ã‚‹ã“ã¨ã§ ç²¾åº¦è‰¯ã $y$ ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ = æ±åŒ–èª¤å·®(generalization error)ãŒå°ã•ã„ $\hat f^n$ ãŒæœ›ã¾ã—ã„äºˆæ¸¬é–¢æ•°ã¨ãªã‚Šã¾ã™ï¼

æ±åŒ–èª¤å·®ã‚’ $y$ ã¨ $\hat f_n$ ã®è·é›¢ã¨è€ƒãˆãŸã¨ãï¼Œ$(y^*, x^*)$ ã§æ¡ä»¶ã¥ã‘ãŸã¨ã

$$
\begin{align*}
&\operatorname{MSE}(\hat f_n)\\
    &= \mathbb E[(y^* - \hat f_n(x^*))^2]\\
    &= \mathbb E[(f(x^*) + \epsilon - \hat f_n(x^*))^2]\\
    &= \mathbb E[\epsilon^2] + \mathbb E[(f(x^*) -\hat f_n(x^*))^2]\\
    &= \mathbb E[\epsilon^2] + \underbrace{\mathbb E[f(x^*) -\hat f_n(x^*)]^2}_{=\text{bias and constant}} + \operatorname{Var}(f(x^*) -\hat f_n(x^*))\\
    &= \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible error}} + \text{Bias}(\hat f_n)^2 + \operatorname{Var}(\hat f_n)
\end{align*}
$$

::: {#exm- .custom_problem }
**ç·šå½¢ã‚¯ãƒ©ã‚¹ã«ãŠã‘ã‚‹Bias and Variance**
<br>

True functionã‚’ 

$$
f(x) = \pmb{\theta}^Tx
$$

ã¨å®šç¾©ã—ï¼Œ$\pmb{\theta}$ ã¯æœªçŸ¥ã§ã‚ã‚‹ã¨ã—ã¾ã™ï¼Training datasetã‚’ç”¨ã„ã¦ï¼Œ

$$
\hat f_n(x) = \widehat{\pmb{\theta}}_n^Tx
$$

ã‚’å­¦ç¿’ã—ãŸã¨ã—ã¾ã™ï¼ã“ã®ã¨ãï¼ŒæœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\pmb{D} = \{(x^*, y^*)\}$ ã«ãŠã„ã¦ï¼Œ

$$
\begin{align*}
\text{Bias}(\hat f_n)
    &= \mathbb E[\hat f_n(x) - f(x) \vert \pmb{D}]\\
    &= \mathbb E[\widehat{\pmb{\theta}}_n^Tx - \pmb{\theta}^Tx\vert \pmb{D}]\\
    &= \mathbb E[\widehat{\pmb{\theta}}_n - \pmb{\theta}\vert \pmb{D}]^Tx\\
    &= \text{Bias}(\widehat{\pmb{\theta}}_n)^Tx
\end{align*}
$$

åŒæ§˜ã«

$$
\begin{align*}
&\operatorname{Var}(\hat f_n)\\
    &= \mathbb E[(\hat f_n(x) - f(x))^2 \vert \pmb{D}]\\
    &= \mathbb E[(\widehat{\pmb{\theta}}_n^Tx - \pmb{\theta}^Tx)^2\vert \pmb{D}]\\
    &= \mathbb E[((\widehat{\pmb{\theta}}_n^T - \pmb{\theta}^T)x)^T((\widehat{\pmb{\theta}}_n^T - \pmb{\theta}^T)x)\vert \pmb{D}]\\
    &= \mathbb E[x^T(\widehat{\pmb{\theta}}_n - \pmb{\theta})(\widehat{\pmb{\theta}}_n - \pmb{\theta})^Tx\vert D]\\
    &= x^T\operatorname{Var}(\widehat{\pmb{\theta}})x
\end{align*}
$$

ãªãŠï¼Œæœ€å¾Œã®å¼å¤‰å½¢ã¯ $a$ ã‚’å®šæ•°ã¨ã—ãŸã¨ãã« $\operatorname{Var}(a - X) = \operatorname{Var}(X)$ ã‚’ç”¨ã„ã¦ã„ã¾ã™ï¼

:::

::: {.nte- .callout-tip icon="false"}
# ğŸµ Green Tea Break

äºˆæ¸¬ã«ãŠã‘ã‚‹Bias-Varianceåˆ†è§£ã¯ *Overfitting*ï¼Œ*Underfitting* ã¨ã„ã†çµã³ã¤ã‘ã¦è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼

<strong > &#9654;&nbsp; Overfitting</strong>

training dataã«ãŠã‘ã‚‹äºˆæ¸¬ç²¾åº¦ã¯ã¨ã¦ã‚‚è‰¯ã„ãŒtest dataã«ãŠã‘ã‚‹äºˆæ¸¬ç²¾åº¦ãŒã¨ã¦ã‚‚æ‚ªã„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦*Overfitting*ã¨ã„ã†è©•ä¾¡ã‚’ã—ã¾ã™ï¼ã“ã‚Œã¯äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãŒtraining dataã®ãƒã‚¤ã‚ºã‚’æ‹¾ã„ã™ãã¦ã—ã¾ã£ãŸã¨ãã«ç™ºç”Ÿã—ã¾ã™ï¼

*Overfitting*ã¯è¡¨ç¾åŠ›(capacity)ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸã¨ãã«ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ï¼å®Ÿéš›ã«ç™ºç”Ÿã—ãŸã¨ãã¯ï¼Œ

- high bias and high variance
- low bias and high variance
- high bias and low variance

ã„ãšã‚Œã‚‚è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒï¼Œ*high bias and low variance* ã¯ $\epsilon$ ã¯ã™ã”ãå°ã•ã„ãŒi.i.dã§ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„ã¨ãã¨ã„ã†ã‚±ãƒ¼ã‚¹ãªã©ã‚’é™¤ã„ã¦ã‚ã¾ã‚Šãªã„ã®ã§ï¼ŒåŸºæœ¬çš„ã«ã¯high varianceã‚’ç–‘ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã¨ã‚‹ã¨ã„ã†å¯¾å‡¦æ–¹é‡ã¨ãªã‚Šã¾ã™ï¼Œæ­£å‰‡åŒ–ï¼Œã‚ˆã‚Šå¤§ãã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ´»ç”¨ï¼Œinputeã¨ã—ã¦ç”¨ã„ã‚‹ç‰¹å¾´é‡ã®æ•°ã‚’å°‘ãªãã™ã‚‹ï¼Œã‚ˆã‚Šè¡¨ç¾åŠ›ãŒç‹­ã‚ã‚‰ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ç”¨ã„ã‚‹ï¼Œã¨ã„ã£ãŸã“ã¨ã‚’ãƒˆãƒ©ã‚¤ã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ï¼

<strong > &#9654;&nbsp; Underfitting</strong>

*Underfitting*ã¨ã¯ï¼Œtraining dataã«ãŠã„ã¦ã‚‚ååˆ†ãªå­¦ç¿’ãŒã§ãã¦ã„ãªã„çŠ¶æ³ã‚’æŒ‡ã™æ¦‚å¿µã§ã™ï¼
ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¾åŠ›ãŒä½ã„å ´åˆã«èµ·ãã‚‹ã“ã¨ãŒå¤šãï¼Œã¾ãŸhigh biasã®çŠ¶æ³ã¨é–¢é€£ã—ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ã§ã™ï¼ã“ã®å ´åˆï¼Œtraing datasetè‡ªä½“ã«fitã§ãã¦ã„ãªã„ã®ã§ï¼Œã•ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿åé›†ã™ã‚‹ã“ã¨ã¯è²»ç”¨å¯¾åŠ¹æœã«è¦‹åˆã„ã¾ã›ã‚“ï¼ç½°å‰‡é …ã®ç·©å’Œï¼Œç‰¹å¾´é‡ã®æ‹¡å……ï¼Œã‚ˆã‚Šè¡¨ç¾åŠ›ã®ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ©ã‚¤ãŒæ¨å¥¨ã•ã‚Œã¾ã™ï¼


<strong > &#9654;&nbsp; Rule of thumbs</strong>

æ±åŒ–æ€§èƒ½ãŒæ‚ªã„å ´åˆã«ï¼Œã¾ãšbias-varianceã®ã©ã¡ã‚‰ã®è¦å› ãŒé‡ãã‚’å ã‚ã¦ã„ã‚‹ã®ã‹ï¼Ÿã‚’è€ƒãˆã‚‹ã“ã¨ãŒé‡è¦ã§ã™ï¼ãã®åˆ¤åˆ¥æ–¹æ³•ã¨ã—ã¦çµŒé¨“å‰‡çš„ã«çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ï¼Œ

- Training data errorãŒå¤§ãã„å ´åˆã¯ï¼Œã‚‚ã—ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã«ã†ã¾ããƒ•ã‚£ãƒƒãƒˆã§ããªã„ã¨ã„ã†ã“ã¨ã«ãªã‚‹ã®ã§ï¼Œãã®ãƒ¢ãƒ‡ãƒ«ã¯high biasã‚’æŒã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã¨åˆ¤æ–­ã™ã‚‹
- Cross validation errorã¨prediction erro in training datasetã®å·®ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚„æ¨å®šé‡ã®åˆ†æ•£ã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã‚‹ã®ã§ï¼ŒCross validation errorãŒé«˜ã„å ´åˆã¯ *overfitting* ã®çŠ¶æ…‹ã¨åˆ¤æ–­ã™ã‚‹

Training dataset errorã«æ¯”ã¹Cross validation errorãŒé«˜ã„ã¨ãã«ï¼Œç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰neural networkãƒ¢ãƒ‡ãƒ«ã¸å¤‰æ›´ã™ã‚‹ã“ã¨ã¯ã‚ã¾ã‚Šæ„å‘³ã®ãªã„ï¼ˆã‚€ã—ã‚çŠ¶æ³ã‚’æ‚ªåŒ–ã•ã›ã‚‹ï¼‰ã¨ãªã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ã¨ã„ã†ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ãªã‚Šã¾ã™ï¼


:::