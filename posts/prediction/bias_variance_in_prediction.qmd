---
title: "Bias and Variance"
author: "Ryo Nakagami"
date: "2024-10-09"
date-modified: last-modified
number_sections: false
code-fold: true
comments:
    utterances:
         repo: RyoNakagami/statistics-for-regression-monkey
         label: discussion
---

## Bias and Variance in Prediction

æœªçŸ¥ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æ¨å®šé‡ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸçµ±è¨ˆçš„æ¨å®šã¨ã“ã¨ãªã‚Šï¼Œæ•™å¸«ã‚ã‚Šæ©Ÿæ¢°å­¦ç¿’ã®äºˆæ¸¬ã§ã¯
$x$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã« $y$ ã‚’è‰¯ãäºˆæ¸¬ã§ãã‚‹é–¢æ•° $h$ 

$$
\begin{align*}
&y = h(x) + \epsilon\\
&\text{where }  \mathbb E[\epsilon \vert x] = 0
\end{align*}
$$

ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ï¼äºˆæ¸¬é–¢æ•°ã®*è‰¯ã•*ã«ã¤ã„ã¦ã¯æ±ºå®šç†è«–(decision theory)ã§è­°è«–ã•ã‚Œã‚‹ã‚‚ã®ã§ã™ãŒï¼Œä¸€æ—¦L2è·é›¢ã§è‰¯ã•ã‚’åˆ¤æ–­ã™ã‚‹å ´åˆï¼Œè‰¯ã„é–¢æ•°ã¯ $\mathbb E[y\vert x]$ ã¨ãªã‚Šã¾ã™ï¼ã“ã®é–¢æ•°ã®å­¦ç¿’ã‚’ç›®æŒ‡ã—ã¦ï¼Œ

$$
D = \{x^{(i)}, y^{(i)}\}_{i=1}^n
$$

ã‚’training datasetã¨ã—ã¦ï¼Œ$\hat f_D$ ã¨ã„ã†é–¢æ•°ã‚’æ§‹ç¯‰ã—ãŸã¨ã—ã¾ã™ï¼æ¯é›†å›£ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
ã•ã‚ŒãŸ $S$ ã«ã¯ãƒã‚¤ã‚º $\epsilon_i$ ãŒå«ã¾ã‚Œã¦ãŠã‚Šï¼Œ$S$ ã‚’ç”¨ã„ã¦å­¦ç¿’ã—ãŸ $\hat f_D$ ã¯ç¢ºç‡çš„ã«å¤‰å‹•ã™ã‚‹å´é¢ãŒã‚ã‚Šã¾ã™ï¼ãã®ä¸Šã§åŒã˜æ¯é›†å›£ã‹ã‚‰i.i.dã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸæœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\{x^{*}, y^{*}\}$ ã«ãŠã„ã¦ã‚‚ $x$ ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ç”¨ã„ã‚‹ã“ã¨ã§ ç²¾åº¦è‰¯ã $y$ ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ = æ±åŒ–èª¤å·®(generalization error)ãŒå°ã•ã„ $\hat f_D$ ãŒæœ›ã¾ã—ã„äºˆæ¸¬é–¢æ•°ã¨ãªã‚Šã¾ã™ï¼

æ±åŒ–èª¤å·®ã‚’ $y$ ã¨ $\hat f_D$ ã®L2è·é›¢ã¨è€ƒãˆï¼Œ$x^*$ ã§æ¡ä»¶ã¥ã‘ãŸå ´åˆï¼Œ 

$$
\begin{align*}
&\operatorname{MSE}(\hat f_D)\\[5pt]
    &= \mathbb E_{D, y^*}[(y^* - \hat f_D(x^*))^2]\\[5pt]
    &= \mathbb E_{D, y^*}[(h(x^*) + \epsilon - \hat f_D(x^*))^2]\\[5pt]
    &= \mathbb E_{y^*}[\epsilon^2] + \mathbb E_D[(h(x^*) -\hat f_D(x^*))^2]\\[5pt]
    &= \mathbb E_D[\epsilon^2] + \underbrace{\mathbb E_D[h(x^*) -\hat f_D(x^*)]^2}_{=\text{bias and constant}} + \operatorname{Var}(h(x^*) -\hat f_D(x^*))\\[5pt]
    &= \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible error}} + \text{Bias}(\hat f_D)^2 + \operatorname{Var}(\hat f_D)
\end{align*}
$${#eq-pred-bias-variance}

ä¸Šå¼ã®ç¬¬ï¼“é …ã®Varianceã¯äºˆæ¸¬é–¢æ•° $f$ ã®ç‰¹å®šã®traing dataé›†åˆ $D$ ã®é¸ã³æ–¹ã«é–¢ã™ã‚‹æ•æ„Ÿã•ã‚’è¡¨ã—ã¦ã„ã‚‹ã¨è§£é‡ˆã•ã‚Œã¾ã™ï¼


<strong > &#9654;&nbsp; regession with gauss basis function</strong>

ä¸Šè¨˜ã®è­°è«–ã‚’è¸ã¾ãˆã‚‹ã¨ï¼ŒBias, Varianceã‚’ãƒãƒ©ãƒ³ã‚¹è‰¯ãå°ã•ãã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒMSEã®æ„å‘³ã§è‰¯ã„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¨ã„ã†ã“ã¨ã«ãªã‚Šã¾ã™ï¼

çµŒé¨“å‰‡çš„ã«ï¼ŒæŸ”è»Ÿæ€§ã®ã‚ã‚‹è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã¯Biasã¯å°ã•ã„ãŒVarianceãŒå¤§ããï¼Œé€†ã«æŸ”è»Ÿæ€§ã®ä½ã„ãƒ¢ãƒ‡ãƒ«ã¯BiasãŒå¤§ãã„ãŒVarianceãŒå°ã•ã„ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ï¼

ä»¥ä¸‹ã®ä¾‹ã§ã¯ï¼Œ

$$
\begin{align*}
y_i &= \sin(2\pi x_i) + \epsilon_i\\
\epsilon_i &\overset{\mathrm{iid}}{\sim}   N(0, 0.5^2)\\
x_i &\overset{\mathrm{iid}}{\sim} \operatorname{Uniform}(0, 1)
\end{align*}
$$

ã¨ã„ã†Data Generating Processã®ã‚‚ã¨ã§ $N_{train} = 25$ ã‹ã‚‰ãªã‚‹sampleã‚’ç”Ÿæˆã—ï¼Œãã®sampleç”Ÿæˆã‚’100å›ç¹°ã‚Šè¿”ã™å‡¦ç†ã‚’è€ƒãˆã¾ã™ï¼ã¤ã¾ã‚Šï¼Œ

- ãã‚Œãã‚Œã®sample sizeã¯ $N_{train} = 25$
- sampleæ•°ã¯ $L = 100$

ã¨ã„ã†ã“ã¨ã§ã™ï¼24å€‹ã®ã‚¬ã‚¦ã‚¹åŸºåº•ã‚’åŸºåº•é–¢æ•°ã¨ç”¨ã„ãŸRidge Linear Regressionã‚’ãã‚Œãã‚Œã®sampleã«å¯¾ã—ã¦å®Ÿè¡Œã—ï¼Œãã®çµæœå¾—ã‚‰ã‚ŒãŸå„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«(100å€‹ã®äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«)ã®äºˆæ¸¬çµæœã«ã¤ã„ã¦Pythonã§ç¢ºèªã—ã¦ã¿ã¾ã—ãŸï¼

ãªãŠï¼ŒRidge penaltyã¯ $[0.1, 1.5, 15]$ ã¨æ£æ„çš„ã«è¨­å®šã—ã¦ã„ã¾ã™ï¼ãªãŠTest datasetã®sample sizeã¯ $N_{test} = 1000$ ã¨ã—ã¦ã„ã¾ã™

```{python}
import numpy as np
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt

import regmonkey_style.stylewizard as sw

sw.set_templates("regmonkey_line")


## create data function
def true_func(x):
    return np.sin(2 * np.pi * x)


def generate_data(func, sample_size, std, domain=[0, 1]):
    x = np.random.uniform(domain[0], domain[1], sample_size)
    t = func(x) + np.random.normal(scale=std, size=x.shape)
    return x, t


def create_gauss_basis(x, gauss_mean, gauss_var):
    if x.ndim == 1:
        x = x[:, None]
    else:
        assert x.ndim == 2
    basis = [np.ones(len(x))]
    for m in gauss_mean:
        gauss_transfomred = np.exp(-0.5 * np.square(x - m) / gauss_var)
        basis.append(gauss_transfomred.flatten())
    return np.asarray(basis).transpose()


## Generate Data
np.random.seed(1212)
gauss_basis_mean = np.linspace(0, np.pi / 2, 24)
gauss_basis_var = 0.01
penalty_list = [15, 1.5, 0.1]

x_test = np.linspace(0, 1, 1000)
X_model_test = create_gauss_basis(x_test, gauss_basis_mean, gauss_basis_var)
y_test = true_func(x_test)

x_train, y_train = generate_data(true_func, 25 * 100, 0.5)
x_train, y_train = x_train.reshape(100, 25), y_train.reshape(100, 25)


for penalty in penalty_list:
    y_list = []
    fig, axes = plt.subplots(1, 2, figsize=(9, 3))
    for i in range(100):
        X_model_train = create_gauss_basis(
            x_train[i, :], gauss_basis_mean, gauss_basis_var
        )
        clf = Ridge(alpha=penalty)
        clf.fit(X_model_train, y_train[i, :])
        y_pred = clf.predict(X_model_test)
        y_list.append(y_pred)

    for y in y_list[:10]:
        axes[0].plot(x_test, y, alpha=0.8, linewidth=1, label=f"penalty: {penalty:.2f}")
    axes[0].set_title(f"model regression with penalty: {penalty:.1f}", fontsize=10)
    for ax in axes:
        ax.set_ylim(-1.9, 1.9)
    axes[1].plot(x_test, y_test, label="true")
    axes[1].plot(
        x_test, np.asarray(y_list).mean(axis=0), linestyle="--", label="pred-mean"
    )
    axes[1].legend()
plt.show()
```

Bias-Variance Decompositionã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¨™æœ¬ã§è¨ˆç®—ã™ã‚‹å ´åˆã¯ @eq-pred-bias-variance ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¨™æœ¬ã¨å¯¾å¿œã•ã›ã‚Œã°è‰¯ã„ã®ã§

$$
\begin{align*}
\overline{f}(x) &= \frac{1}{L}\sum_l^L\hat f^{(l)}(x)\\
\operatorname{Bias}^2 &= \frac{1}{N_{test}}\sum_{n} \{\overline{f}(x_n) - h(x_n)\}^2\\
\operatorname{Variance} &= \frac{1}{N_{test}}\sum_{n}\frac{1}{L}\sum_l\left\{\hat f^{(l)}(x_n) - \overline{f}(x_n)\right\}^2
\end{align*}
$$


ã“ã®è€ƒãˆã«åŸºã¥ã„ã¦ ridge penalty ã«å¯¾ã™ã‚‹Bias, Variance, MSEã‚’ãã‚Œãã‚Œplotã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•é–¢ä¿‚ãŒç¢ºèªã§ãã¾ã™

```{python}
"""
 * Computes the bias-variance decomposition for a given list of penalty values.
 * 
 * @param penalty_list - A list of penalty values.
 * @returns An array of bias and variance values corresponding to each penalty value.
"""
from multiprocessing import Pool

penalty_list = np.linspace(0.1, 8, 500)
bias_list = []
variance_list = []
h_test = true_func(x_test)

x_train, y_train = generate_data(true_func, 25 * 100, 0.5)
x_train, y_train = x_train.reshape(100, 25), y_train.reshape(100, 25)


def compute_bias_variance(penalty):
    y_list = []
    for i in range(100):
        X_model_train = create_gauss_basis(
            x_train[i, :], gauss_basis_mean, gauss_basis_var
        )
        clf = Ridge(alpha=penalty)
        clf.fit(X_model_train, y_train[i, :])
        y_pred = clf.predict(X_model_test)
        y_list.append(y_pred)
    y_list = np.asanyarray(y_list)
    bias_term = np.mean((h_test - np.mean(y_list, axis=0)) ** 2)
    variance_term = np.mean(np.mean((y_list - np.mean(y_list, axis=0)) ** 2, axis=0))

    return bias_term, variance_term


with Pool() as pool:
    results = pool.map(compute_bias_variance, penalty_list)

bias_list, variance_list = zip(*results)

import plotly.graph_objects as go

# Plot bias vs penalty
fig = go.Figure()
fig.add_trace(go.Scatter(x=penalty_list, y=bias_list, mode="lines", name="Bias"))
fig.add_trace(
    go.Scatter(x=penalty_list, y=variance_list, mode="lines", name="Variance")
)
fig.add_trace(
    go.Scatter(
        x=penalty_list,
        y=np.asarray(bias_list) + np.asarray(variance_list),
        mode="lines",
        name="Total",
    )
)
fig.update_layout(
    title="Bias-Variance decomposition", xaxis_title="Penalty", yaxis_title="Value"
)
fig.show()
```

::: {#exm- .custom_problem }
**ç·šå½¢ã‚¯ãƒ©ã‚¹ã«ãŠã‘ã‚‹Bias and Variance**
<br>

True functionã‚’ 

$$
f(x) = \pmb{\theta}^Tx
$$

ã¨å®šç¾©ã—ï¼Œ$\pmb{\theta}$ ã¯æœªçŸ¥ã§ã‚ã‚‹ã¨ã—ã¾ã™ï¼Training dataset $D$ ã‚’ç”¨ã„ã¦ï¼Œ

$$
\hat f_D(x) = \widehat{\pmb{\theta}}_D^Tx
$$

ã‚’å­¦ç¿’ã—ãŸã¨ã—ã¾ã™ï¼ã“ã®ã¨ãï¼ŒæœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\pmb{S} = \{(x^*, y^*)\}$ ã«ãŠã„ã¦ï¼Œ

$$
\begin{align*}
\text{Bias}(\hat f_D)
    &= \mathbb E[\hat f_D(x) - f(x) \vert \pmb{S}]\\
    &= \mathbb E[\widehat{\pmb{\theta}}_D^Tx - \pmb{\theta}^Tx\vert \pmb{S}]\\
    &= \mathbb E[\widehat{\pmb{\theta}}_D - \pmb{\theta}\vert \pmb{S}]^Tx\\
    &= \text{Bias}(\widehat{\pmb{\theta}}_D)^Tx
\end{align*}
$$

åŒæ§˜ã«

$$
\begin{align*}
&\operatorname{Var}(\hat f_D)\\
    &= \mathbb E[(\hat f_D(x) - f(x))^2 \vert \pmb{S}]\\
    &= \mathbb E[(\widehat{\pmb{\theta}}_D^Tx - \pmb{\theta}^Tx)^2\vert \pmb{S}]\\
    &= \mathbb E[((\widehat{\pmb{\theta}}_D^T - \pmb{\theta}^T)x)^T((\widehat{\pmb{\theta}}_D^T - \pmb{\theta}^T)x)\vert \pmb{S}]\\
    &= \mathbb E[x^T(\widehat{\pmb{\theta}}_D - \pmb{\theta})(\widehat{\pmb{\theta}}_D - \pmb{\theta})^Tx\vert  \pmb{S}]\\
    &= x^T\operatorname{Var}(\widehat{\pmb{\theta}})x
\end{align*}
$$

ãªãŠï¼Œæœ€å¾Œã®å¼å¤‰å½¢ã¯ $a$ ã‚’å®šæ•°ã¨ã—ãŸã¨ãã« $\operatorname{Var}(a - X) = \operatorname{Var}(X)$ ã‚’ç”¨ã„ã¦ã„ã¾ã™ï¼

:::

::: {.nte- .callout-tip icon="false"}
# ğŸµ Green Tea Break

äºˆæ¸¬ã«ãŠã‘ã‚‹Bias-Varianceåˆ†è§£ã¯ *Overfitting*ï¼Œ*Underfitting* ã¨ã„ã†çµã³ã¤ã‘ã¦è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼

<strong > &#9654;&nbsp; Overfitting</strong>

training dataã«ãŠã‘ã‚‹äºˆæ¸¬ç²¾åº¦ã¯ã¨ã¦ã‚‚è‰¯ã„ãŒtest dataã«ãŠã‘ã‚‹äºˆæ¸¬ç²¾åº¦ãŒã¨ã¦ã‚‚æ‚ªã„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦*Overfitting*ã¨ã„ã†è©•ä¾¡ã‚’ã—ã¾ã™ï¼ã“ã‚Œã¯äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãŒtraining dataã®ãƒã‚¤ã‚ºã‚’æ‹¾ã„ã™ãã¦ã—ã¾ã£ãŸã¨ãã«ç™ºç”Ÿã—ã¾ã™ï¼

*Overfitting*ã¯è¡¨ç¾åŠ›(capacity)ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸã¨ãã«ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ï¼å®Ÿéš›ã«ç™ºç”Ÿã—ãŸã¨ãã¯ï¼Œ

- high bias and high variance
- low bias and high variance

ã„ãšã‚Œã‚‚è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒï¼ŒåŸºæœ¬çš„ã«ã¯high varianceã‚’ç–‘ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã¨ã‚‹ã¨ã„ã†å¯¾å‡¦æ–¹é‡ã¨ãªã‚Šã¾ã™ï¼Œå¯¾å‡¦æ–¹æ³•ã¨ã—ã¦ï¼Œ

- æ­£å‰‡åŒ–
- ã‚ˆã‚Šå¤§ãã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ´»ç”¨
- inputeã¨ã—ã¦ç”¨ã„ã‚‹ç‰¹å¾´é‡ã®æ•°ã‚’å°‘ãªãã™ã‚‹
- ã‚ˆã‚Šè¡¨ç¾åŠ›ãŒç‹­ã‚ã‚‰ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ç”¨ã„ã‚‹
- Boostingã‚ˆã‚ŠBagging

ã¨ã„ã£ãŸã“ã¨ã‚’ãƒˆãƒ©ã‚¤ã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ï¼

<strong > &#9654;&nbsp; Underfitting</strong>

*Underfitting*ã¨ã¯ï¼Œtraining dataã«ãŠã„ã¦ã‚‚ååˆ†ãªå­¦ç¿’ãŒã§ãã¦ã„ãªã„çŠ¶æ³ã‚’æŒ‡ã™æ¦‚å¿µã§ã™ï¼
ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¾åŠ›ãŒä½ã„å ´åˆã«èµ·ãã‚‹ã“ã¨ãŒå¤šãï¼Œã¾ãŸhigh biasã®çŠ¶æ³ã¨é–¢é€£ã—ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ã§ã™ï¼ã“ã®å ´åˆï¼Œtraing datasetè‡ªä½“ã«fitã§ãã¦ã„ãªã„ã®ã§ï¼Œã•ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿åé›†ã™ã‚‹ã“ã¨ã¯è²»ç”¨å¯¾åŠ¹æœã«è¦‹åˆã„ã¾ã›ã‚“ï¼

- ç½°å‰‡é …ã®ç·©å’Œ
- ç‰¹å¾´é‡ã®æ‹¡å……
- ã‚ˆã‚Šè¡¨ç¾åŠ›ã®ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ©ã‚¤
- Boosting

ãŒæ¨å¥¨ã•ã‚Œã¾ã™ï¼


<strong > &#9654;&nbsp; Rule of thumbs</strong>

æ±åŒ–æ€§èƒ½ãŒæ‚ªã„å ´åˆã«ï¼Œã¾ãšbias-varianceã®ã©ã¡ã‚‰ã®è¦å› ãŒé‡ãã‚’å ã‚ã¦ã„ã‚‹ã®ã‹ï¼Ÿã‚’è€ƒãˆã‚‹ã“ã¨ãŒé‡è¦ã§ã™ï¼ãã®åˆ¤åˆ¥æ–¹æ³•ã¨ã—ã¦çµŒé¨“å‰‡çš„ã«çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ï¼Œ

- Training data errorãŒå¤§ãã„å ´åˆã¯ï¼Œã‚‚ã—ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã«ã†ã¾ããƒ•ã‚£ãƒƒãƒˆã§ããªã„ã¨ã„ã†ã“ã¨ã«ãªã‚‹ã®ã§ï¼Œãã®ãƒ¢ãƒ‡ãƒ«ã¯high biasã‚’æŒã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã¨åˆ¤æ–­ã™ã‚‹
- Cross validation errorã¨prediction erro in training datasetã®å·®ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚„æ¨å®šé‡ã®åˆ†æ•£ã¨ã—ã¦æ‰±ã†ã“ã¨ãŒã§ãã‚‹ã®ã§ï¼ŒCross validation errorãŒé«˜ã„å ´åˆã¯ *overfitting* ã®çŠ¶æ…‹ã¨åˆ¤æ–­ã™ã‚‹

Training dataset errorã«æ¯”ã¹Cross validation errorãŒé«˜ã„ã¨ãã«ï¼Œç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰neural networkãƒ¢ãƒ‡ãƒ«ã¸å¤‰æ›´ã™ã‚‹ã“ã¨ã¯ã‚ã¾ã‚Šæ„å‘³ã®ãªã„ï¼ˆã‚€ã—ã‚çŠ¶æ³ã‚’æ‚ªåŒ–ã•ã›ã‚‹ï¼‰ã¨ãªã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ã¨ã„ã†ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ãªã‚Šã¾ã™ï¼


:::

## Decomposition of Expected Test Error

ä¸Šã§ã¯æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\{x^*, y^*\}$ ã«ã¤ã„ã¦ $\{x^*\}$ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã—ãŸã¨ãã®Bias-Variance Decompositionã‚’ç¢ºèªã—ã¾ã—ãŸãŒ
æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $(\pmb{x}, y)$ ã¨training dataset $D$ ã«ã¤ã„ã¦ã®exptected decompositonã‚’ç¢ºèªã—ã¦ã„ãã¾ã™ï¼

ãªãŠï¼ŒExpected Prediction functionã«ã¤ã„ã¦ä»¥ä¸‹ã®notationã‚’å°å…¥ã—ã¾ã™

$$
\bar{f} = E_{D} \left[ \hat f_D \right] = \int\limits_D \hat f_D \Pr(D) \partial D
$$

$\bar{f}$ ã¯ $D$ ã®ç¢ºç‡ã§é‡ã¿ã¥ã‘ãŸãã‚Œãã‚Œã®äºˆæ¸¬é–¢æ•°ã®weighted averageã¨ç†è§£ã§ãã¾ã™ï¼

$$
\begin{align*}
&\mathbb E_{\pmb x, y, D}[(\hat f_D(\pmb x) - y)^2]\\
&= \mathbb E_{\pmb x, y, D}[(\hat f_D(\pmb x) - \bar{f}(\pmb x) + \bar{f}(\pmb x) - y)^2]\\
&= \underbrace{\mathbb E_{\pmb x, D}[(\hat f_D(\pmb x) - \bar{f}(\pmb x))^2]}_{\text{Variance}} +  \mathbb E_{\pmb x, y}[(\bar{f}(\pmb x) - y)^2]
\end{align*}
$${#eq-bais-variance-decomposition-2}

ãªãŠé€”ä¸­ã®å¼å¤‰å½¢ã«ãŠã„ã¦æ¬¡ã®æ€§è³ªã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™

$$
\begin{align*}
	&\mathbb E_{\mathbf{x}, y, D} \left[\left(\hat f_{D}(\mathbf{x}) - \bar{f}(\mathbf{x})\right) \left(\bar{f}(\mathbf{x}) - y\right)\right] \\
    &= \mathbb E_{\mathbf{x}, y} \left[E_{D} \left[ \hat f_{D}(\mathbf{x}) - \bar{f}(\mathbf{x})\right] \left(\bar{f}(\mathbf{x}) - y\right) \right] \because{\text{LIE}}\\
    &= \mathbb E_{\mathbf{x}, y} \left[ \left( E_{D} \left[ \hat f_{D}(\mathbf{x}) \right] - \bar{f}(\mathbf{x}) \right) \left(\bar{f}(\mathbf{x}) - y \right)\right] \\
    &= \mathbb E_{\mathbf{x}, y} \left[ \left(\bar{f}(\mathbf{x}) - \bar{f}(\mathbf{x}) \right) \left(\bar{f}(\mathbf{x}) - y \right)\right] \\
    &= \mathbb E_{\mathbf{x}, y} \left[ 0 \right] \\
    &= 0
\end{align*}
$$

æ¬¡ã«ï¼Œ@eq-bais-variance-decomposition-2 ã®ç¬¬äºŒé …ã«ã¤ã„ã¦è€ƒãˆã¾ã™ï¼

$$
\begin{align*}
&\mathbb E_{\pmb x, y}[(\bar{f}(\pmb x) - y)^2]\\
&= \mathbb E_{\pmb x, y}[(\bar{f}(\pmb x) - h(\pmb x) + h(\pmb x) - y)^2]\\
&= \mathbb E_{\pmb x}[(\bar{f}(\pmb x) - h(\pmb x))^2] + \mathbb E_{\pmb x, y}[(h(\pmb x) - y)^2]\\
&= \mathbb E_{\pmb x}[\text{Bias}^2(\pmb{x})] + \mathbb E_{\pmb x}[\mathbb E[\epsilon^2 \vert \pmb{x}]]\\
&= \mathbb E[\text{Bias}^2] + \mathbb E[\epsilon^2]
\end{align*}
$$

ä»¥ä¸Šã‚ˆã‚Š

$$
\mathbb E_{\pmb x, y, D}[(\hat f_D(\pmb x) - y)^2] = \mathbb E[\text{Variance of }\hat f_D] + \mathbb E[\text{Bias}^2] + \mathbb E[\epsilon^2]
$$
