---
title: "期待値"
author: "Ryo Nakagami"
date: "2024-09-12"
date-modified: last-modified
number_sections: false
code-fold: true
comments:
    utterances:
         repo: RyoNakagami/statistics-for-regression-monkey
         label: discussion
jupyter: python3
---

確率的変動を考慮する際に，ランダムネスを要約した統計量として期待値を用いる場面は多いです．例えば，
マーケティング担当者は，プロモーションの提供によって得られるリターンの期待値に基づいてプロモーションの実施を判断しますし，
投資家はさまざまな金融商品の期待リターンとリスクに基づいてポートフォリオを選択したりします．

## 期待値の性質

<div class="blog-custom-border">
<strong>Def: 連続確率変数の期待値</strong> <br>

$f$ を確率変数 $X$ の確率密度関数とする．$\int_{\mathbb R} \vert x\vert f(x) \mathrm{d}x < \infty$ 
のとき，$X$ の期待値は以下のように定義する:

$$
\mathbb E[X] = \int_{\mathbb R} x f(x) \mathrm{d}x
$$

また，$X$ の関数 $g(X)$ の期待値は $\int_{\mathbb R} \vert g(x)\vert f(x) \mathrm{d}x < \infty$ ならば

$$
\mathbb E[g(X)] = \int_{\mathbb R} g(x) f(x) \mathrm{d}x
$$


</div>

定義より確率密度関数で重みづけた平均が確率変数の期待値になると解釈することができます．meanは分布の位置を表すパラメーターとも解釈できるので
**location parameter（位置母数）**と呼ぶこともあります．一方，標準偏差 $\sigma$ は**scale parameter（尺度母数）**といいます．

::: {#exm- .custom_problem }
**指数分布の期待値**
<br>

rate parameter $\lambda$ の指数分布に従う確率変数 $X$ を考えます．

$$
\begin{align*}
\mathbb E[X] &= \int^\infty_0 x \lambda \exp(-\lambda x)\mathrm{d}x\\
             &= \bigg[-x\exp(-\lambda x)\bigg]^\infty_0 + \int^\infty_0 \exp(-\lambda x)\mathrm{d}x\\
             &= \int^\infty_0 \exp(-\lambda x)\mathrm{d}x\\
             &= -\frac{1}{\lambda}\bigg[\exp(-\lambda x)\bigg]^\infty_0\\
             &= \frac{1}{\lambda}
\end{align*}
$$


指数分布は電球の寿命などに応用される分布ですが，rate parameter $\lambda$ が小さいほど期待値（= 電球の寿命）が大きくなることが分かります．

:::

::: {#exm- .custom_problem }
**期待値が定義できない離散分布**
<br>

確率変数 $X$ のsupportを加算集合 $\{2, 2^2, 2^3, \cdots\}$ とする．確率関数を

$$
\Pr(X = 2^i) = \frac{1}{2^i} \quad (i = 1, 2, \cdots)
$$

このとき，

$$
\sum_{i=1}^\infty \Pr(X=2^i) = \sum_{i=1}^\infty\frac{1}{2^i} = 1
$$

と確率の公理を満たしていることが分かる．一方，

$$
\begin{align*}
\mathbb E[X]
    &= \sum_{i=1}^\infty 2^i \frac{1}{2^i}\\
    &= \sum_{i=1}^\infty 1 = \infty
\end{align*}
$$

従って，確率変数 $X$ の分布は，期待値が定義できない分布であることがわかる．

:::

::: {#exm- .custom_problem }
**期待値が定義できない連続分布**
<br>

確率密度関数
$$
f(x) = \begin{cases}
0 & x < 1\\
\frac{1}{x^2} & x\geq 1
\end{cases}
$$

という確率変数 $X$ を考える．

$$
\begin{align*}
\int_1^\infty f(x) \mathrm{d}x
    &= \left[\frac{1}{x}\right]^1_\infty = 1
\end{align*}
$$

一方，

$$
\begin{align*}
\mathbb E[X]
    &= \int_1^\infty xf(x) \mathrm{d}x\\
    &= \int_1^\infty\frac{1}{x}\mathrm{d}x\\
    &= \left[\log(x)\right]_1^\infty = \infty
\end{align*}
$$

従って，確率変数 $X$ の分布は，期待値が定義できない分布であることがわかる．

:::

<strong > &#9654;&nbsp; 離散確率変数の変数変換と期待値</strong>

離散型確率変数 $X$ について $Y = g(X)$ を考えたとき，$A_y = \{x\vert g(x)=y\}$ とおくと

$$
\begin{align*}
\mathbb E[g(X)]
    &= \sum_x p(x)g(x)\\
    &= \sum_y\sum_{A_y} p(x)g(x)\\
    &= \sum_y\sum_{A_y} p(x)y\\
    &= \sum_y y \sum_{A_y} p(x)\\
    &= \sum_y \Pr(Y=y)y\\
    &= \mathbb E[Y]
\end{align*}
$$

上の式展開では, $\Pr(X=x) = p(x)$ としています．

<strong > &#9654;&nbsp; 期待値と重心</strong>

期待値の解釈の１つとして 確率変数 $X$ の重心と考えるパターンがあります．$x_i$ の値を $p_i$ の確率でとる離散型確率変数 $X$
の場合，

- 重さのない棒の中央を原点とする
- 原点から右側をプラス，左側をマイナスとして，原券からの距離 $x_i$ の場所に重さ $p_i$ のおもりを吊り下げる

このとき，重心 $\mu$ はモーメントの釣り合い = 右回りモーメントが0になる地点となりますが

$$
\begin{align*}
\text{右回りモーメント}
    &= \sum_i (x_i - \mu)p_i\\
    &= \sum x_ip_i - \sum_i p_i \mu\\
    &= \mathbb E[X] - \mu = 0
\end{align*}
$$

従って，$\mathbb E[X] = \mu$ より，期待値と重心が対応することがわかります．連続型確率変数でも確率密度関数を重さのある棒の断面積と
みなすことで離散型と同じく期待値と重心が対応することを確かめることができます．

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Tail probabilities**
<br>

$[0, b]$ の定義域をもつ非負確率変数 $X$ を考える．$F$ を累積分布関数とするとき

$$
\mathbb E[X] = \int_0^b (1 - F(x))\mathrm{d}x
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\bigg[xF(x)\bigg]^b_0 = \int^b_0xf(x) \mathrm{d}x + \int^b_0F(x) \mathrm{d}x
$$

を用いると

$$
\begin{align*}
\mathbb E[X] &= b - \int^b_0F(x) \mathrm{d}x\\
             &= \int^b_0 1 \mathrm{d}x - \int^b_0F(x) \mathrm{d}x\\
             &= \int_0^b (1 - F(x))\mathrm{d}x
\end{align*}
$$


:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Tail probabilities**
<br>

$\mathbb E[\vert X\vert]<\infty$ をもつ非負の確率変数 $X$ について，$F(x)$ を分布関数とすると

$$
\mathbb E[X] = \int_0^\infty (1 - F(x))\mathrm{d}x
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\int_0^\infty (1 - F(x))\mathrm{d}x
    &= \int_0^\infty \Pr(X\geq x)\mathrm{d}x\\
    &= \int_0^\infty \mathbb E[\mathbb 1(X\geq x)]\mathrm{d}x\\
    &= \int_0^\infty \int_0^\infty \mathbb 1(X\geq x) \mathrm{d}F(x)\mathrm{d}x\\
    &= \int_0^\infty \left\{\int_0^\infty \mathbb 1(X\geq x) \mathrm{d}x\right\}\mathrm{d}F(x) \quad\because\text{積分の順序交換}\\
    &= \mathbb E\left[\int_0^\infty \mathbb 1(X\geq x) \mathrm{d}x\right]\\
    &= \mathbb E\left[\int_0^X 1 \mathrm{d}x\right]\\
    &= \mathbb E[X]
\end{align*}
$$

:::

$\lim_{b\to\infty}\bigg[x(1 - F(x))\bigg]^b_0$ について，$\lim_{b\to\infty}b(1 - F(b))=0$ とは限らない点に注意が必要です．

<div class="blog-custom-border">
::: {#thm- .custom_problem }
<br>

$[0, \infty)$ の定義域をもつ非負確率変数 $X$ を考える．$\mathbb E[\vert X^{p+1} \vert] <\infty$ が定義可能及び， $F$ を累積分布関数とするとき

$$
\mathbb E[X^p] = \int_0^\infty px^{p-1} (1 - F(x))\mathrm{d}x \quad \text{where } p > 0
$$

:::
</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\bigg[x^p(1 - F(x))\bigg]^\infty_0 = \int^\infty_0 p x^{p-1}(1 -F(x))\mathrm{d}x - \int^\infty_0 x^{p}f(x)\mathrm{d}x
\end{align*}
$$

$\text{RHS} = 0$ であるので

$$
\mathbb E[X^p] = \int_0^\infty px^{p-1} (1 - F(x))\mathrm{d}x 
$$

:::

::: {#exm-discrete-tail .custom_problem }
**: Discrete Tail Probability**
<br>

同様の考えで定義域を $0,1,2,3,\cdots$ とする離散確率変数 $X$ について

$$
\mathbb E[X] = \sum_{k=0}^\infty \Pr(X > k)
$$

が成立します．

$$
\begin{align*}
\Pr(X > k) &= \Pr(X = k+1) + \Pr(X = k+2) + \cdots\\
           &= \sum_{l=k+1}^\infty \Pr(X=l)
\end{align*}
$$

従って，

$$
\begin{align*}
\sum_{k=0}^\infty \Pr(X > k) &= \sum_{k=0}^\infty \sum_{l=k+1}^\infty \Pr(X=l)\\
                             &= \sum_{l=1}^\infty\sum_{k=0}^{l-1}\Pr(X=l) \quad\because \Pr(X=l) > 0 \\
                             &= \sum_{l=1}^\infty l\Pr(X=l)\\
                             &= \sum_{l=0}^\infty l\Pr(X=l)\\
                             &= \mathbb E[X]
\end{align*}
$$
$$\tag*{\(\blacksquare\)}$$

:::

::: {#exm- .custom_problem }
<br>

$0,1,2,3,\cdots$ とする離散確率変数 $X$ について

$$
\mathbb E[X^2] = \sum_{k=0}^\infty \Pr(X > k)(2k+1)
$$

も成立する．

$$
\begin{align*}
\sum_{k=0}^\infty \Pr(X > k)(2k+1) 
    &= \sum_{k=0}^\infty \sum_{l=k+1}^\infty \Pr(X=l)(2k+1)\\
    &= \sum_{l=1}^\infty \sum_{k=0}^{l-1}\Pr(X=l)(2k+1)\\
    &= \sum_{l=1}^\infty \Pr(X=l)\sum_{k=0}^{l-1}(2k+1)\\
    &= \sum_{l=1}^\infty l^2\Pr(X=l)\\
    &= \mathbb E[X^2]
\end{align*}
$$

$$\tag*{\(\blacksquare\)}$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**期待値の線型性**
<br>

$a, b$ を実数，確率変数 $X, Y$ について以下が成り立つ

$$
\mathbb E[aX + bY] = a\mathbb E[X] + b\mathbb E[Y]
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

確率変数 $X, Y$ が有限加算な標本空間で定義されているケースにて以下を示す．

1. $\mathbb E[X + Y] = \mathbb E[X] + \mathbb E[Y]$
2. $\mathbb E[cX] = c\mathbb E[X]$

<strong > &#9654;&nbsp; 1. $\mathbb E[X + Y] = \mathbb E[X] + \mathbb E[Y]$</strong>

確率変数 $X$ は $\{x_1, \cdots, x_m\}$, 確率変数 $Y$ は $\{y_1, \cdots, y_n\}$ の値をそれぞれ取りうるとする．
このとき，$Z = X + Y$ の標本空間 $\{z_1, \cdots, z_k\}$ について $k\leq m + n$ が成り立つ．

$A_l = \{(i,j): x_i + y_j = z_l\}$ としたとき，

$$
\begin{align*}
\mathbb E[X+Y]
    &= \sum_{l=1}^kz_l\Pr(A_l)\\
    &= \sum_{l=1}^k\sum_{(i,j)\in Z_l}(x_i + y_j)\Pr(x_i, y_j)\\
    &= \sum_{i=1}^m\sum_{j=1}^n(x_i + y_j)\Pr(x_i, y_j)\\
    &= \sum_{i=1}^m\sum_{j=1}^nx_i\Pr(x_i, y_j) + y_j\Pr(x_i, y_j)\\
    &= \sum_{i=1}^m\sum_{j=1}^n[x_i\Pr(x_i, y_j) + y_j\Pr(x_i, y_j)]\\
    &= \sum_{i=1}^mx_i\sum_{j=1}^nPr(x_i, y_j) + \sum_{j=1}^ny_j\sum_{i=1}^m\Pr(x_i, y_j)\\
    &=\sum_{i=1}^mx_i \Pr(x_i) + \sum_{j=1}^ny_j \Pr(y_j)\\
    &= \mathbb E[X] + \mathbb E[Y]
\end{align*}
$$


<strong > &#9654;&nbsp; 2. $\mathbb E[cX] = c\mathbb E[X]$</strong>

$$
\begin{align*}
\mathbb E[cX]
    &= \sum_{i=1}^m cx_i = \Pr(cX = cx_i)\\
    &= c\sum_{i=1}^m x_i = \Pr(X = x_i)\\
    &= c\mathbb E[X]
\end{align*}
$$

:::


::: {#exm- .custom_problem }
**: 変数変換と分散**
<br>

mean $\mu$ をもつ確率変数 $X$ と実数 $a, b$ について

$$
\operatorname{Var}(aX + b) = a^2\operatorname{Var}(X)
$$

が成立します．証明は以下，

$$
\begin{align*}
\operatorname{Var}(aX + b) 
    &= \mathbb E[(aX + b) - (a\mu +b)^2]\\
    &= \mathbb E[a^2(X - \mu)^2]\\
    &= a^2 \mathbb E[(X - \mu)^2]\\
    &= a^2\operatorname{Var}(X)
\end{align*}
$$

$$\tag*{\(\blacksquare\)}$$

:::


<div class="blog-custom-border">
::: {#thm- .custom_problem }
**positive operator**
<br>

確率変数 $X, Y$ について，$X\geq Y$ が成り立つとき，

$$
\mathbb E[X] \geq \mathbb E[Y]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$X\geq Y$ より $X - Y \geq 0$. 期待値はpositive operatorなので

$$
\mathbb E[X - Y] \geq 0
$$

従って，期待値の線型性を用いると

$$
\begin{align*}
\mathbb E[X - Y] &= \mathbb E[X] - \mathbb E[Y] \geq 0
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
<br>

確率変数 $X$ について,

$$
\mathbb E[\vert X \vert] \geq \vert \mathbb E[X] \vert
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\vert X\vert \geq X$ より 

$$
\mathbb E[\vert X \vert] \geq \mathbb E[X]
$$ 

また, $\vert X\vert + X \geq 0$ より，$\mathbb E[\vert X\vert + X] \geq 0$，
つまり，

$$
\mathbb E[\vert X \vert] \geq -\mathbb E[X]
$$

以上より，$\mathbb E[\vert X \vert] \geq \vert \mathbb E[X] \vert$


:::




<div class="blog-custom-border">
::: {#thm- .custom_problem }
**互いに独立な確率変数の積の期待値**
<br>

$\mathbb E[\vert X\vert ]<\infty, \mathbb E[\vert Y\vert ]<\infty$ を満たす, 確率空間 $(\Omega, \mathscr{F},P)$ 上で定義された確率変数 $X, Y$ を考える．
$X \perp Y$ であるとき，次が成立する

$$
\mathbb E[XY] = \mathbb E[X]\mathbb E[Y]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[XY] &= \int\int_\Omega xy f(x, y)\mathrm{d}x\mathrm{d}y\\
              &= \int\int_\Omega xy f_X(x)f_Y(y)\mathrm{d}x\mathrm{d}y \quad\because{\text{independence}}\\
              &= \left(\int xf_X(x)\mathrm{d}x\right)\left(\int yf_Y(y)\mathrm{d}y\right)\\
              &= \mathbb E[X]\mathbb E[Y]
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Schwarz inquality**
<br>

確率変数 $X, Y$ についてシュワルツの不等式が成立することを示せ

$$
\left(\mathbb E[XY]\right)^2 \leq \mathbb E[X^2]\mathbb E[Y^2]
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

Quadratic functionを以下のように定義します

$$
\begin{align*}
g(t) 
    &= \mathbb E[(tX - Y)^2]\\
    &= t^2\mathbb E[X^2] - 2t\mathbb E[XY] + E[Y^2]\\
    &\geq 0
\end{align*}
$$

このとき，$g(t)$ はnon-negativeなので判別式について以下が成立する

$$
D/4 = \left(\mathbb E[XY]\right)^2 - \mathbb E[X^2]\mathbb E[Y^2]\leq 0 
$$

従って，$\left(\mathbb E[XY]\right)^2 \leq \mathbb E[X^2]\mathbb E[Y^2]$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: Triangle inequality**
<br>

確率変数 $X, Y$ について，以下のような三角不等式が成立することを示せ

$$
\sqrt{\mathbb E[(X+Y)^2]} \leq \sqrt{\mathbb E[X^2]} + \sqrt{\mathbb E[Y^2]}
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

シュワルツの不等式を用いて以下のように示せる

$$
\begin{align*}
\mathbb E[(X+Y)^2]
    &= \mathbb E[X^2] + 2\mathbb E[XY] + \mathbb E[Y^2]\\
    &= \mathbb E[X^2] + 2\sqrt{(\mathbb E[XY])^2} + \mathbb E[Y^2]\\
    &\leq \mathbb E[X^2] + 2\sqrt{\mathbb E[X^2]\mathbb E[Y^2]} + \mathbb E[Y^2]\\
    &= (\sqrt{\mathbb E[X^2]} + \sqrt{\mathbb E[Y^2]})^2
\end{align*}
$$

両辺について，square rootをとると，

$$
\sqrt{\mathbb E[(X+Y)^2]} \leq \sqrt{\mathbb E[X^2]} + \sqrt{\mathbb E[Y^2]}
$$


:::



### 条件付き期待値

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Law of Total Expectation**
<br>

$$
\mathbb E[Y] = \mathbb E[\mathbb E[Y\vert X]]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[\mathbb E[Y\vert X]]
    &= \int \mathbb E[Y\vert X=u]f_X(u)\mathrm{d}u\\
    &= \int \left[\int t f_Y(t\vert x=u)\mathrm{d}t\right]f_X(u)\mathrm{d}u\\
    &= \int \int t f_Y(t\vert x=u)f_X(u)\mathrm{d}u\mathrm{d}t\\
    &= \int t\left[\int f_{X,Y}(u, t)\mathrm{d}u\right]\mathrm{d}t\\
    &= \int t f_Y(t)\mathrm{d}t\\
    &= \mathbb E[Y]
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: CEF Decomposition Property**
<br>

確率変数 $X, Y$ について，

$$
Y = \mathbb E[Y\vert X] + \epsilon
$$

としたとき，

1. $\epsilon$ は $X$ について mean-independent, i.e., $\mathbb E[\epsilon\vert X] = 0$
2. $\epsilon$ は $X$ の任意の関数に対して無相関, i.e., $\operatorname{Cov}(h(X), \epsilon) = 0$
3. $\operatorname{Var}(\epsilon) = \mathbb E[\operatorname{Var}(Y\vert X)]$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

<strong > &#9654;&nbsp; (1)</strong>

$$
\begin{align*}
\mathbb E[\epsilon\vert X] 
    &= \mathbb E[Y - \mathbb E[Y\vert X]\vert X]\\
    &= \mathbb E[Y\vert X] - \mathbb E[Y\vert X]\\
    &= 0
\end{align*}
$$

<strong > &#9654;&nbsp; (2)</strong>

$$
\begin{align*}
\mathbb E[h(X)\epsilon] &= E[\mathbb E[h(X)\epsilon\vert X]]\\
                        &= E[h(X)\mathbb E[\epsilon\vert X]]\\
                        &= 0 \quad \because{\text{mean independence}}
\end{align*}
$$

<strong > &#9654;&nbsp; (3)</strong>

条件付き期待値の公式 $\mathbb E[g(X, Y)] = \mathbb E_X[\mathbb E_{Y\vert X}[g(X, Y)\vert X]]$ より

$$
\begin{align*}
\operatorname{Var}(\epsilon)
    &= \mathbb E[(Y - \mathbb E[Y\vert X] )^2]\\
    &= \mathbb E\{\mathbb E[(Y - \mathbb E[Y\vert X])^2 \vert X]\}\\
    &= \mathbb E[\operatorname{Var}(Y\vert X)]
\end{align*}
$$

:::

<div class="blog-custom-border">
<strong >📘 REMARKS</strong> <br>

CEF Decomposition Propertyは，確率変数 $Y$ は確率変数 $X$ で説明できるパートと，$X$ の任意の関数と直行（orthogonal）
な誤差項のパートに分解できることを示しています．

</div>

### Moments

<div class="blog-custom-border">
<strong>Def: モーメント</strong> <br>

$0 < r < \infty$ を満たすような正の整数 $r$ に対して，確率変数 $X$ のthe r-th moment（原点周りのモーメント）は以下のように定義される

$$
\mathbb E[X^r] = \int_{\mathbb R} x^r \mathrm{d}F_X(x)
$$

the r-th central moment(平均周りのモーメント)は $\mathbb E[(X - \mathbb E[X])^r]$ と定義される．

</div>

原点周りのモーメントと平均周りのモーメントは以下のような関係で理解することができます．$\mathbb E[X] = \mu$ とすると，

$$
\begin{align*}
(X - \mu + \mu)^r
    &= \sum_{k=0}^k \left(\begin{array}{c}r\\ k\end{array}\right)(X - \mu)^k\mu^{r-k}
\end{align*}
$$

両辺の期待値をとると

$$
\begin{align*}
\mathbb E[X^r] = \sum_{k=0}^k \left(\begin{array}{c}r\\ k\end{array}\right)\mathbb E[(X - \mu)^k]\mu^{r-k}
\end{align*}
$$

また，平均周りのモーメントを原点周りのモーメントで表すとなると，

$$
\begin{align*}
(X - \mu)^r = \sum_{k=0}^r(-1)^{r-k}X^r\mu^{r-k}
\end{align*}
$$

同様に期待値をとると

$$
\begin{align*}
\mathbb E[(X - \mu)^r] = \sum_{k=0}^r(-1)^{r-k}\mathbb E[X^r]\mu^{r-k}
\end{align*}
$$


<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: higher moment and lower moment**
<br>

$\mathbb E[\vert X\vert^r] < \infty$ のとき，$0 < q < r$ について，$\mathbb E[\vert X\vert^q] < \infty$ が成立する．

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

関数 $g:\mathbb R\to \mathbb R$ を

$$
g(x) = \vert x\vert^r + 1
$$

関数 $g:\mathbb h\to \mathbb R$ を $h(x) = \vert x\vert^q$ と定義する．$0 < q < r$ より，

$$
h(x) < g(x) \quad \forall x \in \operatorname{support}(X) 
$$

このとき，

$$
\begin{align*}
\int_{\mathbb{R}} |x|^r + 1 \mathrm{d}F 
    &= \int_{\mathbb{R}} |x|^r\mathrm{d}F + \underbrace{\int_{\mathbb{R}} 1\mathrm{d}F}_{=1}\\
    &> \int_{\mathbb{R}} |x|^s\mathrm{d}F
\end{align*}
$$

従って，

$$
\begin{align*}
\infty &> \mathbb E[\vert X\vert^r] + 1\\
       &> \int_{\mathbb{R}} |x|^q \mathrm{d}F \\
       &= \mathbb E[\vert X\vert^q]
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: Mean minimizes squared error**
<br>

確率変数 $X$ について，$\mathbb E[\vert X\vert^2]<\infty$ とする．このとき，

$$
\mathbb E[X] = \arg\min_b \mathbb E[(X - b)^2]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\mu_X = \mathbb E[X], \sigma^2 = \mathbb E[(X - \mu_X)^2]$ とおく．

$$
\begin{align*}
 E[(X - b)^2] 
    &= \mathbb E[(X - \mu_X + \mu_X - b)^2]\\\
    &= \mathbb E[(X - \mu_X)^2] + \mathbb E[(\mu_X - b)^2] + \mathbb E[(X - \mu_X)(\mu_x - b)]\\
    &= \sigma^2 + \mathbb E[(\mu_X - b)^2] + (\mu_X - b)\mathbb E[X - \mu_X]\\
    &= \sigma^2 + \mathbb E[(\mu_X - b)^2]
\end{align*}
$$

$\mathbb E[(\mu_X - b)^2] \geq 0$ が成立し，また, 等号成立条件は $\mu_X = b$．従って，

$$
\mathbb E[X] = \arg\min_b \mathbb E[(X - b)^2]
$$

:::



### Markov and Chebyshev Inequalities

確率変数 $X$ について，確率密度関数や分布関数がわかっている状況は少ないです．また，データが得られたとしても
それらを計算することはかんたんではありません．その中で，

- $X$ が mean $\mu$ からどれくらい離れる可能性があるのか
- $\Pr(\vert X \leq a\vert )$ のupper boundはどれくらいか？

という統計的推測をしたいときに使用されるMarkov and Chebyshev Inequalitiesを解説します．


<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Markov’s Inequality**
<br>

non-negative 確率変数 $X \geq 0$，constant $k >0$ について以下が成立する

$$
\Pr(X \geq k) \leq \frac{\mathbb E[X]}{k}
$$

つまり，

$$
\Pr(X \geq k\mathbb E[X]) \leq \frac{1}{k}
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[X] &= \int_0^\infty xf(x)\mathrm{d}x\\
             &= \int_0^k xf(x)\mathrm{d}x + \int_k^\infty xf(x)\mathrm{d}x\\
             &\geq \int_k^\infty xf(x)\mathrm{d}x\\
             &\geq \int_k^\infty kf(x)\mathrm{d}x\\
             &= k \Pr(X \geq k)
\end{align*}
$$

:::

::: {.callout-note collapse="false" icon=false}
## Proof: 変数変換

$$
Y = 
\begin{cases}
    0 & \text{if} X < k\\[5pt]
    k & \text{if} X \geq k
\end{cases}
$$

のように変数変換をすると常に $Y \leq X$ であるので $\mathbb E[Y] \leq \mathbb E[X]$.

$$
\begin{align*}
&\mathbb E[Y] = k\Pr(X\geq k)\\
\Rightarrow &\Pr(X\geq k)\leq \frac{\mathbb E[X]}{k}
\end{align*}
$$

:::




<div class="blog-custom-border">
<strong >📘 REMARKS</strong> <br>

- Markov’s inequalityは 確率変数 $X$ がnon-negative, population mean $\mu$ の知識のみで使用可能
- 一方，bound幅は大きく，weakest inequalityである

</div>

::: {#exm- .custom_problem }
<br>

点数範囲が $\Omega_x=[0, 110]$ の試験をついて，そのテストスコア確率変数 $X$ を考える．分布の情報はわからないが
population meanは 25 であることが知られている．このとき，$\Pr(X \geq 100)$ のupper boundはMarkov's inequalityを用いて
以下のように計算できます．

$X$ がnon-negativeなので

$$
\begin{align*}
\Pr(X\geq 100) &\leq \frac{25}{100}\\
               &= \frac{1}{4}
\end{align*}
$$

:::

::: {#exm-binom-markov .custom_problem }
**: weak inequality**
<br>

$X_i \overset{\mathrm{iid}}{\sim} \operatorname{Bernoulli}(0.2)$ を20回繰り返す試行を考える．この試行の結果のアウトカムを $Y$ としたとき，

$$
\Pr(Y \geq 16) = \sum_{k=16}^{20} {}_{20}C_{k} 0.2^k 0.8^{20-k} \approx 1.38\cdot 10^{-8}
$$

一方，Markov's inequalityを用いると

$$
\begin{align*}
\Pr(Y \geq 16) \leq \frac{4}{16} = \frac{1}{4}
\end{align*}
$$

このように，bound幅は大きいことが分かる．

:::


<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Chebyshev’s inequality**
<br>

$X \sim D(\mu, \sigma^2)$ とする．ただし，$D$ の形状はわからない．実数 $\alpha >0$ について，以下が成立する

$$
\Pr(\vert X - \mu \vert \geq \alpha) \leq \frac{\sigma^2}{\alpha^2}
$$

つまり，

$$
\Pr(\vert X - \mu \vert \geq \alpha \sigma) \leq \frac{1}{\alpha^2}
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$I = \{x: \vert x -\mu \vert \geq k\}$ とする．

$$
\begin{align*}
\sigma^2 &= \int_{\mathbb R} (x - \mu)^2f(x)\mathrm{d}x\\
         &\geq  \int_{I} (x - \mu)^2f(x)\mathrm{d}x\\
         &\geq  \int_{I} k^2f(x)\mathrm{d}x\\
         &= k^2 \Pr(\vert x - \mu\vert \geq k)
\end{align*}
$$

以上より，$\displaystyle\Pr(\vert X - \mu \vert \geq k) \leq \frac{\sigma^2}{k^2}$ を得る．

:::

::: {.callout-note collapse="false" icon=false}
## Proof: using Markov's inequality

$(x - \mu)^2$ を確率変数と考えると，non-negative確率変数になる，つまりMarkov's inequalityを用いることができるので

$$
\begin{align*}
\Pr(\vert x - \mu\vert \geq k) &= \Pr((x - \mu)^2 \geq k^2)\\
                               &\leq \frac{\mathbb E[(x - \mu)^2]}{k^2} \because{\text{Markov's inequality}}\\
                               &= \frac{\sigma^2}{k^2}
\end{align*}
$$


:::

::: {#exm- .custom_problem }
**Markov's inequality vs Chebyshev’s inequality**
<br>

$X \sim \operatorname{Binom}(n=20, p=0.2)$ について，[weak inequality](#exm-binom-markov)
で確認したように，Markov's inequalityのより

$$
\Pr(X \geq 16) = \Pr(X \geq 4\mathbb E[X]) \leq \frac{1}{4}
$$

一方，Chebyshev’s inequalityを用いると

$$
\begin{align*}
\Pr(X \geq 16) &\leq \Pr(\vert X - 4\vert \geq 12)\\
               &\leq \frac{\operatorname{Var}(X)}{12^2}\\
               &\leq \frac{3.2}{12^2}\\
               &= \frac{1}{45}
\end{align*}
$$

:::

<div class="blog-custom-border">
<strong >📘 REMARKS</strong> <br>

- Chebyshev’s inequalityはMarkov's inqualityと異なり，確率変数 $X$ がnon-negativeである必要はない
- meanからの距離についての情報を得ることができる

</div>

### Weak Law of Large Numbers

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Weak Law of Large Numbers**
<br>

平均 $\mu$, 分散 $\sigma^2$ の分布に独立に従う確率変数 $X_1, \cdots, X_n$ を考える．標本平均を $\overline{X_n} = \frac{1}{n}\sum_{i=1}^nX_i$ とする．

このとき，任意の実数 $\epsilon >0$ に対して，

$$
\lim_{n\to\infty}\Pr(\vert \overline{X_n} - \mu \vert > \epsilon) = 0
$$

つまり，**標本平均は母平均に確率収束**する．

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

Chebyshev’s inequalityを用いて以下のように示せる

$$
\begin{align*}
\lim_{n\to\infty}\Pr(\vert \overline{X_n} - \mu \vert > \epsilon) 
                &\leq \lim_{n\to\infty} \frac{\operatorname{Var}(\overline{X_n})}{\epsilon^2}\\
                &= \lim_{n\to\infty} \frac{\sigma^2}{n\epsilon^2}\\
                &=0
\end{align*}
$$

:::

## 分散

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: Bienaymé Equality**
<br>

互いに独立な確率変数 $X, Y$ について以下が成立する

$$
\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y)
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\operatorname{Var}(X+Y)
    &= \mathbb E[((X+Y) - (\mu_X+\mu_Y))^2]\\
    &= \mathbb E[((X- \mu_X)+(Y - \mu_Y))^2]\\
    &= \mathbb E[(X- \mu_X)^2] + 2\mathbb E[(X- \mu_X)(Y- \mu_Y)] + \mathbb E[(Y- \mu_Y)^2]\\
    &= \mathbb E[(X- \mu_X)^2] + 2\mathbb E[(X- \mu_X)]\mathbb E[(Y- \mu_Y)] + \mathbb E[(Y- \mu_Y)^2] \quad \because{\text{独立性}}\\ 
    &= \operatorname{Var}(X) + \operatorname{Var}(Y)
\end{align*}
$$

:::

なお，確率変数 $X, Y$ が独立ではない場合は

$$
\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X, Y)
$$

が成立します．

### 条件付き分散

確率変数 $X, Y$ についての条件付き分散は以下のような意味を持つ

- $\operatorname{Var}(X\vert Y=y)$ は，$Y = y$ と固定したときの $X$ の分散
- $\operatorname{Var}(X\vert Y)$ は，$Y$ がランダムに選ばれた値に固定された場合の $X$ の分散

$\operatorname{Var}(X\vert Y)$ は $Y$ のランダムネスに依存した確率変数である一方，
$\operatorname{Var}(X\vert Y=y)$ は $y$ の関数という違いがある

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**条件付き分散**
<br>

$$
\operatorname{Var}(Y\vert X) = \mathbb E[(Y^2\vert X)] - (\mathbb E[(Y\vert X)])^2 = \mathbb E[(Y - \mathbb E[Y\vert X])^2\vert X]
$$


:::

</div>

<br>

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Law of Total Variance**
<br>

$$
\operatorname{Var}(Y) = \operatorname{Var}(\mathbb E[Y\vert X]) + \mathbb E_X[\operatorname{Var}(Y\vert X)]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\epsilon = Y - E\mathbb E[Y\vert X]$ としたとき，$\epsilon$ と $E\mathbb E[Y\vert X]$ は無相関なので，

$$
\operatorname{Var}(Y) = \operatorname{Var}(\mathbb E[Y\vert X]) + \operatorname{Var}(\epsilon)
$$

$\mathbb E[\epsilon] = 0$ より，

$$
\begin{align*}
\operatorname{Var}(\epsilon)
    &= \mathbb E[\epsilon^2] - (\mathbb E[\epsilon])^2\\
    &= \mathbb E[\epsilon^2]\\
    &= \mathbb E_X(\mathbb E[\epsilon^2\vert X])\\
    &= \mathbb E_X[\operatorname{Var}(Y\vert X)]
\end{align*}
$$

従って，

$$
\operatorname{Var}(Y) = \operatorname{Var}(\mathbb E[Y\vert X]) + \mathbb E_X[\operatorname{Var}(Y\vert X)]
$$

:::

Law of Total Varianceより $Y$ の分散は，CEFの分散 + 誤差項の分散に分解できることを示しています．
実務における分析において，賃金のバラツキを

- 賃金を説明する各個人の特徴のバラツキ
- 特徴で説明することのできない賃金のバラツキ(=誤差項)の期待値

に分解して考察する際にLaw of Total Varianceを使用したりします．

### 歪度と尖度

$X_1\sim N(1, 1)$ と $X_2\sim\operatorname{Exponential}(1)$ を２つの分布を考える．どちらも平均，分散共に 1 で一致していますが以下のように分布の形状は大きく異なります．

```{python}
import numpy as np
import plotly.express as px
import polars as pl

np.random.seed(42)
n = 1000

x1 = np.random.normal(loc=1, scale=1, size=n)
x2 = np.random.exponential(scale=1, size=n)

df = pl.DataFrame({"normal_dist": x1, "exp_dist": x2})

px.histogram(df, histnorm="probability density",
             opacity=0.8, barmode="overlay",
             title='Exp(1) vs Normal(1, 1): same mean and variance')
```

平均や分散(locationとscale)によって確率分布の様子はある程度わかりますが，locationとscaleが同じにもかかわらず上記の指数分布は $N(1, 1)$ に対して，右の裾が長い分布になっています．分布の非対称性や尖りの程度を理解するにあたって尖度と歪度を用いることがあります．

<div class="blog-custom-border">
<strong>Def: 歪度(skewness)と尖度(kurtosis)</strong> <br>

確率変数 $X$ について，歪度と尖度は以下のように定義される

$$
\begin{gather*}
\operatorname{skewness} = \mathbb E\left[\left(\frac{X - \mathbb E[X]}{\sqrt{\operatorname{Var}(X)}}\right)^3\right]\\
\operatorname{kurtosis} = \mathbb E\left[\left(\frac{X - \mathbb E[X]}{\sqrt{\operatorname{Var}(X)}}\right)^4\right]
\end{gather*}
$$

</div>

<div class="blog-custom-border">
<strong >📘 REMARKS</strong> <br>

- skewnessはpositiveならば右の裾が長く，negativeならば左に裾が長い
- kurtosisは大きいほど，鋭いピークと長く太い裾をもった分布になる
- skewness,kurtosisともに標準化してから3rd-moment, 4th-momentを計算しているので，non-zeroの $a, b$ を定数としたとき，$aX + b$ と変数変換を行っても，計算結果は変わらない = 尺度の変換関して不変

</div>



::: {#exm- .custom_problem }
**: 一様分布の歪度と尖度**
<br>

$X\sim\operatorname{Unif}(0, 1)$ としたとき，一様分布はlocationから左右対称の分布なので計算することなく 

$$
\operatorname{skewness} = 0
$$

とわかる．一方，尖度は

$$
\begin{align*}
\operatorname{kurtosis}
    &= \frac{1}{\sigma^4}\int_0^1 \left(x - \frac{1}{2}\right)^4\mathrm{d}x\\
    &= 144 \times \frac{1}{5}\left[\left(x - \frac{1}{2}\right)^5\right]^1_0\\
    &= \frac{9}{5}
\end{align*}
$$

標準正規分布の尖度を基準にして

$$
\operatorname{kurtosis} = \frac{9}{5}-3=-\frac{6}{5}
$$

と表現する場合もある

:::
