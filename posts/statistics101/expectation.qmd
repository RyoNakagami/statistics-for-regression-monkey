---
title: "æœŸå¾…å€¤"
author: "Ryo Nakagami"
date: "2024-09-12"
date-modified: last-modified
number_sections: false
code-fold: true
comments:
    utterances:
         repo: RyoNakagami/statistics-for-regression-monkey
         label: discussion
jupyter: python3
---

ç¢ºç‡çš„å¤‰å‹•ã‚’è€ƒæ…®ã™ã‚‹éš›ã«ï¼Œãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚’è¦ç´„ã—ãŸçµ±è¨ˆé‡ã¨ã—ã¦æœŸå¾…å€¤ã‚’ç”¨ã„ã‚‹å ´é¢ã¯å¤šã„ã§ã™ï¼ä¾‹ãˆã°ï¼Œ
ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ‹…å½“è€…ã¯ï¼Œãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã®æä¾›ã«ã‚ˆã£ã¦å¾—ã‚‰ã‚Œã‚‹ãƒªã‚¿ãƒ¼ãƒ³ã®æœŸå¾…å€¤ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿæ–½ã‚’åˆ¤æ–­ã—ã¾ã™ã—ï¼Œ
æŠ•è³‡å®¶ã¯ã•ã¾ã–ã¾ãªé‡‘èå•†å“ã®æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã¨ãƒªã‚¹ã‚¯ã«åŸºã¥ã„ã¦ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚’é¸æŠã—ãŸã‚Šã—ã¾ã™ï¼

## æœŸå¾…å€¤ã®æ€§è³ª

<div class="blog-custom-border">
<strong>Def: é€£ç¶šç¢ºç‡å¤‰æ•°ã®æœŸå¾…å€¤</strong> <br>

$f$ ã‚’ç¢ºç‡å¤‰æ•° $X$ ã®ç¢ºç‡å¯†åº¦é–¢æ•°ã¨ã™ã‚‹ï¼$\int_{\mathbb R} \vert x\vert f(x) \mathrm{d}x < \infty$ 
ã®ã¨ãï¼Œ$X$ ã®æœŸå¾…å€¤ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹:

$$
\mathbb E[X] = \int_{\mathbb R} x f(x) \mathrm{d}x
$$

ã¾ãŸï¼Œ$X$ ã®é–¢æ•° $g(X)$ ã®æœŸå¾…å€¤ã¯ $\int_{\mathbb R} \vert g(x)\vert f(x) \mathrm{d}x < \infty$ ãªã‚‰ã°

$$
\mathbb E[g(X)] = \int_{\mathbb R} g(x) f(x) \mathrm{d}x
$$


</div>

å®šç¾©ã‚ˆã‚Šç¢ºç‡å¯†åº¦é–¢æ•°ã§é‡ã¿ã¥ã‘ãŸå¹³å‡ãŒç¢ºç‡å¤‰æ•°ã®æœŸå¾…å€¤ã«ãªã‚‹ã¨è§£é‡ˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼meanã¯åˆ†å¸ƒã®ä½ç½®ã‚’è¡¨ã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¨ã‚‚è§£é‡ˆã§ãã‚‹ã®ã§
**location parameterï¼ˆä½ç½®æ¯æ•°ï¼‰**ã¨å‘¼ã¶ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ï¼ä¸€æ–¹ï¼Œæ¨™æº–åå·® $\sigma$ ã¯**scale parameterï¼ˆå°ºåº¦æ¯æ•°ï¼‰**ã¨ã„ã„ã¾ã™ï¼

::: {#exm- .custom_problem }
**æŒ‡æ•°åˆ†å¸ƒã®æœŸå¾…å€¤**
<br>

rate parameter $\lambda$ ã®æŒ‡æ•°åˆ†å¸ƒã«å¾“ã†ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã¾ã™ï¼

$$
\begin{align*}
\mathbb E[X] &= \int^\infty_0 x \lambda \exp(-\lambda x)\mathrm{d}x\\
             &= \bigg[-x\exp(-\lambda x)\bigg]^\infty_0 + \int^\infty_0 \exp(-\lambda x)\mathrm{d}x\\
             &= \int^\infty_0 \exp(-\lambda x)\mathrm{d}x\\
             &= -\frac{1}{\lambda}\bigg[\exp(-\lambda x)\bigg]^\infty_0\\
             &= \frac{1}{\lambda}
\end{align*}
$$


æŒ‡æ•°åˆ†å¸ƒã¯é›»çƒã®å¯¿å‘½ãªã©ã«å¿œç”¨ã•ã‚Œã‚‹åˆ†å¸ƒã§ã™ãŒï¼Œrate parameter $\lambda$ ãŒå°ã•ã„ã»ã©æœŸå¾…å€¤ï¼ˆ= é›»çƒã®å¯¿å‘½ï¼‰ãŒå¤§ãããªã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ï¼

:::

::: {#exm- .custom_problem }
**æœŸå¾…å€¤ãŒå®šç¾©ã§ããªã„é›¢æ•£åˆ†å¸ƒ**
<br>

ç¢ºç‡å¤‰æ•° $X$ ã®supportã‚’åŠ ç®—é›†åˆ $\{2, 2^2, 2^3, \cdots\}$ ã¨ã™ã‚‹ï¼ç¢ºç‡é–¢æ•°ã‚’

$$
\Pr(X = 2^i) = \frac{1}{2^i} \quad (i = 1, 2, \cdots)
$$

ã“ã®ã¨ãï¼Œ

$$
\sum_{i=1}^\infty \Pr(X=2^i) = \sum_{i=1}^\infty\frac{1}{2^i} = 1
$$

ã¨ç¢ºç‡ã®å…¬ç†ã‚’æº€ãŸã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ï¼ä¸€æ–¹ï¼Œ

$$
\begin{align*}
\mathbb E[X]
    &= \sum_{i=1}^\infty 2^i \frac{1}{2^i}\\
    &= \sum_{i=1}^\infty 1 = \infty
\end{align*}
$$

å¾“ã£ã¦ï¼Œç¢ºç‡å¤‰æ•° $X$ ã®åˆ†å¸ƒã¯ï¼ŒæœŸå¾…å€¤ãŒå®šç¾©ã§ããªã„åˆ†å¸ƒã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ï¼

:::

::: {#exm- .custom_problem }
**æœŸå¾…å€¤ãŒå®šç¾©ã§ããªã„é€£ç¶šåˆ†å¸ƒ**
<br>

ç¢ºç‡å¯†åº¦é–¢æ•°
$$
f(x) = \begin{cases}
0 & x < 1\\
\frac{1}{x^2} & x\geq 1
\end{cases}
$$

ã¨ã„ã†ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã‚‹ï¼

$$
\begin{align*}
\int_1^\infty f(x) \mathrm{d}x
    &= \left[\frac{1}{x}\right]^1_\infty = 1
\end{align*}
$$

ä¸€æ–¹ï¼Œ

$$
\begin{align*}
\mathbb E[X]
    &= \int_1^\infty xf(x) \mathrm{d}x\\
    &= \int_1^\infty\frac{1}{x}\mathrm{d}x\\
    &= \left[\log(x)\right]_1^\infty = \infty
\end{align*}
$$

å¾“ã£ã¦ï¼Œç¢ºç‡å¤‰æ•° $X$ ã®åˆ†å¸ƒã¯ï¼ŒæœŸå¾…å€¤ãŒå®šç¾©ã§ããªã„åˆ†å¸ƒã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ï¼

:::

<strong > &#9654;&nbsp; é›¢æ•£ç¢ºç‡å¤‰æ•°ã®å¤‰æ•°å¤‰æ›ã¨æœŸå¾…å€¤</strong>

é›¢æ•£å‹ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦ $Y = g(X)$ ã‚’è€ƒãˆãŸã¨ãï¼Œ$A_y = \{x\vert g(x)=y\}$ ã¨ãŠãã¨

$$
\begin{align*}
\mathbb E[g(X)]
    &= \sum_x p(x)g(x)\\
    &= \sum_y\sum_{A_y} p(x)g(x)\\
    &= \sum_y\sum_{A_y} p(x)y\\
    &= \sum_y y \sum_{A_y} p(x)\\
    &= \sum_y \Pr(Y=y)y\\
    &= \mathbb E[Y]
\end{align*}
$$

ä¸Šã®å¼å±•é–‹ã§ã¯, $\Pr(X=x) = p(x)$ ã¨ã—ã¦ã„ã¾ã™ï¼

<strong > &#9654;&nbsp; æœŸå¾…å€¤ã¨é‡å¿ƒ</strong>

æœŸå¾…å€¤ã®è§£é‡ˆã®ï¼‘ã¤ã¨ã—ã¦ ç¢ºç‡å¤‰æ•° $X$ ã®é‡å¿ƒã¨è€ƒãˆã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚Šã¾ã™ï¼$x_i$ ã®å€¤ã‚’ $p_i$ ã®ç¢ºç‡ã§ã¨ã‚‹é›¢æ•£å‹ç¢ºç‡å¤‰æ•° $X$
ã®å ´åˆï¼Œ

- é‡ã•ã®ãªã„æ£’ã®ä¸­å¤®ã‚’åŸç‚¹ã¨ã™ã‚‹
- åŸç‚¹ã‹ã‚‰å³å´ã‚’ãƒ—ãƒ©ã‚¹ï¼Œå·¦å´ã‚’ãƒã‚¤ãƒŠã‚¹ã¨ã—ã¦ï¼ŒåŸåˆ¸ã‹ã‚‰ã®è·é›¢ $x_i$ ã®å ´æ‰€ã«é‡ã• $p_i$ ã®ãŠã‚‚ã‚Šã‚’åŠã‚Šä¸‹ã’ã‚‹

ã“ã®ã¨ãï¼Œé‡å¿ƒ $\mu$ ã¯ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã®é‡£ã‚Šåˆã„ = å³å›ã‚Šãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãŒ0ã«ãªã‚‹åœ°ç‚¹ã¨ãªã‚Šã¾ã™ãŒ

$$
\begin{align*}
\text{å³å›ã‚Šãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ}
    &= \sum_i (x_i - \mu)p_i\\
    &= \sum x_ip_i - \sum_i p_i \mu\\
    &= \mathbb E[X] - \mu = 0
\end{align*}
$$

å¾“ã£ã¦ï¼Œ$\mathbb E[X] = \mu$ ã‚ˆã‚Šï¼ŒæœŸå¾…å€¤ã¨é‡å¿ƒãŒå¯¾å¿œã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼é€£ç¶šå‹ç¢ºç‡å¤‰æ•°ã§ã‚‚ç¢ºç‡å¯†åº¦é–¢æ•°ã‚’é‡ã•ã®ã‚ã‚‹æ£’ã®æ–­é¢ç©ã¨
ã¿ãªã™ã“ã¨ã§é›¢æ•£å‹ã¨åŒã˜ãæœŸå¾…å€¤ã¨é‡å¿ƒãŒå¯¾å¿œã™ã‚‹ã“ã¨ã‚’ç¢ºã‹ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Tail probabilities**
<br>

$[0, b]$ ã®å®šç¾©åŸŸã‚’ã‚‚ã¤éè² ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã‚‹ï¼$F$ ã‚’ç´¯ç©åˆ†å¸ƒé–¢æ•°ã¨ã™ã‚‹ã¨ã

$$
\mathbb E[X] = \int_0^b (1 - F(x))\mathrm{d}x
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\bigg[xF(x)\bigg]^b_0 = \int^b_0xf(x) \mathrm{d}x + \int^b_0F(x) \mathrm{d}x
$$

ã‚’ç”¨ã„ã‚‹ã¨

$$
\begin{align*}
\mathbb E[X] &= b - \int^b_0F(x) \mathrm{d}x\\
             &= \int^b_0 1 \mathrm{d}x - \int^b_0F(x) \mathrm{d}x\\
             &= \int_0^b (1 - F(x))\mathrm{d}x
\end{align*}
$$


:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Tail probabilities**
<br>

$\mathbb E[\vert X\vert]<\infty$ ã‚’ã‚‚ã¤éè² ã®ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦ï¼Œ$F(x)$ ã‚’åˆ†å¸ƒé–¢æ•°ã¨ã™ã‚‹ã¨

$$
\mathbb E[X] = \int_0^\infty (1 - F(x))\mathrm{d}x
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\int_0^\infty (1 - F(x))\mathrm{d}x
    &= \int_0^\infty \Pr(X\geq x)\mathrm{d}x\\
    &= \int_0^\infty \mathbb E[\mathbb 1(X\geq x)]\mathrm{d}x\\
    &= \int_0^\infty \int_0^\infty \mathbb 1(X\geq x) \mathrm{d}F(x)\mathrm{d}x\\
    &= \int_0^\infty \left\{\int_0^\infty \mathbb 1(X\geq x) \mathrm{d}x\right\}\mathrm{d}F(x) \quad\because\text{ç©åˆ†ã®é †åºäº¤æ›}\\
    &= \mathbb E\left[\int_0^\infty \mathbb 1(X\geq x) \mathrm{d}x\right]\\
    &= \mathbb E\left[\int_0^X 1 \mathrm{d}x\right]\\
    &= \mathbb E[X]
\end{align*}
$$

:::

$\lim_{b\to\infty}\bigg[x(1 - F(x))\bigg]^b_0$ ã«ã¤ã„ã¦ï¼Œ$\lim_{b\to\infty}b(1 - F(b))=0$ ã¨ã¯é™ã‚‰ãªã„ç‚¹ã«æ³¨æ„ãŒå¿…è¦ã§ã™ï¼

<div class="blog-custom-border">
::: {#thm- .custom_problem }
<br>

$[0, \infty)$ ã®å®šç¾©åŸŸã‚’ã‚‚ã¤éè² ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã‚‹ï¼$\mathbb E[\vert X^{p+1} \vert] <\infty$ ãŒå®šç¾©å¯èƒ½åŠã³ï¼Œ $F$ ã‚’ç´¯ç©åˆ†å¸ƒé–¢æ•°ã¨ã™ã‚‹ã¨ã

$$
\mathbb E[X^p] = \int_0^\infty px^{p-1} (1 - F(x))\mathrm{d}x \quad \text{where } p > 0
$$

:::
</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\bigg[x^p(1 - F(x))\bigg]^\infty_0 = \int^\infty_0 p x^{p-1}(1 -F(x))\mathrm{d}x - \int^\infty_0 x^{p}f(x)\mathrm{d}x
\end{align*}
$$

$\text{RHS} = 0$ ã§ã‚ã‚‹ã®ã§

$$
\mathbb E[X^p] = \int_0^\infty px^{p-1} (1 - F(x))\mathrm{d}x 
$$

:::

::: {#exm-discrete-tail .custom_problem }
**: Discrete Tail Probability**
<br>

åŒæ§˜ã®è€ƒãˆã§å®šç¾©åŸŸã‚’ $0,1,2,3,\cdots$ ã¨ã™ã‚‹é›¢æ•£ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦

$$
\mathbb E[X] = \sum_{k=0}^\infty \Pr(X > k)
$$

ãŒæˆç«‹ã—ã¾ã™ï¼

$$
\begin{align*}
\Pr(X > k) &= \Pr(X = k+1) + \Pr(X = k+2) + \cdots\\
           &= \sum_{l=k+1}^\infty \Pr(X=l)
\end{align*}
$$

å¾“ã£ã¦ï¼Œ

$$
\begin{align*}
\sum_{k=0}^\infty \Pr(X > k) &= \sum_{k=0}^\infty \sum_{l=k+1}^\infty \Pr(X=l)\\
                             &= \sum_{l=1}^\infty\sum_{k=0}^{l-1}\Pr(X=l) \quad\because \Pr(X=l) > 0 \\
                             &= \sum_{l=1}^\infty l\Pr(X=l)\\
                             &= \sum_{l=0}^\infty l\Pr(X=l)\\
                             &= \mathbb E[X]
\end{align*}
$$
$$\tag*{\(\blacksquare\)}$$

:::

::: {#exm- .custom_problem }
<br>

$0,1,2,3,\cdots$ ã¨ã™ã‚‹é›¢æ•£ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦

$$
\mathbb E[X^2] = \sum_{k=0}^\infty \Pr(X > k)(2k+1)
$$

ã‚‚æˆç«‹ã™ã‚‹ï¼

$$
\begin{align*}
\sum_{k=0}^\infty \Pr(X > k)(2k+1) 
    &= \sum_{k=0}^\infty \sum_{l=k+1}^\infty \Pr(X=l)(2k+1)\\
    &= \sum_{l=1}^\infty \sum_{k=0}^{l-1}\Pr(X=l)(2k+1)\\
    &= \sum_{l=1}^\infty \Pr(X=l)\sum_{k=0}^{l-1}(2k+1)\\
    &= \sum_{l=1}^\infty l^2\Pr(X=l)\\
    &= \mathbb E[X^2]
\end{align*}
$$

$$\tag*{\(\blacksquare\)}$$

:::

<div class="blog-custom-border">
::: {#thm-exp-linear .custom_problem }
**æœŸå¾…å€¤ã®ç·šå‹æ€§**
<br>

$a, b$ ã‚’å®Ÿæ•°ï¼Œç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ä»¥ä¸‹ãŒæˆã‚Šç«‹ã¤

$$
\mathbb E[aX + bY] = a\mathbb E[X] + b\mathbb E[Y]
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

ç¢ºç‡å¤‰æ•° $X, Y$ ãŒæœ‰é™åŠ ç®—ãªæ¨™æœ¬ç©ºé–“ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã«ã¦ä»¥ä¸‹ã‚’ç¤ºã™ï¼

1. $\mathbb E[X + Y] = \mathbb E[X] + \mathbb E[Y]$
2. $\mathbb E[cX] = c\mathbb E[X]$

<strong > &#9654;&nbsp; 1. $\mathbb E[X + Y] = \mathbb E[X] + \mathbb E[Y]$</strong>

ç¢ºç‡å¤‰æ•° $X$ ã¯ $\{x_1, \cdots, x_m\}$, ç¢ºç‡å¤‰æ•° $Y$ ã¯ $\{y_1, \cdots, y_n\}$ ã®å€¤ã‚’ãã‚Œãã‚Œå–ã‚Šã†ã‚‹ã¨ã™ã‚‹ï¼
ã“ã®ã¨ãï¼Œ$Z = X + Y$ ã®æ¨™æœ¬ç©ºé–“ $\{z_1, \cdots, z_k\}$ ã«ã¤ã„ã¦ $k\leq m + n$ ãŒæˆã‚Šç«‹ã¤ï¼

$A_l = \{(i,j): x_i + y_j = z_l\}$ ã¨ã—ãŸã¨ãï¼Œ

$$
\begin{align*}
\mathbb E[X+Y]
    &= \sum_{l=1}^kz_l\Pr(A_l)\\
    &= \sum_{l=1}^k\sum_{(i,j)\in Z_l}(x_i + y_j)\Pr(x_i, y_j)\\
    &= \sum_{i=1}^m\sum_{j=1}^n(x_i + y_j)\Pr(x_i, y_j)\\
    &= \sum_{i=1}^m\sum_{j=1}^nx_i\Pr(x_i, y_j) + y_j\Pr(x_i, y_j)\\
    &= \sum_{i=1}^m\sum_{j=1}^n[x_i\Pr(x_i, y_j) + y_j\Pr(x_i, y_j)]\\
    &= \sum_{i=1}^mx_i\sum_{j=1}^nPr(x_i, y_j) + \sum_{j=1}^ny_j\sum_{i=1}^m\Pr(x_i, y_j)\\
    &=\sum_{i=1}^mx_i \Pr(x_i) + \sum_{j=1}^ny_j \Pr(y_j)\\
    &= \mathbb E[X] + \mathbb E[Y]
\end{align*}
$$


<strong > &#9654;&nbsp; 2. $\mathbb E[cX] = c\mathbb E[X]$</strong>

$$
\begin{align*}
\mathbb E[cX]
    &= \sum_{i=1}^m cx_i = \Pr(cX = cx_i)\\
    &= c\sum_{i=1}^m x_i = \Pr(X = x_i)\\
    &= c\mathbb E[X]
\end{align*}
$$

:::

[æœŸå¾…å€¤ã®ç·šå‹æ€§å®šç†](#thm-exp-linear)ã®å¿œç”¨ã¨ã—ã¦, æœŸå¾…å€¤ãŒå®šç¾©ã§ãã‚‹ã¨ã„ã†å‰æã®ä¸‹ï¼Œ

$$
\begin{align*}
\mathbb E[g(X, Y) + h(X, Y)] &= \mathbb E[g(X, Y)] + \mathbb E[h(X, Y)]\\
\mathbb E[ag(X, Y) + b] &= a\mathbb E[g(X, Y)] + b \quad (a, b \text{: constants})
\end{align*}
$$
ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼

$$
\begin{align*}
\mathbb E[g(X, Y) + h(X, Y)]
    &= \int_{\mathbb R}\int_{\mathbb R}(g(x, y) + h(x, y))f(x, y) \,\mathrm{d}x\,\mathrm{d}y\\
    &= \int_{\mathbb R}\int_{\mathbb R}g(x, y)f(x, y) \,\mathrm{d}x\,\mathrm{d}y + \int_{\mathbb R}\int_{\mathbb R}h(x, y)f(x, y) \,\mathrm{d}x\,\mathrm{d}y\\
    &= \mathbb E[g(X, Y)] + \mathbb E[h(X, Y)]
\end{align*}
$$

ä¸Šè¨˜ã®ã‚ˆã†ã«ç¢ºèªã§ãã¾ã™ï¼


::: {#exm- .custom_problem }
**: å¤‰æ•°å¤‰æ›ã¨åˆ†æ•£**
<br>

mean $\mu$ ã‚’ã‚‚ã¤ç¢ºç‡å¤‰æ•° $X$ ã¨å®Ÿæ•° $a, b$ ã«ã¤ã„ã¦

$$
\operatorname{Var}(aX + b) = a^2\operatorname{Var}(X)
$$

ãŒæˆç«‹ã—ã¾ã™ï¼è¨¼æ˜ã¯ä»¥ä¸‹ï¼Œ

$$
\begin{align*}
\operatorname{Var}(aX + b) 
    &= \mathbb E[(aX + b) - (a\mu +b)^2]\\
    &= \mathbb E[a^2(X - \mu)^2]\\
    &= a^2 \mathbb E[(X - \mu)^2]\\
    &= a^2\operatorname{Var}(X)
\end{align*}
$$

$$\tag*{\(\blacksquare\)}$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: ç¢ºç‡å¤‰æ•°ã®æ¨™æº–åŒ–**
<br>

ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦, $\mathbb E[X] = \mu, \operatorname{Var}(X) =\sigma^2$ ãŒå­˜åœ¨ã™ã‚‹ã¨ãï¼Œ

$$
Z = \frac{X - \mu}{\sigma}
$$

ã¨ãŠãã¨ï¼Œ$\mathbb E[Z] = 0, \operatorname{Var}(Z) = 1$ ãŒæˆç«‹ã™ã‚‹ï¼

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[Z]
    &= \mathbb E\left[\frac{X - \mu}{\sigma}\right]\\
    &= \frac{\mathbb E[X] - \mu}{\sigma}\\
    &= 0
\end{align*}
$$

$$
\begin{align*}
\operatorname{Var}(Z)
    &= \mathbb E[Z^2]\\
    &= \mathbb E\left[\frac{(X - \mu)^2}{\sigma^2}\right]\\
    &= \frac{\sigma^2}{\sigma^2}\\
    &= 1
\end{align*}
$$



:::






<div class="blog-custom-border">
::: {#thm- .custom_problem }
**positive operator**
<br>

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ï¼Œ$X\geq Y$ ãŒæˆã‚Šç«‹ã¤ã¨ãï¼Œ

$$
\mathbb E[X] \geq \mathbb E[Y]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$X\geq Y$ ã‚ˆã‚Š $X - Y \geq 0$. æœŸå¾…å€¤ã¯positive operatorãªã®ã§

$$
\mathbb E[X - Y] \geq 0
$$

å¾“ã£ã¦ï¼ŒæœŸå¾…å€¤ã®ç·šå‹æ€§ã‚’ç”¨ã„ã‚‹ã¨

$$
\begin{align*}
\mathbb E[X - Y] &= \mathbb E[X] - \mathbb E[Y] \geq 0
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
<br>

ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦,

$$
\mathbb E[\vert X \vert] \geq \vert \mathbb E[X] \vert
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\vert X\vert \geq X$ ã‚ˆã‚Š 

$$
\mathbb E[\vert X \vert] \geq \mathbb E[X]
$$ 

ã¾ãŸ, $\vert X\vert + X \geq 0$ ã‚ˆã‚Šï¼Œ$\mathbb E[\vert X\vert + X] \geq 0$ï¼Œ
ã¤ã¾ã‚Šï¼Œ

$$
\mathbb E[\vert X \vert] \geq -\mathbb E[X]
$$

ä»¥ä¸Šã‚ˆã‚Šï¼Œ$\mathbb E[\vert X \vert] \geq \vert \mathbb E[X] \vert$


:::

<div class="blog-custom-border">
::: {#thm-schwarz .custom_problem }
**Schwarz inquality**
<br>

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„ã®ä¸ç­‰å¼ãŒæˆç«‹ã™ã‚‹ã“ã¨ã‚’ç¤ºã›

$$
\left(\mathbb E[XY]\right)^2 \leq \mathbb E[X^2]\mathbb E[Y^2]
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

Quadratic functionã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™

$$
\begin{align*}
g(t) 
    &= \mathbb E[(tX - Y)^2]\\
    &= t^2\mathbb E[X^2] - 2t\mathbb E[XY] + E[Y^2]\\
    &\geq 0
\end{align*}
$$

ã“ã®ã¨ãï¼Œ$g(t)$ ã¯non-negativeãªã®ã§åˆ¤åˆ¥å¼ã«ã¤ã„ã¦ä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹

$$
D/4 = \left(\mathbb E[XY]\right)^2 - \mathbb E[X^2]\mathbb E[Y^2]\leq 0 
$$

å¾“ã£ã¦ï¼Œ$\left(\mathbb E[XY]\right)^2 \leq \mathbb E[X^2]\mathbb E[Y^2]$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: Triangle inequality**
<br>

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ï¼Œä»¥ä¸‹ã®ã‚ˆã†ãªä¸‰è§’ä¸ç­‰å¼ãŒæˆç«‹ã™ã‚‹ã“ã¨ã‚’ç¤ºã›

$$
\sqrt{\mathbb E[(X+Y)^2]} \leq \sqrt{\mathbb E[X^2]} + \sqrt{\mathbb E[Y^2]}
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„ã®ä¸ç­‰å¼ã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«ç¤ºã›ã‚‹

$$
\begin{align*}
\mathbb E[(X+Y)^2]
    &= \mathbb E[X^2] + 2\mathbb E[XY] + \mathbb E[Y^2]\\
    &= \mathbb E[X^2] + 2\sqrt{(\mathbb E[XY])^2} + \mathbb E[Y^2]\\
    &\leq \mathbb E[X^2] + 2\sqrt{\mathbb E[X^2]\mathbb E[Y^2]} + \mathbb E[Y^2]\\
    &= (\sqrt{\mathbb E[X^2]} + \sqrt{\mathbb E[Y^2]})^2
\end{align*}
$$

ä¸¡è¾ºã«ã¤ã„ã¦ï¼Œsquare rootã‚’ã¨ã‚‹ã¨ï¼Œ

$$
\sqrt{\mathbb E[(X+Y)^2]} \leq \sqrt{\mathbb E[X^2]} + \sqrt{\mathbb E[Y^2]}
$$


:::

### ç‹¬ç«‹ãªç¢ºç‡å¤‰æ•°ã¨æœŸå¾…å€¤

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**äº’ã„ã«ç‹¬ç«‹ãªç¢ºç‡å¤‰æ•°ã®ç©ã®æœŸå¾…å€¤**
<br>

$\mathbb E[\vert X\vert ]<\infty, \mathbb E[\vert Y\vert ]<\infty$ ã‚’æº€ãŸã™, ç¢ºç‡ç©ºé–“ $(\Omega, \mathscr{F},P)$ ä¸Šã§å®šç¾©ã•ã‚ŒãŸç¢ºç‡å¤‰æ•° $X, Y$ ã‚’è€ƒãˆã‚‹ï¼
$X \perp Y$ ã§ã‚ã‚‹ã¨ãï¼Œæ¬¡ãŒæˆç«‹ã™ã‚‹

$$
\mathbb E[XY] = \mathbb E[X]\mathbb E[Y]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[XY] &= \int\int_\Omega xy f(x, y)\mathrm{d}x\mathrm{d}y\\
              &= \int\int_\Omega xy f_X(x)f_Y(y)\mathrm{d}x\mathrm{d}y \quad\because{\text{independence}}\\
              &= \left(\int xf_X(x)\mathrm{d}x\right)\left(\int yf_Y(y)\mathrm{d}y\right)\\
              &= \mathbb E[X]\mathbb E[Y]
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**äº’ã„ã«ç‹¬ç«‹ãªç¢ºç‡å¤‰æ•°ã«ã¤ã„ã¦ã®é–¢æ•°ã®ç©ã®æœŸå¾…å€¤**
<br>

$\mathbb E[\vert X\vert ]<\infty, \mathbb E[\vert Y\vert ]<\infty$ ã‚’æº€ãŸã™, ç¢ºç‡ç©ºé–“ $(\Omega, \mathscr{F},P)$ ä¸Šã§å®šç¾©ã•ã‚ŒãŸç¢ºç‡å¤‰æ•° $X, Y$ ã‚’è€ƒãˆã‚‹ï¼
ã¾ãŸï¼Œé–¢æ•° $g(X)$, $h(Y)$ ã¯æœŸå¾…å€¤ãŒå­˜åœ¨ã™ã‚‹ã¨ã™ã‚‹ï¼$X \perp Y$ ã§ã‚ã‚‹ã¨ãï¼Œæ¬¡ãŒæˆç«‹ã™ã‚‹

$$
\mathbb E[g(X)h(Y)] = \mathbb E[g(X)]\mathbb E[h(Y)]
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$X \perp Y$ ãªã®ã§ï¼Œç¢ºç‡å¯†åº¦é–¢æ•°ã«ã¤ã„ã¦ $f(x, y) = f_X(x)f_Y(y)$ ãŒæˆç«‹ã—ã¾ã™ï¼

$$
\begin{align*}
\mathbb E [g(X)h(Y)]
    &= \int_{\mathbb R}\int_{\mathbb R}g(x)h(x)f(x, y)\,\mathrm{d}x\,\mathrm{d}y\\
    &= \int_{\mathbb R}\int_{\mathbb R}g(x)h(x)f(x, y)\,\mathrm{d}x\,\mathrm{d}y\\
    &= \left\{\int_{\mathbb R}g(x)f_X(x)\,\mathrm{d}x\right\}\left\{\int_{\mathbb R}h(x)f_Y(y)\,\mathrm{d}y\right\}\\
    &= \mathbb E[g(X)]\mathbb E[h(Y)]
\end{align*}
$$

:::


### æ¡ä»¶ä»˜ãæœŸå¾…å€¤

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Law of Total Expectation**
<br>

$$
\mathbb E[Y] = \mathbb E[\mathbb E[Y\vert X]]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

<strong > &#9654;&nbsp; é€£ç¶šç¢ºç‡å¤‰æ•°ã®å ´åˆ</strong>

$$
\begin{align*}
\mathbb E[\mathbb E[Y\vert X]]
    &= \int \mathbb E[Y\vert X=u]f_X(u)\mathrm{d}u\\
    &= \int \left[\int t f_Y(t\vert x=u)\mathrm{d}t\right]f_X(u)\mathrm{d}u\\
    &= \int \int t f_Y(t\vert x=u)f_X(u)\mathrm{d}u\mathrm{d}t\\
    &= \int t\left[\int f_{X,Y}(u, t)\mathrm{d}u\right]\mathrm{d}t\\
    &= \int t f_Y(t)\mathrm{d}t\\
    &= \mathbb E[Y]
\end{align*}
$$

<strong > &#9654;&nbsp; é›¢æ•£ç¢ºç‡å¤‰æ•°ã®å ´åˆ</strong>

$$
\begin{align*}
\mathbb E[\mathbb E[Y\vert X]]
    &= \sum_{x\in \mathcal{X}}\mathbb E[Y\vert X] f_X(x)\\
    &= \sum_{x\in \mathcal{X}}\sum_{y\in \mathcal{Y}} y f_{Y\vert X}(y \vert x) f_X(x)\\
    &= \sum_{x\in \mathcal{X}}\sum_{y\in \mathcal{Y}} y f(x, y)\\
    &= \sum_{y\in \mathcal{Y}}y f_Y(y)\\
    &= \mathbb E[Y]
\end{align*}
$$

:::

LTEã®æ³•å‰‡ã‚ˆã‚Š

$$
\begin{align*}
\mathbb E[Y] &= \mathbb E[\mathbb E[Y\vert X, Z]]\\
\mathbb E[Y\vert X] &= \mathbb E[\mathbb E[Y\vert X, Z]\vert X]
\end{align*}
$$

ãŒæˆç«‹ã—ã¾ã™ï¼

ãªãŠï¼Œ$\mathbb E[Y\vert X], \mathbb E[Y\vert X=x]$ ã®é•ã„ã«ã¯æ³¨æ„ãŒå¿…è¦ã§ã™ï¼å‰è€…ã¯ç¢ºç‡å¤‰æ•°ã§ã™ãŒï¼Œ
å¾Œè€…ã¯ç¢ºç‡å¤‰æ•°ã®å®Ÿç¾å€¤ã®å–ã‚Šã†ã‚‹å€¤ã‚’è¡¨ã—ã¦ã„ã¾ã™ï¼$X$ ã‚’ $x_1, \cdots, x_k$ ã®å€¤ã‚’å–ã‚‹é›¢æ•£ç¢ºç‡å¤‰æ•°ã¨ã™ã‚‹ã¨ï¼Œ
$\mathbb E[Y\vert X]$ ã¯ $k$ å€‹ã®å€¤ã‚’å–ã‚‹é›¢æ•£ç¢ºç‡å¤‰æ•°ã¨ãªã‚Šã¾ã™ï¼

<div class="blog-custom-border">
::: {#thm- .custom_problem }
<br>

$g(\cdot)$ ã‚’ $\mathbb E[g(X)Y] < \infty$ ã‚’æº€ãŸã™ $X$ ã®ä»»æ„ã®é–¢æ•°ã¨ã™ã‚‹ï¼ã“ã®ã¨ãï¼Œ

$$
\mathbb E[\mathbb E[g(X)Y\vert X]] = \mathbb E[g(X)Y] 
$$

ãŒæˆã‚Šç«‹ã¤ï¼

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\mu_Y(x) = \mathbb E[Y\vert X = x]$ ã¨ã™ã‚‹ï¼ã“ã®ã¨ãï¼Œ

$$
\begin{align*}
\mathbb E[g(X)\mathbb E[Y\vert X]]
    &= \mathbb E[g(X)\mu_Y(X))\\
    &= \sum_{x\in\mathcal{X}}g(x)\mu_Y(x)f_X(x)\\
    &= \sum_{x\in\mathcal{X}}g(x)\sum_{y\in\mathcal{Y}}y f_{Y\vert X}(y\vert x)f_X(x)\\
    &= \sum_{x\in\mathcal{X}}g(x)\sum_{y\in\mathcal{Y}} y f(x, y)\\
    &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}g(x)y f(x, y)\\
    &= \mathbb E[g(X)Y]
\end{align*}
$$

:::





<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: CEF Decomposition Property**
<br>

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ï¼Œ

$$
Y = \mathbb E[Y\vert X] + \epsilon
$$

ã¨ã—ãŸã¨ãï¼Œ

1. $\epsilon$ ã¯ $X$ ã«ã¤ã„ã¦ mean-independent, ã¤ã¾ã‚Š, $$\mathbb E[\epsilon\vert X] = 0$$
2. $\epsilon$ ã¯ $X$ ã®ä»»æ„ã®é–¢æ•°ã«å¯¾ã—ã¦ç„¡ç›¸é–¢, ã¤ã¾ã‚Š, $$\operatorname{Cov}(h(X), \epsilon) = 0$$
3. $\operatorname{Var}(\epsilon) = \mathbb E[\operatorname{Var}(Y\vert X)]$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

<strong > &#9654;&nbsp; (1)</strong>

$$
\begin{align*}
\mathbb E[\epsilon\vert X] 
    &= \mathbb E[Y - \mathbb E[Y\vert X]\vert X]\\
    &= \mathbb E[Y\vert X] - \mathbb E[Y\vert X]\\
    &= 0
\end{align*}
$$

<strong > &#9654;&nbsp; (2)</strong>

$$
\begin{align*}
\mathbb E[h(X)\epsilon] &= E[\mathbb E[h(X)\epsilon\vert X]]\\
                        &= E[h(X)\mathbb E[\epsilon\vert X]]\\
                        &= 0 \quad \because{\text{mean independence}}
\end{align*}
$$

<strong > &#9654;&nbsp; (3)</strong>

æ¡ä»¶ä»˜ãæœŸå¾…å€¤ã®å…¬å¼ $\mathbb E[g(X, Y)] = \mathbb E_X[\mathbb E_{Y\vert X}[g(X, Y)\vert X]]$ ã‚ˆã‚Š

$$
\begin{align*}
\operatorname{Var}(\epsilon)
    &= \mathbb E[(Y - \mathbb E[Y\vert X] )^2]\\
    &= \mathbb E\{\mathbb E[(Y - \mathbb E[Y\vert X])^2 \vert X]\}\\
    &= \mathbb E[\operatorname{Var}(Y\vert X)]
\end{align*}
$$

:::

CEF Decomposition Propertyã¯ï¼Œç¢ºç‡å¤‰æ•° $Y$ ã¯ç¢ºç‡å¤‰æ•° $X$ ã§èª¬æ˜ã§ãã‚‹ãƒ‘ãƒ¼ãƒˆã¨ï¼Œ$X$ ã®ä»»æ„ã®é–¢æ•°ã¨ç›´è¡Œï¼ˆorthogonalï¼‰
ãªèª¤å·®é …ã®ãƒ‘ãƒ¼ãƒˆã«åˆ†è§£ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼

<div class="blog-custom-border">
::: {#thm-mse-minimizer .custom_problem }
**: MSE minimizer**
<br>

$g(\cdot)$ ã‚’ $X$ ã®ä»»æ„ã®é–¢æ•°ã¨ã™ã‚‹ï¼ã“ã®ã¨ãï¼Œ

$$
\mathbb E[(Y - g(X))^2] \geq \mathbb E[(Y - \mathbb E[Y\vert X])^2]
$$

ãŒæˆç«‹ã™ã‚‹ï¼ã¤ã¾ã‚Šï¼Œ$\mathbb E[Y\vert X]$ ã¯ï¼Œ$X$ ã®ã™ã¹ã¦ã®é–¢æ•°ã®ä¸­ã§ï¼ŒMSEã®æ„å‘³ã§ $Y$
ã®æœ€è‰¯è¿‘ä¼¼ã‚’ä¸ãˆã‚‹ï¼

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\mu_Y(X) = \mathbb E[Y\vert X]$ ã¨ã™ã‚‹ï¼

$$
\begin{align*}
\mathbb E[(Y - g(X))^2]
    =& \mathbb E[(Y - g(X) + \mu_Y(X) - \mu_Y(X))^2]\\[5pt]
    =& \mathbb E[(Y - \mu_Y(X))^2 + (\mu_Y(X)- g(X))^2\\
     &+ 2(Y - \mu_Y(X))(\mu_Y(X)- g(X))]\\[5pt]
    =& \mathbb E[(Y - \mu_Y(X))^2] + \mathbb E[(\mu_Y(X)- g(X))^2]\\
     &+ 2\mathbb E[(Y - \mu_Y(X))(\mu_Y(X)- g(X))]\\[5pt]
    =& \mathbb E[(Y - \mu_Y(X))^2] + \mathbb E[(\mu_Y(X)- g(X))^2]\\[5pt]
    \geq& \mathbb E[(Y - \mu_Y(X))^2]
\end{align*}
$$

:::


### Moments

<div class="blog-custom-border">
<strong>Def: ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ</strong> <br>

$0 < r < \infty$ ã‚’æº€ãŸã™ã‚ˆã†ãªæ­£ã®æ•´æ•° $r$ ã«å¯¾ã—ã¦ï¼Œç¢ºç‡å¤‰æ•° $X$ ã®the r-th momentï¼ˆåŸç‚¹å‘¨ã‚Šã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼‰ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹

$$
\mathbb E[X^r] = \int_{\mathbb R} x^r \mathrm{d}F_X(x)
$$

the r-th central moment(å¹³å‡å‘¨ã‚Šã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ)ã¯ $\mathbb E[(X - \mathbb E[X])^r]$ ã¨å®šç¾©ã•ã‚Œã‚‹ï¼

</div>

åŸç‚¹å‘¨ã‚Šã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã¨å¹³å‡å‘¨ã‚Šã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã¯ä»¥ä¸‹ã®ã‚ˆã†ãªé–¢ä¿‚ã§ç†è§£ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼$\mathbb E[X] = \mu$ ã¨ã™ã‚‹ã¨ï¼Œ

$$
\begin{align*}
(X - \mu + \mu)^r
    &= \sum_{k=0}^k \left(\begin{array}{c}r\\ k\end{array}\right)(X - \mu)^k\mu^{r-k}
\end{align*}
$$

ä¸¡è¾ºã®æœŸå¾…å€¤ã‚’ã¨ã‚‹ã¨

$$
\begin{align*}
\mathbb E[X^r] = \sum_{k=0}^k \left(\begin{array}{c}r\\ k\end{array}\right)\mathbb E[(X - \mu)^k]\mu^{r-k}
\end{align*}
$$

ã¾ãŸï¼Œå¹³å‡å‘¨ã‚Šã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’åŸç‚¹å‘¨ã‚Šã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã§è¡¨ã™ã¨ãªã‚‹ã¨ï¼Œ

$$
\begin{align*}
(X - \mu)^r = \sum_{k=0}^r(-1)^{r-k}X^r\mu^{r-k}
\end{align*}
$$

åŒæ§˜ã«æœŸå¾…å€¤ã‚’ã¨ã‚‹ã¨

$$
\begin{align*}
\mathbb E[(X - \mu)^r] = \sum_{k=0}^r(-1)^{r-k}\mathbb E[X^r]\mu^{r-k}
\end{align*}
$$


<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: higher moment and lower moment**
<br>

$\mathbb E[\vert X\vert^r] < \infty$ ã®ã¨ãï¼Œ$0 < q < r$ ã«ã¤ã„ã¦ï¼Œ$\mathbb E[\vert X\vert^q] < \infty$ ãŒæˆç«‹ã™ã‚‹ï¼

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

é–¢æ•° $g:\mathbb R\to \mathbb R$ ã‚’

$$
g(x) = \vert x\vert^r + 1
$$

é–¢æ•° $g:\mathbb h\to \mathbb R$ ã‚’ $h(x) = \vert x\vert^q$ ã¨å®šç¾©ã™ã‚‹ï¼$0 < q < r$ ã‚ˆã‚Šï¼Œ

$$
h(x) < g(x) \quad \forall x \in \operatorname{support}(X) 
$$

ã“ã®ã¨ãï¼Œ

$$
\begin{align*}
\int_{\mathbb{R}} |x|^r + 1 \mathrm{d}F 
    &= \int_{\mathbb{R}} |x|^r\mathrm{d}F + \underbrace{\int_{\mathbb{R}} 1\mathrm{d}F}_{=1}\\
    &> \int_{\mathbb{R}} |x|^s\mathrm{d}F
\end{align*}
$$

å¾“ã£ã¦ï¼Œ

$$
\begin{align*}
\infty &> \mathbb E[\vert X\vert^r] + 1\\
       &> \int_{\mathbb{R}} |x|^q \mathrm{d}F \\
       &= \mathbb E[\vert X\vert^q]
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: Mean minimizes squared error**
<br>

ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦ï¼Œ$\mathbb E[\vert X\vert^2]<\infty$ ã¨ã™ã‚‹ï¼ã“ã®ã¨ãï¼Œ

$$
\mathbb E[X] = \arg\min_b \mathbb E[(X - b)^2]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\mu_X = \mathbb E[X], \sigma^2 = \mathbb E[(X - \mu_X)^2]$ ã¨ãŠãï¼

$$
\begin{align*}
 E[(X - b)^2] 
    &= \mathbb E[(X - \mu_X + \mu_X - b)^2]\\\
    &= \mathbb E[(X - \mu_X)^2] + \mathbb E[(\mu_X - b)^2] + \mathbb E[(X - \mu_X)(\mu_x - b)]\\
    &= \sigma^2 + \mathbb E[(\mu_X - b)^2] + (\mu_X - b)\mathbb E[X - \mu_X]\\
    &= \sigma^2 + \mathbb E[(\mu_X - b)^2]
\end{align*}
$$

$\mathbb E[(\mu_X - b)^2] \geq 0$ ãŒæˆç«‹ã—ï¼Œã¾ãŸ, ç­‰å·æˆç«‹æ¡ä»¶ã¯ $\mu_X = b$ï¼å¾“ã£ã¦ï¼Œ

$$
\mathbb E[X] = \arg\min_b \mathbb E[(X - b)^2]
$$

:::

### Markov and Chebyshev Inequalities

ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦ï¼Œç¢ºç‡å¯†åº¦é–¢æ•°ã‚„åˆ†å¸ƒé–¢æ•°ãŒã‚ã‹ã£ã¦ã„ã‚‹çŠ¶æ³ã¯å°‘ãªã„ã§ã™ï¼ã¾ãŸï¼Œãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚ŒãŸã¨ã—ã¦ã‚‚
ãã‚Œã‚‰ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã¯ã‹ã‚“ãŸã‚“ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ãã®ä¸­ã§ï¼Œ

- $X$ ãŒ mean $\mu$ ã‹ã‚‰ã©ã‚Œãã‚‰ã„é›¢ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã‹
- $\Pr(\vert X \leq a\vert )$ ã®upper boundã¯ã©ã‚Œãã‚‰ã„ã‹ï¼Ÿ

ã¨ã„ã†çµ±è¨ˆçš„æ¨æ¸¬ã‚’ã—ãŸã„ã¨ãã«ä½¿ç”¨ã•ã‚Œã‚‹Markov and Chebyshev Inequalitiesã‚’è§£èª¬ã—ã¾ã™ï¼


<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Markovâ€™s Inequality**
<br>

non-negative ç¢ºç‡å¤‰æ•° $X \geq 0$ï¼Œconstant $k >0$ ã«ã¤ã„ã¦ä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹

$$
\Pr(X \geq k) \leq \frac{\mathbb E[X]}{k}
$$

ã¤ã¾ã‚Šï¼Œ

$$
\Pr(X \geq k\mathbb E[X]) \leq \frac{1}{k}
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[X] &= \int_0^\infty xf(x)\mathrm{d}x\\
             &= \int_0^k xf(x)\mathrm{d}x + \int_k^\infty xf(x)\mathrm{d}x\\
             &\geq \int_k^\infty xf(x)\mathrm{d}x\\
             &\geq \int_k^\infty kf(x)\mathrm{d}x\\
             &= k \Pr(X \geq k)
\end{align*}
$$

:::

::: {.callout-note collapse="false" icon=false}
## Proof: å¤‰æ•°å¤‰æ›

$$
Y = 
\begin{cases}
    0 & \text{if} X < k\\[5pt]
    k & \text{if} X \geq k
\end{cases}
$$

ã®ã‚ˆã†ã«å¤‰æ•°å¤‰æ›ã‚’ã™ã‚‹ã¨å¸¸ã« $Y \leq X$ ã§ã‚ã‚‹ã®ã§ $\mathbb E[Y] \leq \mathbb E[X]$.

$$
\begin{align*}
&\mathbb E[Y] = k\Pr(X\geq k)\\
\Rightarrow &\Pr(X\geq k)\leq \frac{\mathbb E[X]}{k}
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Markovâ€™s Inequality with function**
<br>

ç¢ºç‡å¤‰æ•° $X$ ã®é–¢æ•° $g(X)$ ãŒ $g(X)\geq 0$ ã‚’æº€ãŸã™ã¨ã™ã‚‹ï¼ä»»æ„ã®æ­£ã®å®Ÿæ•° $c\in\mathbb R_{++}$ ã«å¯¾ã—ã¦

$$
\Pr(g(X)\geq c) \leq \frac{\mathbb E[g(X)]}{c}
$$

ãŒæˆç«‹ã™ã‚‹ï¼

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[g(X)]
    &= \mathbb E[g(X)(\mathbb 1(g(X) \geq c) + \mathbb 1(g(X) < c) )]\\
    &\geq \mathbb E[g(X)\mathbb 1(g(X) \geq c)]\\
    &\geq c\mathbb E[\mathbb 1(g(X) \geq c)]\\
    &= c\Pr(g(X)\geq c)
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
<br>

ç¢ºç‡å¤‰æ•° $X$ ã®é–¢æ•° $g(X)$ ãŒ $g(X)\geq 0$ ã‹ã¤ $\mathbb E[g(X)] = 0$ ã‚’æº€ãŸã™ã¨ã™ã‚‹ï¼
ã“ã®ã¨ãï¼Œ

$$
\Pr(g(X) = 0) = 1
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

èƒŒç†æ³•ã§ç¤ºã—ã¾ã™ï¼$\Pr(g(X) = 0) < 1$ ã¨ã™ã‚‹ã¨ï¼Œã‚ã‚‹ $c >0$ ã«ã¤ã„ã¦ $\Pr(g(X) \geq c) > 0$ ã¨ãªã‚Šã¾ã™ï¼

ãƒãƒ«ã‚³ãƒ•ä¸ç­‰å¼ã‚ˆã‚Šï¼Œ

$$
\Pr(g(X) \geq c) \leq \frac{\mathbb E[g(X)]}{c} = 0
$$

ã¨ãªã‚‹ã¯ãšã§ã™ãŒï¼Œã“ã‚Œã¯ $\Pr(g(X) \geq c) > 0$ ã«çŸ›ç›¾ï¼å¾“ã£ã¦ï¼Œï¼$\Pr(g(X) = 0) = 1$ ãŒæˆã‚Šç«‹ã¡ã¾ã™ï¼ 

:::

<div class="blog-custom-border">
<strong >ğŸ“˜ REMARKS</strong> <br>

- Markovâ€™s inequalityã¯ ç¢ºç‡å¤‰æ•° $X$ ãŒnon-negative, population mean $\mu$ ã®çŸ¥è­˜ã®ã¿ã§ä½¿ç”¨å¯èƒ½
- ä¸€æ–¹ï¼Œboundå¹…ã¯å¤§ããï¼Œweakest inequalityã§ã‚ã‚‹

</div>

::: {#exm- .custom_problem }
<br>

ç‚¹æ•°ç¯„å›²ãŒ $\Omega_x=[0, 110]$ ã®è©¦é¨“ã‚’ã¤ã„ã¦ï¼Œãã®ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã‚‹ï¼åˆ†å¸ƒã®æƒ…å ±ã¯ã‚ã‹ã‚‰ãªã„ãŒ
population meanã¯ 25 ã§ã‚ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ï¼ã“ã®ã¨ãï¼Œ$\Pr(X \geq 100)$ ã®upper boundã¯Markov's inequalityã‚’ç”¨ã„ã¦
ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã§ãã¾ã™ï¼

$X$ ãŒnon-negativeãªã®ã§

$$
\begin{align*}
\Pr(X\geq 100) &\leq \frac{25}{100}\\
               &= \frac{1}{4}
\end{align*}
$$

:::

::: {#exm-binom-markov .custom_problem }
**: weak inequality**
<br>

$X_i \overset{\mathrm{iid}}{\sim} \operatorname{Bernoulli}(0.2)$ ã‚’20å›ç¹°ã‚Šè¿”ã™è©¦è¡Œã‚’è€ƒãˆã‚‹ï¼ã“ã®è©¦è¡Œã®çµæœã®ã‚¢ã‚¦ãƒˆã‚«ãƒ ã‚’ $Y$ ã¨ã—ãŸã¨ãï¼Œ

$$
\Pr(Y \geq 16) = \sum_{k=16}^{20} {}_{20}C_{k} 0.2^k 0.8^{20-k} \approx 1.38\cdot 10^{-8}
$$

ä¸€æ–¹ï¼ŒMarkov's inequalityã‚’ç”¨ã„ã‚‹ã¨

$$
\begin{align*}
\Pr(Y \geq 16) \leq \frac{4}{16} = \frac{1}{4}
\end{align*}
$$

ã“ã®ã‚ˆã†ã«ï¼Œboundå¹…ã¯å¤§ãã„ã“ã¨ãŒåˆ†ã‹ã‚‹ï¼

:::


<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Chebyshevâ€™s inequality**
<br>

$X \sim D(\mu, \sigma^2)$ ã¨ã™ã‚‹ï¼ãŸã ã—ï¼Œ$D$ ã®å½¢çŠ¶ã¯ã‚ã‹ã‚‰ãªã„ï¼å®Ÿæ•° $\alpha >0$ ã«ã¤ã„ã¦ï¼Œä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹

$$
\Pr(\vert X - \mu \vert \geq \alpha) \leq \frac{\sigma^2}{\alpha^2}
$$

ã¤ã¾ã‚Šï¼Œ

$$
\Pr(\vert X - \mu \vert \geq \alpha \sigma) \leq \frac{1}{\alpha^2}
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$I = \{x: \vert x -\mu \vert \geq k\}$ ã¨ã™ã‚‹ï¼

$$
\begin{align*}
\sigma^2 &= \int_{\mathbb R} (x - \mu)^2f(x)\mathrm{d}x\\
         &\geq  \int_{I} (x - \mu)^2f(x)\mathrm{d}x\\
         &\geq  \int_{I} k^2f(x)\mathrm{d}x\\
         &= k^2 \Pr(\vert x - \mu\vert \geq k)
\end{align*}
$$

ä»¥ä¸Šã‚ˆã‚Šï¼Œ$\displaystyle\Pr(\vert X - \mu \vert \geq k) \leq \frac{\sigma^2}{k^2}$ ã‚’å¾—ã‚‹ï¼

:::

::: {.callout-note collapse="false" icon=false}
## Proof: using Markov's inequality

$(x - \mu)^2$ ã‚’ç¢ºç‡å¤‰æ•°ã¨è€ƒãˆã‚‹ã¨ï¼Œnon-negativeç¢ºç‡å¤‰æ•°ã«ãªã‚‹ï¼Œã¤ã¾ã‚ŠMarkov's inequalityã‚’ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§

$$
\begin{align*}
\Pr(\vert x - \mu\vert \geq k) &= \Pr((x - \mu)^2 \geq k^2)\\
                               &\leq \frac{\mathbb E[(x - \mu)^2]}{k^2} \because{\text{Markov's inequality}}\\
                               &= \frac{\sigma^2}{k^2}
\end{align*}
$$


:::

::: {#exm- .custom_problem }
**Markov's inequality vs Chebyshevâ€™s inequality**
<br>

$X \sim \operatorname{Binom}(n=20, p=0.2)$ ã«ã¤ã„ã¦ï¼Œ[weak inequality](#exm-binom-markov)
ã§ç¢ºèªã—ãŸã‚ˆã†ã«ï¼ŒMarkov's inequalityã®ã‚ˆã‚Š

$$
\Pr(X \geq 16) = \Pr(X \geq 4\mathbb E[X]) \leq \frac{1}{4}
$$

ä¸€æ–¹ï¼ŒChebyshevâ€™s inequalityã‚’ç”¨ã„ã‚‹ã¨

$$
\begin{align*}
\Pr(X \geq 16) &\leq \Pr(\vert X - 4\vert \geq 12)\\
               &\leq \frac{\operatorname{Var}(X)}{12^2}\\
               &\leq \frac{3.2}{12^2}\\
               &= \frac{1}{45}
\end{align*}
$$

:::

::: {.nte- .callout-tip icon="false"}
# ğŸµ Green Tea Break

- Chebyshevâ€™s inequalityã¯Markov's inqualityã¨ç•°ãªã‚Šï¼Œç¢ºç‡å¤‰æ•° $X$ ãŒnon-negativeã§ã‚ã‚‹å¿…è¦ã¯ãªã„
- meanã‹ã‚‰ã®è·é›¢ã«ã¤ã„ã¦ã®æƒ…å ±ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹

:::

::: {#exm- .custom_problem }
**: ãƒ•ã‚§ã‚¢ãªã‚µã‚¤ã‚³ãƒ­ã¨å‡ºãŸç›®ã®ç¯„å›²**
<br>

$n$ å›ç‹¬ç«‹ã«ï¼–é¢ã®ãƒ•ã‚§ã‚¢ãªã‚µã‚¤ã‚³ãƒ­ã‚’æŠ•ã’ã‚‹è©¦è¡Œã‚’è€ƒãˆã¾ã™ï¼ã“ã®ã¨ãï¼Œ6ã®ç›®ãŒå‡ºãŸå›æ•° $X$ ãŒ $[n/6 - \sqrt{n}, n/6 + \sqrt{n}]$ ã«åã¾ã‚‹ç¢ºç‡ã¯
exactã«è¨ˆç®—ã™ã‚‹ã«ã¯å¤§å¤‰ã§ã™ãŒï¼ŒChebyshev inequalityã‚’ç”¨ã„ã¦lower bouldã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼

ã‚µã‚¤ã‚³ãƒ­ã‚’iå›ç›®æŠ•ã’ãŸã¨ãã«å‡ºãŸç›®ã«ã¤ã„ã¦

$$
\begin{align*}
Z_i = \left\{\begin{array}{cc}
    1 & \text{if 6ã®ç›®ãŒã§ã‚‹}\\
    0 & \text{otherwise}
\end{array}\right.
\end{align*}
$$

ã¨ã„ã†ç¢ºç‡å¤‰æ•°ã‚’è€ƒãˆãŸã¨ãï¼Œ

$$
X = \sum_{i=1}^n Z_i \sim \operatorname{Binom}(n, 1/6)
$$

ã¨è¡¨ã™ã“ã¨ãŒã§ãã¾ã™ï¼ã“ã®ã¨ãChebyshev inequalityã‚ˆã‚Š

$$
\begin{align*}
P(\vert X - \mu \vert \geq k) \leq \frac{\sigma^2}{k^2}\\
\Rightarrow P(\vert X - \mu \vert \geq \sqrt{n}) \leq \frac{1}{n}\frac{5n}{36} = \frac{5}{36}
\end{align*}
$$

ã—ãŸãŒã£ã¦ï¼Œ

$$
P(\vert X - \mu \vert < \sqrt{n}) \geq 1- \frac{5}{36} = \frac{31}{36}
$$


<strong > &#9654;&nbsp; Pythonã§ã®Boundã®ç¢ºèª</strong>

$n$ ã®å›æ•°ã‚’å¢—ã‚„ã—ã¦lower-boundã«è¿‘ã¥ã„ã¦ã„ãã®ã‹ç¢ºèªã—ãŸæ‰€ï¼Œãã®ã‚ˆã†ãªæŒ™å‹•ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ç¢ºèªã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼
ä¸€æ–¹ï¼Œæ­£è¦åˆ†å¸ƒã§ã®è¿‘ä¼¼ã®å€¤ã«åæŸã—ã¦ã„ãã“ã¨ãŒç¢ºèªã§ãã¾ã™ï¼åæŸå…ˆã®å€¤ $p^*$ ã¯

$$
p^* = \Phi(1/\sqrt{5/36}) - \Phi(-1/\sqrt{5/36}) \approx 0.9927
$$

```{python}
import numpy as np
from scipy.stats import binom
import plotly.graph_objects as go
from scipy.stats import norm
from regmonkey_style import stylewizard as sw

sw.set_templates("regmonkey_twoline")

N = np.arange(1, 1000)
p = 1 / 6
norm_instance = norm(N / 6, np.sqrt(N * 5 / 36))

exact_prob = binom.cdf(N / 6 + np.sqrt(N), N, p) - binom.cdf(N / 6 - np.sqrt(N), N, p)
norm_approx = norm_instance.cdf(N / 6 + np.sqrt(N)) - norm(
    N / 6, np.sqrt(N * 5 / 36)
).cdf(N / 6 - np.sqrt(N))
lowerbound_prob = 31 / 36

# Create the plot
fig = go.Figure()

# Add exact probability line
fig.add_trace(
    go.Scatter(
        x=N, y=exact_prob, mode="lines", line=dict(width=3), name="Exact Probability"
    )
)

fig.add_trace(
    go.Scatter(
        x=N,
        y=norm_approx,
        mode="lines",
        line=dict(width=3),
        name="Normal-approx Probability",
    )
)

# Add approximate probability line
fig.add_trace(
    go.Scatter(
        x=N, y=[lowerbound_prob] * len(N), mode="lines", name="Lower-bound Probability"
    )
)

# Add labels and title
fig.update_layout(
    title="Exact vs normal-approx vs lower-bound Probability",
    xaxis_title="N (log-scale)",
    yaxis_title="Probability",
    legend=dict(x=0.8, y=0.99),
    yaxis_range=[0.85, 1.05],
        xaxis=dict(
        type='log',
        tickvals=[1, 10, 100, 1000, 10000, 100000],
        ticktext=['1', '10', '100', '1k', '10k', '100k']
    )
)

# Show the plot
fig.show()
```

:::


### Weak Law of Large Numbers

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Weak Law of Large Numbers**
<br>

å¹³å‡ $\mu$, åˆ†æ•£ $\sigma^2$ ã®åˆ†å¸ƒã«ç‹¬ç«‹ã«å¾“ã†ç¢ºç‡å¤‰æ•° $X_1, \cdots, X_n$ ã‚’è€ƒãˆã‚‹ï¼æ¨™æœ¬å¹³å‡ã‚’ $\overline{X_n} = \frac{1}{n}\sum_{i=1}^nX_i$ ã¨ã™ã‚‹ï¼

ã“ã®ã¨ãï¼Œä»»æ„ã®å®Ÿæ•° $\epsilon >0$ ã«å¯¾ã—ã¦ï¼Œ

$$
\lim_{n\to\infty}\Pr(\vert \overline{X_n} - \mu \vert > \epsilon) = 0
$$

ã¤ã¾ã‚Šï¼Œ**æ¨™æœ¬å¹³å‡ã¯æ¯å¹³å‡ã«ç¢ºç‡åæŸ**ã™ã‚‹ï¼

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

Chebyshevâ€™s inequalityã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«ç¤ºã›ã‚‹

$$
\begin{align*}
\lim_{n\to\infty}\Pr(\vert \overline{X_n} - \mu \vert > \epsilon) 
                &\leq \lim_{n\to\infty} \frac{\operatorname{Var}(\overline{X_n})}{\epsilon^2}\\
                &= \lim_{n\to\infty} \frac{\sigma^2}{n\epsilon^2}\\
                &=0
\end{align*}
$$

:::

## Stein identity

<div class="blog-custom-border">
::: {#thm-stein-identity .custom_problem }
**: ã‚¹ã‚¿ã‚¤ãƒ³ã®ç­‰å¼**
<br>

ç¢ºç‡å¤‰æ•° $X\sim N(\mu, \sigma^2)$ ã¨ã™ã‚‹ï¼$g(\cdot)$ ãŒå¾®åˆ†å¯èƒ½ã§ $\mathbb E[\vert g^\prime(X)\vert]<\infty$ åŠã³ $\mathbb E[\vert g(X)\vert]<\infty$ ã®ã¨ãï¼Œ

$$
\mathbb E[(X-\mu)g(X)] = \sigma^2\mathbb E[g^\prime(X)]
$$

ãŒæˆã‚Šç«‹ã¤ï¼

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\mathbb E[(X - \mu)g(X)] = \frac{1}{\sqrt{2\pi\sigma^2}}\int^\infty_{-\infty}g(x)(x-\mu)\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\,\mathrm{d}x
$$

ã“ã“ã§ï¼Œ

$$
h(x) = -\sigma^2\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

ã¨ãŠãã¨ï¼Œ

$$
h^\prime(x) =(x-\mu)\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

å¾“ã£ã¦ï¼Œ

$$
\begin{align*}
\mathbb E[(X - \mu)g(X)] 
    =& \frac{1}{\sqrt{2\pi\sigma^2}}[h(x)g(x)]^\infty_{-\infty} - \frac{1}{\sqrt{2\pi\sigma^2}}\int^\infty_{-\infty} g^\prime(x)h(x)\,\mathrm{d}x\\
    =& \frac{1}{\sqrt{2\pi\sigma^2}}\left[-\sigma^2\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)g(x)\right]^\infty_{-\infty}\\
     & + \frac{\sigma^2}{\sqrt{2\pi\sigma^2}}\int^\infty_{-\infty} g^\prime(x)\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\,\mathrm{d}x\\
    =& \sigma^2\mathbb E[g^\prime(X)]
\end{align*}
$$

:::

<strong > &#9654;&nbsp; ã‚¹ã‚¿ã‚¤ãƒ³ã®ç­‰å¼ã¨ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆè¨ˆç®—</strong>

$m$ æ¬¡ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã¯

$$
\begin{align*}
\mathbb E[X^m]
    &= \mathbb E[(X - \mu)X^{m-1} + \mu X^{m-1}]\\
    &= \mathbb E[(X - \mu)X^{m-1}] + \mu \mathbb E[X^{m-1}]
\end{align*}
$$

ã“ã“ã§ã‚¹ã‚¿ã‚¤ãƒ³ã®ç­‰å¼ã‚’ç”¨ã„ã‚‹ã¨

$$
\mathbb E[(X - \mu)X^{m-1}] = \sigma^2\mathbb E[(m-1)X^{m-2}]
$$

å¾“ã£ã¦ï¼Œ

$$
\mathbb E[X^m] = \sigma^2\mathbb E[(m-1)X^{m-2}] + \mu \mathbb E[X^{m-1}]
$$



## åˆ†æ•£

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: BienaymÃ© Equality**
<br>

äº’ã„ã«ç‹¬ç«‹ãªç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹

$$
\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y)
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\operatorname{Var}(X+Y)
    &= \mathbb E[((X+Y) - (\mu_X+\mu_Y))^2]\\
    &= \mathbb E[((X- \mu_X)+(Y - \mu_Y))^2]\\
    &= \mathbb E[(X- \mu_X)^2] + 2\mathbb E[(X- \mu_X)(Y- \mu_Y)] + \mathbb E[(Y- \mu_Y)^2]\\
    &= \mathbb E[(X- \mu_X)^2] + 2\mathbb E[(X- \mu_X)]\mathbb E[(Y- \mu_Y)] + \mathbb E[(Y- \mu_Y)^2] \quad \because{\text{ç‹¬ç«‹æ€§}}\\ 
    &= \operatorname{Var}(X) + \operatorname{Var}(Y)
\end{align*}
$$

:::

ãªãŠï¼Œç¢ºç‡å¤‰æ•° $X, Y$ ãŒç‹¬ç«‹ã§ã¯ãªã„å ´åˆã¯

$$
\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X, Y)
$$

ãŒæˆç«‹ã—ã¾ã™ï¼$\mu_X = \mathbb E[X], \mu_Y = \mathbb E[Y]$ ã¨ã™ã‚‹ã¨ï¼Œ

$$
\begin{align*}
\operatorname{Var}(X+Y) &= \mathbb E[(X+Y - \mu_x - \mu_y)^2]\\
         &= \mathbb E[(X-\mu_x)^2] + 2\mathbb E[(X-\mu_x)(Y-\mu_y)] + \mathbb E[(Y-\mu_y)^2]\\
         &= \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X,Y)
\end{align*}
$$

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: nå€‹ã®ç¢ºç‡å¤‰æ•°ã®å’Œã®åˆ†æ•£**
<br>

ç¢ºç‡å¤‰æ•° $X_1, \cdots, X_n$, ãã‚Œãã‚Œã®æœŸå¾…å€¤ãŒå­˜åœ¨ã— $\mu_1, \cdots, \mu_n$ ã¨è¡¨ã›ã‚‹ã¨ãï¼Œ

$$
\operatorname{Var}(X_1+X_2 + \cdots + X_n) = \sum_i^n \operatorname{Var}(X_i) + 2\sum_{i<j}\operatorname{Cov}(X_i, X_j)
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\operatorname{Var}(X_1+\cdots+X_n) 
    &= \mathbb E\left[(X_1+\cdots+X_n - \mu_1 - \cdots - \mu_n)^2\right]\\[5pt]
    &= \mathbb E\left[\{(X_1+\mu_1)\cdots + \cdots + (X_n - \mu_n)\}^2\right]\\[5pt] 
    &= \mathbb E\left[\sum (X_i - \mu_i)^2 + 2\sum_{i<j}(X_i - \mu_i)(X_j-\mu_j)\right]\\[5pt]
    &= \sum \mathbb E[(X_i - \mu_i)^2] +2\sum_{i<j}\mathbb E[(X_i - \mu_i)(X_j-\mu_j)]\\
    &= \sum_i^n \operatorname{Var}(X_i) + 2\sum_{i<j}\operatorname{Cov}(X_i, X_j)
\end{align*}
$$

:::


### æ¡ä»¶ä»˜ãåˆ†æ•£

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ã®æ¡ä»¶ä»˜ãåˆ†æ•£ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ„å‘³ã‚’æŒã¤

- $\operatorname{Var}(X\vert Y=y)$ ã¯ï¼Œ$Y = y$ ã¨å›ºå®šã—ãŸã¨ãã® $X$ ã®åˆ†æ•£
- $\operatorname{Var}(X\vert Y)$ ã¯ï¼Œ$Y$ ãŒãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚ŒãŸå€¤ã«å›ºå®šã•ã‚ŒãŸå ´åˆã® $X$ ã®åˆ†æ•£

$\operatorname{Var}(X\vert Y)$ ã¯ $Y$ ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã«ä¾å­˜ã—ãŸç¢ºç‡å¤‰æ•°ã§ã‚ã‚‹ä¸€æ–¹ï¼Œ
$\operatorname{Var}(X\vert Y=y)$ ã¯ $y$ ã®é–¢æ•°ã¨ã„ã†é•ã„ãŒã‚ã‚‹

<div class="blog-custom-border">
<strong>Def: æ¡ä»¶ä»˜ãåˆ†æ•£</strong> <br>

$$
\operatorname{Var}(Y\vert X) = \mathbb E[(Y^2\vert X)] - (\mathbb E[(Y\vert X)])^2 = \mathbb E[(Y - \mathbb E[Y\vert X])^2\vert X]
$$

</div>

<br>

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Law of Total Variance**
<br>

$$
\operatorname{Var}(Y) = \operatorname{Var}(\mathbb E[Y\vert X]) + \mathbb E_X[\operatorname{Var}(Y\vert X)]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\epsilon = Y - \mathbb E[Y\vert X]$ ã¨ã—ãŸã¨ãï¼Œ$\epsilon$ ã¨ $\mathbb E[Y\vert X]$ ã¯ç„¡ç›¸é–¢ãªã®ã§ï¼Œ

$$
\operatorname{Var}(Y) = \operatorname{Var}(\mathbb E[Y\vert X]) + \operatorname{Var}(\epsilon)
$$

$\mathbb E[\epsilon] = 0$ ã‚ˆã‚Šï¼Œ

$$
\begin{align*}
\operatorname{Var}(\epsilon)
    &= \mathbb E[\epsilon^2] - (\mathbb E[\epsilon])^2\\
    &= \mathbb E[\epsilon^2]\\
    &= \mathbb E_X(\mathbb E[\epsilon^2\vert X])\\
    &= \mathbb E_X[\operatorname{Var}(Y\vert X)]
\end{align*}
$$

å¾“ã£ã¦ï¼Œ

$$
\operatorname{Var}(Y) = \operatorname{Var}(\mathbb E[Y\vert X]) + \mathbb E_X[\operatorname{Var}(Y\vert X)]
$$

:::

Law of Total Varianceã‚ˆã‚Š $Y$ ã®åˆ†æ•£ã¯ï¼ŒCEFã®åˆ†æ•£ + èª¤å·®é …ã®åˆ†æ•£ã«åˆ†è§£ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼
å®Ÿå‹™ã«ãŠã‘ã‚‹åˆ†æã«ãŠã„ã¦ï¼Œè³ƒé‡‘ã®ãƒãƒ©ãƒ„ã‚­ã‚’

- è³ƒé‡‘ã‚’èª¬æ˜ã™ã‚‹å„å€‹äººã®ç‰¹å¾´ã®ãƒãƒ©ãƒ„ã‚­
- ç‰¹å¾´ã§èª¬æ˜ã™ã‚‹ã“ã¨ã®ã§ããªã„è³ƒé‡‘ã®ãƒãƒ©ãƒ„ã‚­(=èª¤å·®é …)ã®æœŸå¾…å€¤

ã«åˆ†è§£ã—ã¦è€ƒå¯Ÿã™ã‚‹éš›ã«Law of Total Varianceã‚’ä½¿ç”¨ã—ãŸã‚Šã—ã¾ã™ï¼

### æ­ªåº¦ã¨å°–åº¦

$X_1\sim N(1, 1)$ ã¨ $X_2\sim\operatorname{Exponential}(1)$ ã‚’ï¼’ã¤ã®åˆ†å¸ƒã‚’è€ƒãˆã¾ã™ï¼ã©ã¡ã‚‰ã‚‚å¹³å‡ï¼Œåˆ†æ•£å…±ã« 1 ã§ä¸€è‡´ã—ã¦ã„ã¾ã™ãŒä»¥ä¸‹ã®ã‚ˆã†ã«åˆ†å¸ƒã®å½¢çŠ¶ã¯å¤§ããç•°ãªã‚Šã¾ã™ï¼

```{python}
import numpy as np
import plotly.express as px
import polars as pl

np.random.seed(42)
n = 1000

x1 = np.random.normal(loc=1, scale=1, size=n)
x2 = np.random.exponential(scale=1, size=n)

df = pl.DataFrame({"normal_dist": x1, "exp_dist": x2})

px.histogram(df, histnorm="probability density",
             opacity=0.8, barmode="overlay",
             title='Exp(1) vs Normal(1, 1): same mean and variance')
```

å¹³å‡ã‚„åˆ†æ•£(locationã¨scale)ã«ã‚ˆã£ã¦ç¢ºç‡åˆ†å¸ƒã®æ§˜å­ã¯ã‚ã‚‹ç¨‹åº¦ã‚ã‹ã‚Šã¾ã™ãŒï¼Œlocationã¨scaleãŒåŒã˜ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšä¸Šè¨˜ã®æŒ‡æ•°åˆ†å¸ƒã¯ $N(1, 1)$ ã«å¯¾ã—ã¦ï¼Œå³ã®è£¾ãŒé•·ã„åˆ†å¸ƒã«ãªã£ã¦ã„ã¾ã™ï¼åˆ†å¸ƒã®éå¯¾ç§°æ€§ã‚„å°–ã‚Šã®ç¨‹åº¦ã‚’ç†è§£ã™ã‚‹ã«ã‚ãŸã£ã¦å°–åº¦ã¨æ­ªåº¦ã‚’ç”¨ã„ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼

<div class="blog-custom-border">
<strong>Def: æ­ªåº¦(skewness)ã¨å°–åº¦(kurtosis)</strong> <br>

ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦ï¼Œæ­ªåº¦ã¨å°–åº¦ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹

$$
\begin{gather*}
\operatorname{skewness} = \mathbb E\left[\left(\frac{X - \mathbb E[X]}{\sqrt{\operatorname{Var}(X)}}\right)^3\right]\\
\operatorname{kurtosis} = \mathbb E\left[\left(\frac{X - \mathbb E[X]}{\sqrt{\operatorname{Var}(X)}}\right)^4\right]
\end{gather*}
$$

</div>

::: {.nte- .callout-tip icon="false"}
# ğŸµ Green Tea Break

- skewnessã¯positiveãªã‚‰ã°å³ã®è£¾ãŒé•·ãï¼Œnegativeãªã‚‰ã°å·¦ã«è£¾ãŒé•·ã„, 0ãªã‚‰ã°å¯¾ç§°åˆ†å¸ƒ
- kurtosisã¯å¤§ãã„ã»ã©ï¼Œé‹­ã„ãƒ”ãƒ¼ã‚¯ã¨é•·ãå¤ªã„è£¾ã‚’ã‚‚ã£ãŸåˆ†å¸ƒã«ãªã‚‹
- skewness,kurtosisã¨ã‚‚ã«æ¨™æº–åŒ–ã—ã¦ã‹ã‚‰3rd-moment, 4th-momentã‚’è¨ˆç®—ã—ã¦ã„ã‚‹ã®ã§ï¼Œnon-zeroã® $a, b$ ã‚’å®šæ•°ã¨ã—ãŸã¨ãï¼Œ$aX + b$ ã¨å¤‰æ•°å¤‰æ›ã‚’è¡Œã£ã¦ã‚‚ï¼Œè¨ˆç®—çµæœã¯å¤‰ã‚ã‚‰ãªã„ = å°ºåº¦ã®å¤‰æ›é–¢ã—ã¦ä¸å¤‰


:::

::: {#exm- .custom_problem }
**: ä¸€æ§˜åˆ†å¸ƒã®æ­ªåº¦ã¨å°–åº¦**
<br>

$X\sim\operatorname{Unif}(0, 1)$ ã¨ã—ãŸã¨ãï¼Œä¸€æ§˜åˆ†å¸ƒã¯locationã‹ã‚‰å·¦å³å¯¾ç§°ã®åˆ†å¸ƒãªã®ã§è¨ˆç®—ã™ã‚‹ã“ã¨ãªã 

$$
\operatorname{skewness} = 0
$$

ã¨ã‚ã‹ã‚‹ï¼ä¸€æ–¹ï¼Œå°–åº¦ã¯

$$
\begin{align*}
\operatorname{kurtosis}
    &= \frac{1}{\sigma^4}\int_0^1 \left(x - \frac{1}{2}\right)^4\mathrm{d}x\\
    &= 144 \times \frac{1}{5}\left[\left(x - \frac{1}{2}\right)^5\right]^1_0\\
    &= \frac{9}{5}
\end{align*}
$$

æ¨™æº–æ­£è¦åˆ†å¸ƒã®å°–åº¦ã‚’åŸºæº–ã«ã—ã¦

$$
\operatorname{kurtosis} = \frac{9}{5}-3=-\frac{6}{5}
$$

ã¨è¡¨ç¾ã™ã‚‹å ´åˆã‚‚ã‚ã‚‹

:::
