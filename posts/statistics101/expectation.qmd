---
title: "æœŸå¾…å€¤"
author: "Ryo Nakagami"
date: "2024-09-12"
date-modified: last-modified
number_sections: false
code-fold: true
comments:
    utterances:
         repo: RyoNakagami/statistics-for-regression-monkey
         label: discussion
jupyter: python3
---

## æœŸå¾…å€¤ã®æ€§è³ª

<div class="blog-custom-border">
<strong>Def: é€£ç¶šç¢ºç‡å¤‰æ•°ã®æœŸå¾…å€¤</strong> <br>

$f$ ã‚’ç¢ºç‡å¤‰æ•° $X$ ã®ç¢ºç‡å¯†åº¦é–¢æ•°ã¨ã™ã‚‹ï¼$\int_{\mathbb R} \vert x\vert f(x) \mathrm{d}x < \infty$ 
ã®ã¨ãï¼Œ$X$ ã®æœŸå¾…å€¤ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹:

$$
\mathbb E[X] = \int_{\mathbb R} x f(x) \mathrm{d}x
$$

ã¾ãŸï¼Œ$X$ ã®é–¢æ•° $g(X)$ ã®æœŸå¾…å€¤ã¯ $\int_{\mathbb R} \vert g(x)\vert f(x) \mathrm{d}x < \infty$ ãªã‚‰ã°

$$
\mathbb E[g(X)] = \int_{\mathbb R} g(x) f(x) \mathrm{d}x
$$


</div>

å®šç¾©ã‚ˆã‚Šç¢ºç‡å¯†åº¦é–¢æ•°ã§é‡ã¿ã¥ã‘ãŸå¹³å‡ãŒç¢ºç‡å¤‰æ•°ã®æœŸå¾…å€¤ã«ãªã‚‹ã¨è§£é‡ˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼meanã¯åˆ†å¸ƒã®ä½ç½®ã‚’è¡¨ã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¨ã‚‚è§£é‡ˆã§ãã‚‹ã®ã§
**location parameterï¼ˆä½ç½®æ¯æ•°ï¼‰**ã¨å‘¼ã¶ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ï¼ä¸€æ–¹ï¼Œæ¨™æº–åå·® $\sigma$ ã¯**scale parameterï¼ˆå°ºåº¦æ¯æ•°ï¼‰**ã¨ã„ã„ã¾ã™ï¼

::: {#exm- .custom_problem }
**æŒ‡æ•°åˆ†å¸ƒã®æœŸå¾…å€¤**
<br>

rate parameter $\lambda$ ã®æŒ‡æ•°åˆ†å¸ƒã«å¾“ã†ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã¾ã™ï¼

$$
\begin{align*}
\mathbb E[X] &= \int^\infty_0 x \lambda \exp(-\lambda x)\mathrm{d}x\\
             &= \bigg[-x\exp(-\lambda x)\bigg]^\infty_0 + \int^\infty_0 \exp(-\lambda x)\mathrm{d}x\\
             &= \int^\infty_0 \exp(-\lambda x)\mathrm{d}x\\
             &= -\frac{1}{\lambda}\bigg[\exp(-\lambda x)\bigg]^\infty_0\\
             &= \frac{1}{\lambda}
\end{align*}
$$


æŒ‡æ•°åˆ†å¸ƒã¯é›»çƒã®å¯¿å‘½ãªã©ã«å¿œç”¨ã•ã‚Œã‚‹åˆ†å¸ƒã§ã™ãŒï¼Œrate parameter $\lambda$ ãŒå°ã•ã„ã»ã©æœŸå¾…å€¤ï¼ˆ= é›»çƒã®å¯¿å‘½ï¼‰ãŒå¤§ãããªã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ï¼

:::

::: {#exm- .custom_problem }
**æœŸå¾…å€¤ãŒå®šç¾©ã§ããªã„é›¢æ•£åˆ†å¸ƒ**
<br>

ç¢ºç‡å¤‰æ•° $X$ ã®supportã‚’åŠ ç®—é›†åˆ $\{2, 2^2, 2^3, \cdots\}$ ã¨ã™ã‚‹ï¼ç¢ºç‡é–¢æ•°ã‚’

$$
\Pr(X = 2^i) = \frac{1}{2^i} \quad (i = 1, 2, \cdots)
$$

ã“ã®ã¨ãï¼Œ

$$
\sum_{i=1}^\infty \Pr(X=2^i) = \sum_{i=1}^\infty\frac{1}{2^i} = 1
$$

ã¨ç¢ºç‡ã®å…¬ç†ã‚’æº€ãŸã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ï¼ä¸€æ–¹ï¼Œ

$$
\begin{align*}
\mathbb E[X]
    &= \sum_{i=1}^\infty 2^i \frac{1}{2^i}\\
    &= \sum_{i=1}^\infty 1 = \infty
\end{align*}
$$

å¾“ã£ã¦ï¼Œç¢ºç‡å¤‰æ•° $X$ ã®åˆ†å¸ƒã¯ï¼ŒæœŸå¾…å€¤ãŒå®šç¾©ã§ããªã„åˆ†å¸ƒã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ï¼

:::

::: {#exm- .custom_problem }
**æœŸå¾…å€¤ãŒå®šç¾©ã§ããªã„é€£ç¶šåˆ†å¸ƒ**
<br>

ç¢ºç‡å¯†åº¦é–¢æ•°
$$
f(x) = \begin{cases}
0 & x < 1\\
\frac{1}{x^2} & x\geq 1
\end{cases}
$$

ã¨ã„ã†ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã‚‹ï¼

$$
\begin{align*}
\int_1^\infty f(x) \mathrm{d}x
    &= \left[\frac{1}{x}\right]^1_\infty = 1
\end{align*}
$$

ä¸€æ–¹ï¼Œ

$$
\begin{align*}
\mathbb E[X]
    &= \int_1^\infty xf(x) \mathrm{d}x\\
    &= \int_1^\infty\frac{1}{x}\mathrm{d}x\\
    &= \left[\log(x)\right]_1^\infty = \infty
\end{align*}
$$

å¾“ã£ã¦ï¼Œç¢ºç‡å¤‰æ•° $X$ ã®åˆ†å¸ƒã¯ï¼ŒæœŸå¾…å€¤ãŒå®šç¾©ã§ããªã„åˆ†å¸ƒã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ï¼

:::


<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Tail probabilities**
<br>

$[0, b]$ ã®å®šç¾©åŸŸã‚’ã‚‚ã¤éè² ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã‚‹ï¼$F$ ã‚’ç´¯ç©åˆ†å¸ƒé–¢æ•°ã¨ã™ã‚‹ã¨ã

$$
\mathbb E[X] = \int_0^b (1 - F(x))\mathrm{d}x
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\bigg[xF(x)\bigg]^b_0 = \int^b_0xf(x) \mathrm{d}x + \int^b_0F(x) \mathrm{d}x
$$

ã‚’ç”¨ã„ã‚‹ã¨

$$
\begin{align*}
\mathbb E[X] &= b - \int^b_0F(x) \mathrm{d}x\\
             &= \int^b_0 1 \mathrm{d}x - \int^b_0F(x) \mathrm{d}x\\
             &= \int_0^b (1 - F(x))\mathrm{d}x
\end{align*}
$$


:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
<br>

$[0, \infty)$ ã®å®šç¾©åŸŸã‚’ã‚‚ã¤éè² ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã‚‹ï¼$\mathbb E[\vert X^{p+1} \vert] <\infty$ ãŒå®šç¾©å¯èƒ½åŠã³ï¼Œ $F$ ã‚’ç´¯ç©åˆ†å¸ƒé–¢æ•°ã¨ã™ã‚‹ã¨ã

$$
\mathbb E[X^p] = \int_0^\infty px^{p-1} (1 - F(x))\mathrm{d}x \quad \text{where } p > 0
$$

:::
</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\bigg[x^p(1 - F(x))\bigg]^\infty_0 = \int^\infty_0 p x^{p-1}(1 -F(x))\mathrm{d}x - \int^\infty_0 x^{p}f(x)\mathrm{d}x
\end{align*}
$$

$\text{RHS} = 0$ ã§ã‚ã‚‹ã®ã§

$$
\mathbb E[X^p] = \int_0^\infty px^{p-1} (1 - F(x))\mathrm{d}x 
$$

:::

::: {#exm- .custom_problem }
<br>

åŒæ§˜ã®è€ƒãˆã§å®šç¾©åŸŸã‚’ $0,1,2,3,\cdots$ ã¨ã™ã‚‹é›¢æ•£ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦

$$
\mathbb E[X] = \sum_{k=0}^\infty \Pr(X > k)
$$

ãŒæˆç«‹ã—ã¾ã™ï¼

$$
\begin{align*}
\Pr(X > k) &= \Pr(X = k+1) + \Pr(X = k+2) + \cdots\\
           &= \sum_{l=k+1}^\infty \Pr(X=l)
\end{align*}
$$

å¾“ã£ã¦ï¼Œ

$$
\begin{align*}
\sum_{k=0}^\infty \Pr(X > k) &= \sum_{k=0}^\infty \sum_{l=k+1}^\infty \Pr(X=l)\\
                             &= \sum_{l=1}^\infty\sum_{k=0}^{l-1}\Pr(X=l) \quad\because \Pr(X=l) > 0 \\
                             &= \sum_{l=1}^\infty l\Pr(X=l)\\
                             &= \sum_{l=0}^\infty l\Pr(X=l)\\
                             &= \mathbb E[X]
\end{align*}
$$
$$\tag*{\(\blacksquare\)}$$

:::

::: {#exm- .custom_problem }
<br>

$0,1,2,3,\cdots$ ã¨ã™ã‚‹é›¢æ•£ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦

$$
\mathbb E[X^2] = \sum_{k=0}^\infty \Pr(X > k)(2k+1)
$$

ã‚‚æˆç«‹ã™ã‚‹ï¼

$$
\begin{align*}
\sum_{k=0}^\infty \Pr(X > k)(2k+1) 
    &= \sum_{k=0}^\infty \sum_{l=k+1}^\infty \Pr(X=l)(2k+1)\\
    &= \sum_{l=1}^\infty \sum_{k=0}^{l-1}\Pr(X=l)(2k+1)\\
    &= \sum_{l=1}^\infty \Pr(X=l)\sum_{k=0}^{l-1}(2k+1)\\
    &= \sum_{l=1}^\infty l^2\Pr(X=l)\\
    &= \mathbb E[X^2]
\end{align*}
$$

$$\tag*{\(\blacksquare\)}$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**æœŸå¾…å€¤ã®ç·šå‹æ€§**
<br>

$a, b$ ã‚’å®Ÿæ•°ï¼Œç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ä»¥ä¸‹ãŒæˆã‚Šç«‹ã¤

$$
\mathbb E[aX + bY] = a\mathbb E[X] + b\mathbb E[Y]
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

ç¢ºç‡å¤‰æ•° $X, Y$ ãŒæœ‰é™åŠ ç®—ãªæ¨™æœ¬ç©ºé–“ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã«ã¦ä»¥ä¸‹ã‚’ç¤ºã™ï¼

1. $\mathbb E[X + Y] = \mathbb E[X] + \mathbb E[Y]$
2. $\mathbb E[cX] = c\mathbb E[X]$

<strong > &#9654;&nbsp; 1. $\mathbb E[X + Y] = \mathbb E[X] + \mathbb E[Y]$</strong>

ç¢ºç‡å¤‰æ•° $X$ ã¯ $\{x_1, \cdots, x_m\}$, ç¢ºç‡å¤‰æ•° $Y$ ã¯ $\{y_1, \cdots, y_n\}$ ã®å€¤ã‚’ãã‚Œãã‚Œå–ã‚Šã†ã‚‹ã¨ã™ã‚‹ï¼
ã“ã®ã¨ãï¼Œ$Z = X + Y$ ã®æ¨™æœ¬ç©ºé–“ $\{z_1, \cdots, z_k\}$ ã«ã¤ã„ã¦ $k\leq m + n$ ãŒæˆã‚Šç«‹ã¤ï¼

$A_l = \{(i,j): x_i + y_j = z_l\}$ ã¨ã—ãŸã¨ãï¼Œ

$$
\begin{align*}
\mathbb E[X+Y]
    &= \sum_{l=1}^kz_l\Pr(A_l)\\
    &= \sum_{l=1}^k\sum_{(i,j)\in Z_l}(x_i + y_j)\Pr(x_i, y_j)\\
    &= \sum_{i=1}^m\sum_{j=1}^n(x_i + y_j)\Pr(x_i, y_j)\\
    &= \sum_{i=1}^m\sum_{j=1}^nx_i\Pr(x_i, y_j) + y_j\Pr(x_i, y_j)\\
    &= \sum_{i=1}^m\sum_{j=1}^n[x_i\Pr(x_i, y_j) + y_j\Pr(x_i, y_j)]\\
    &= \sum_{i=1}^mx_i\sum_{j=1}^nPr(x_i, y_j) + \sum_{j=1}^ny_j\sum_{i=1}^m\Pr(x_i, y_j)\\
    &=\sum_{i=1}^mx_i \Pr(x_i) + \sum_{j=1}^ny_j \Pr(y_j)\\
    &= \mathbb E[X] + \mathbb E[Y]
\end{align*}
$$


<strong > &#9654;&nbsp; 2. $\mathbb E[cX] = c\mathbb E[X]$</strong>

$$
\begin{align*}
\mathbb E[cX]
    &= \sum_{i=1}^m cx_i = \Pr(cX = cx_i)\\
    &= c\sum_{i=1}^m x_i = \Pr(X = x_i)\\
    &= c\mathbb E[X]
\end{align*}
$$

:::


::: {#exm- .custom_problem }
**: å¤‰æ•°å¤‰æ›ã¨åˆ†æ•£**
<br>

mean $\mu$ ã‚’ã‚‚ã¤ç¢ºç‡å¤‰æ•° $X$ ã¨å®Ÿæ•° $a, b$ ã«ã¤ã„ã¦

$$
\operatorname{Var}(aX + b) = a^2\operatorname{Var}(X)
$$

ãŒæˆç«‹ã—ã¾ã™ï¼è¨¼æ˜ã¯ä»¥ä¸‹ï¼Œ

$$
\begin{align*}
\operatorname{Var}(aX + b) 
    &= \mathbb E[(aX + b) - (a\mu +b)^2]\\
    &= \mathbb E[a^2(X - \mu)^2]\\
    &= a^2 \mathbb E[(X - \mu)^2]\\
    &= a^2\operatorname{Var}(X)
\end{align*}
$$

$$\tag*{\(\blacksquare\)}$$

:::


<div class="blog-custom-border">
::: {#thm- .custom_problem }
**positive operator**
<br>

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ï¼Œ$X\geq Y$ ãŒæˆã‚Šç«‹ã¤ã¨ãï¼Œ

$$
\mathbb E[X] \geq \mathbb E[Y]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$X\geq Y$ ã‚ˆã‚Š $X - Y \geq 0$. æœŸå¾…å€¤ã¯positive operatorãªã®ã§

$$
\mathbb E[X - Y] \geq 0
$$

å¾“ã£ã¦ï¼ŒæœŸå¾…å€¤ã®ç·šå‹æ€§ã‚’ç”¨ã„ã‚‹ã¨

$$
\begin{align*}
\mathbb E[X - Y] &= \mathbb E[X] - \mathbb E[Y] \geq 0
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
<br>

ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦,

$$
\mathbb E[\vert X \vert] \geq \vert \mathbb E[X] \vert
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\vert X\vert \geq X$ ã‚ˆã‚Š 

$$
\mathbb E[\vert X \vert] \geq \mathbb E[X]
$$ 

ã¾ãŸ, $\vert X\vert + X \geq 0$ ã‚ˆã‚Šï¼Œ$\mathbb E[\vert X\vert + X] \geq 0$ï¼Œ
ã¤ã¾ã‚Šï¼Œ

$$
\mathbb E[\vert X \vert] \geq -\mathbb E[X]
$$

ä»¥ä¸Šã‚ˆã‚Šï¼Œ$\mathbb E[\vert X \vert] \geq \vert \mathbb E[X] \vert$


:::




<div class="blog-custom-border">
::: {#thm- .custom_problem }
**äº’ã„ã«ç‹¬ç«‹ãªç¢ºç‡å¤‰æ•°ã®ç©ã®æœŸå¾…å€¤**
<br>

$\mathbb E[\vert X\vert ]<\infty, \mathbb E[\vert Y\vert ]<\infty$ ã‚’æº€ãŸã™, ç¢ºç‡ç©ºé–“ $(\Omega, \mathscr{F},P)$ ä¸Šã§å®šç¾©ã•ã‚ŒãŸç¢ºç‡å¤‰æ•° $X, Y$ ã‚’è€ƒãˆã‚‹ï¼
$X \perp Y$ ã§ã‚ã‚‹ã¨ãï¼Œæ¬¡ãŒæˆç«‹ã™ã‚‹

$$
\mathbb E[XY] = \mathbb E[X]\mathbb E[Y]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[XY] &= \int\int_\Omega xy f(x, y)\mathrm{d}x\mathrm{d}y\\
              &= \int\int_\Omega xy f_X(x)f_Y(y)\mathrm{d}x\mathrm{d}y \quad\because{\text{independence}}\\
              &= \left(\int xf_X(x)\mathrm{d}x\right)\left(\int yf_Y(y)\mathrm{d}y\right)\\
              &= \mathbb E[X]\mathbb E[Y]
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Schwarz inquality**
<br>

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„ã®ä¸ç­‰å¼ãŒæˆç«‹ã™ã‚‹ã“ã¨ã‚’ç¤ºã›

$$
\left(\mathbb E[XY]\right)^2 \leq \mathbb E[X^2]\mathbb E[Y^2]
$$


:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

Quadratic functionã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™

$$
\begin{align*}
g(t) 
    &= \mathbb E[(tX - Y)^2]\\
    &= t^2\mathbb E[X^2] - 2t\mathbb E[XY] + E[Y^2]\\
    &\geq 0
\end{align*}
$$

ã“ã®ã¨ãï¼Œ$g(t)$ ã¯non-negativeãªã®ã§åˆ¤åˆ¥å¼ã«ã¤ã„ã¦ä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹

$$
D/4 = \left(\mathbb E[XY]\right)^2 - \mathbb E[X^2]\mathbb E[Y^2]\leq 0 
$$

å¾“ã£ã¦ï¼Œ$\left(\mathbb E[XY]\right)^2 \leq \mathbb E[X^2]\mathbb E[Y^2]$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: Triangle inequality**
<br>

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ï¼Œä»¥ä¸‹ã®ã‚ˆã†ãªä¸‰è§’ä¸ç­‰å¼ãŒæˆç«‹ã™ã‚‹ã“ã¨ã‚’ç¤ºã›

$$
\sqrt{\mathbb E[(X+Y)^2]} \leq \sqrt{\mathbb E[X^2]} + \sqrt{\mathbb E[Y^2]}
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„ã®ä¸ç­‰å¼ã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«ç¤ºã›ã‚‹

$$
\begin{align*}
\mathbb E[(X+Y)^2]
    &= \mathbb E[X^2] + 2\mathbb E[XY] + \mathbb E[Y^2]\\
    &= \mathbb E[X^2] + 2\sqrt{(\mathbb E[XY])^2} + \mathbb E[Y^2]\\
    &\leq \mathbb E[X^2] + 2\sqrt{\mathbb E[X^2]\mathbb E[Y^2]} + \mathbb E[Y^2]\\
    &= (\sqrt{\mathbb E[X^2]} + \sqrt{\mathbb E[Y^2]})^2
\end{align*}
$$

ä¸¡è¾ºã«ã¤ã„ã¦ï¼Œsquare rootã‚’ã¨ã‚‹ã¨ï¼Œ

$$
\sqrt{\mathbb E[(X+Y)^2]} \leq \sqrt{\mathbb E[X^2]} + \sqrt{\mathbb E[Y^2]}
$$


:::



### æ¡ä»¶ä»˜ãæœŸå¾…å€¤

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Law of Total Expectation**
<br>

$$
\mathbb E[Y] = \mathbb E[\mathbb E[Y\vert X]]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[\mathbb E[Y\vert X]]
    &= \int \mathbb E[Y\vert X=u]f_X(u)\mathrm{d}u\\
    &= \int \left[\int t f_Y(t\vert x=u)\mathrm{d}t\right]f_X(u)\mathrm{d}u\\
    &= \int \int t f_Y(t\vert x=u)f_X(u)\mathrm{d}u\mathrm{d}t\\
    &= \int t\left[\int f_{X,Y}(u, t)\mathrm{d}u\right]\mathrm{d}t\\
    &= \int t f_Y(t)\mathrm{d}t\\
    &= \mathbb E[Y]
\end{align*}
$$

:::

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: CEF Decomposition Property**
<br>

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ï¼Œ

$$
Y = \mathbb E[Y\vert X] + \epsilon
$$

ã¨ã—ãŸã¨ãï¼Œ

1. $\epsilon$ ã¯ $X$ ã«ã¤ã„ã¦ mean-independent, i.e., $\mathbb E[\epsilon\vert X] = 0$
2. $\epsilon$ ã¯ $X$ ã®ä»»æ„ã®é–¢æ•°ã«å¯¾ã—ã¦ç„¡ç›¸é–¢, i.e., $\operatorname{Cov}(h(X), \epsilon) = 0$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

<strong > &#9654;&nbsp; (1)</strong>

$$
\begin{align*}
\mathbb E[\epsilon\vert X] 
    &= \mathbb E[Y - \mathbb E[Y\vert X]\vert X]\\
    &= \mathbb E[Y\vert X] - \mathbb E[Y\vert X]\\
    &= 0
\end{align*}
$$

<strong > &#9654;&nbsp; (2)</strong>

$$
\begin{align*}
\mathbb E[h(X)\epsilon] &= E[\mathbb E[h(X)\epsilon\vert X]]\\
                        &= E[h(X)\mathbb E[\epsilon\vert X]]\\
                        &= 0 \quad \because{\text{mean independence}}
\end{align*}
$$


:::

<div class="blog-custom-border">
<strong >ğŸ“˜ REMARKS</strong> <br>

CEF Decomposition Propertyã¯ï¼Œç¢ºç‡å¤‰æ•° $Y$ ã¯ç¢ºç‡å¤‰æ•° $X$ ã§èª¬æ˜ã§ãã‚‹ãƒ‘ãƒ¼ãƒˆã¨ï¼Œ$X$ ã®ä»»æ„ã®é–¢æ•°ã¨ç›´è¡Œï¼ˆorthogonalï¼‰
ãªèª¤å·®é …ã®ãƒ‘ãƒ¼ãƒˆã«åˆ†è§£ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼

</div>




## Markov and Chebyshev Inequalities

ç¢ºç‡å¤‰æ•° $X$ ã«ã¤ã„ã¦ï¼Œç¢ºç‡å¯†åº¦é–¢æ•°ã‚„åˆ†å¸ƒé–¢æ•°ãŒã‚ã‹ã£ã¦ã„ã‚‹çŠ¶æ³ã¯å°‘ãªã„ã§ã™ï¼ã¾ãŸï¼Œãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚ŒãŸã¨ã—ã¦ã‚‚
ãã‚Œã‚‰ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã¯ã‹ã‚“ãŸã‚“ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ãã®ä¸­ã§ï¼Œ

- $X$ ãŒ mean $\mu$ ã‹ã‚‰ã©ã‚Œãã‚‰ã„é›¢ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã‹
- $\Pr(\vert X \leq a\vert )$ ã®upper boundã¯ã©ã‚Œãã‚‰ã„ã‹ï¼Ÿ

ã¨ã„ã†çµ±è¨ˆçš„æ¨æ¸¬ã‚’ã—ãŸã„ã¨ãã«ä½¿ç”¨ã•ã‚Œã‚‹Markov and Chebyshev Inequalitiesã‚’è§£èª¬ã—ã¾ã™ï¼

### Markovâ€™s Inequality

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Markovâ€™s Inequality**
<br>

non-negative ç¢ºç‡å¤‰æ•° $X \geq 0$ï¼Œconstant $k >0$ ã«ã¤ã„ã¦ä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹

$$
\Pr(X \geq k) \leq \frac{\mathbb E[X]}{k}
$$

ã¤ã¾ã‚Šï¼Œ

$$
\Pr(X \geq k\mathbb E[X]) \leq \frac{1}{k}
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\mathbb E[X] &= \int_0^\infty xf(x)\mathrm{d}x\\
             &= \int_0^k xf(x)\mathrm{d}x + \int_k^\infty xf(x)\mathrm{d}x\\
             &\leq \int_k^\infty xf(x)\mathrm{d}x\\
             &\leq \int_k^\infty kf(x)\mathrm{d}x\\
             &= k \Pr(X \geq k)
\end{align*}
$$

:::

::: {.callout-note collapse="false" icon=false}
## Proof: å¤‰æ•°å¤‰æ›

$$
Y = 
\begin{cases}
    0 & \text{if} X < k\\[5pt]
    k & \text{if} X \geq k
\end{cases}
$$

ã®ã‚ˆã†ã«å¤‰æ•°å¤‰æ›ã‚’ã™ã‚‹ã¨å¸¸ã« $Y \leq X$ ã§ã‚ã‚‹ã®ã§ $\mathbb E[Y] \leq \mathbb E[X]$.

$$
\begin{align*}
&\mathbb E[Y] = k\Pr(X\geq k)\\
\Rightarrow &\Pr(X\geq k)\leq \frac{\mathbb E[X]}{k}
\end{align*}
$$

:::




<div class="blog-custom-border">
<strong >ğŸ“˜ REMARKS</strong> <br>

- Markovâ€™s inequalityã¯ ç¢ºç‡å¤‰æ•° $X$ ãŒnon-negative, population mean $\mu$ ã®çŸ¥è­˜ã®ã¿ã§ä½¿ç”¨å¯èƒ½
- ä¸€æ–¹ï¼Œboundå¹…ã¯å¤§ããï¼Œweakest inequalityã§ã‚ã‚‹

</div>

::: {#exm- .custom_problem }
<br>

ç‚¹æ•°ç¯„å›²ãŒ $\Omega_x=[0, 110]$ ã®è©¦é¨“ã‚’ã¤ã„ã¦ï¼Œãã®ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢ç¢ºç‡å¤‰æ•° $X$ ã‚’è€ƒãˆã‚‹ï¼åˆ†å¸ƒã®æƒ…å ±ã¯ã‚ã‹ã‚‰ãªã„ãŒ
population meanã¯ 25 ã§ã‚ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ï¼ã“ã®ã¨ãï¼Œ$\Pr(X \geq 100)$ ã®upper boundã¯Markov's inequalityã‚’ç”¨ã„ã¦
ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã§ãã¾ã™ï¼

$X$ ãŒnon-negativeãªã®ã§

$$
\begin{align*}
\Pr(X\geq 100) &\leq \frac{25}{100}\\
               &= \frac{1}{4}
\end{align*}
$$

:::

::: {#exm-binom-markov .custom_problem }
**: weak inequality**
<br>

$X_i \overset{\mathrm{iid}}{\sim} \operatorname{Bernoulli}(0.2)$ ã‚’20å›ç¹°ã‚Šè¿”ã™è©¦è¡Œã‚’è€ƒãˆã‚‹ï¼ã“ã®è©¦è¡Œã®çµæœã®ã‚¢ã‚¦ãƒˆã‚«ãƒ ã‚’ $Y$ ã¨ã—ãŸã¨ãï¼Œ

$$
\Pr(Y \geq 16) = \sum_{k=16}^{20} {}_{20}C_{k} 0.2^k 0.8^{20-k} \approx 1.38\cdot 10^{-8}
$$

ä¸€æ–¹ï¼ŒMarkov's inequalityã‚’ç”¨ã„ã‚‹ã¨

$$
\begin{align*}
\Pr(Y \geq 16) \leq \frac{4}{16} = \frac{1}{4}
\end{align*}
$$

ã“ã®ã‚ˆã†ã«ï¼Œboundå¹…ã¯å¤§ãã„ã“ã¨ãŒåˆ†ã‹ã‚‹ï¼

:::

### Chebyshevâ€™s Inequality

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Chebyshevâ€™s inequality**
<br>

$X \sim D(\mu, \sigma^2)$ ã¨ã™ã‚‹ï¼ãŸã ã—ï¼Œ$D$ ã®å½¢çŠ¶ã¯ã‚ã‹ã‚‰ãªã„ï¼å®Ÿæ•° $\alpha >0$ ã«ã¤ã„ã¦ï¼Œä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹

$$
\Pr(\vert X - \mu \vert \geq \alpha) \leq \frac{\sigma^2}{\alpha^2}
$$

ã¤ã¾ã‚Šï¼Œ

$$
\Pr(\vert X - \mu \vert \geq \alpha \sigma) \leq \frac{1}{\alpha^2}
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$I = \{x: \vert x -\mu \vert \geq k\}$ ã¨ã™ã‚‹ï¼

$$
\begin{align*}
\sigma^2 &= \int_{\mathbb R} (x - \mu)^2f(x)\mathrm{d}x\\
         &\geq  \int_{I} (x - \mu)^2f(x)\mathrm{d}x\\
         &\geq  \int_{I} k^2f(x)\mathrm{d}x\\
         &= k^2 \Pr(\vert x - \mu\vert \geq k)
\end{align*}
$$

ä»¥ä¸Šã‚ˆã‚Šï¼Œ$\displaystyle\Pr(\vert X - \mu \vert \geq k) \leq \frac{\sigma^2}{k^2}$ ã‚’å¾—ã‚‹ï¼

:::

::: {.callout-note collapse="false" icon=false}
## Proof: using Markov's inequality

$(x - \mu)^2$ ã‚’ç¢ºç‡å¤‰æ•°ã¨è€ƒãˆã‚‹ã¨ï¼Œnon-negativeç¢ºç‡å¤‰æ•°ã«ãªã‚‹ï¼Œã¤ã¾ã‚ŠMarkov's inequalityã‚’ç”¨ã„ã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§

$$
\begin{align*}
\Pr(\vert x - \mu\vert \geq k) &= \Pr((x - \mu)^2 \geq k)\\
                               &\leq \frac{\mathbb E[(x - \mu)^2]}{k^2} \because{\text{Markov's inequality}}\\
                               &= \frac{\sigma^2}{k^2}
\end{align*}
$$


:::

::: {#exm- .custom_problem }
**Markov's inequality vs Chebyshevâ€™s inequality**
<br>

$X \sim \operatorname{Binom}(n=20, p=0.2)$ ã«ã¤ã„ã¦ï¼Œ[weak inequality](#exm-binom-markov)
ã§ç¢ºèªã—ãŸã‚ˆã†ã«ï¼ŒMarkov's inequalityã®ã‚ˆã‚Š

$$
\Pr(X \geq 16) = \Pr(X \geq 4\mathbb E[X]) \leq \frac{1}{4}
$$

ä¸€æ–¹ï¼ŒChebyshevâ€™s inequalityã‚’ç”¨ã„ã‚‹ã¨

$$
\begin{align*}
\Pr(X \geq 16) &\leq \Pr(\vert X - 4\vert \geq 12)\\
               &\leq \frac{\operatorname{Var}(X)}{12^2}\\
               &\leq \frac{3.2}{12^2}\\
               &= \frac{1}{45}
\end{align*}
$$

:::

<div class="blog-custom-border">
<strong >ğŸ“˜ REMARKS</strong> <br>

- Chebyshevâ€™s inequalityã¯Markov's inqualityã¨ç•°ãªã‚Šï¼Œç¢ºç‡å¤‰æ•° $X$ ãŒnon-negativeã§ã‚ã‚‹å¿…è¦ã¯ãªã„
- meanã‹ã‚‰ã®è·é›¢ã«ã¤ã„ã¦ã®æƒ…å ±ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹

</div>

### Weak Law of Large Numbers

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Weak Law of Large Numbers**
<br>

å¹³å‡ $\mu$, åˆ†æ•£ $\sigma^2$ ã®åˆ†å¸ƒã«ç‹¬ç«‹ã«å¾“ã†ç¢ºç‡å¤‰æ•° $X_1, \cdots, X_n$ ã‚’è€ƒãˆã‚‹ï¼æ¨™æœ¬å¹³å‡ã‚’ $\overline{X_n} = \frac{1}{n}\sum_{i=1}^nX_i$ ã¨ã™ã‚‹ï¼

ã“ã®ã¨ãï¼Œä»»æ„ã®å®Ÿæ•° $\epsilon >0$ ã«å¯¾ã—ã¦ï¼Œ

$$
\lim_{n\to\infty}\Pr(\vert \overline{X_n} - \mu \vert > \epsilon) = 0
$$

ã¤ã¾ã‚Šï¼Œ**æ¨™æœ¬å¹³å‡ã¯æ¯å¹³å‡ã«ç¢ºç‡åæŸ**ã™ã‚‹ï¼

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

Chebyshevâ€™s inequalityã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«ç¤ºã›ã‚‹

$$
\begin{align*}
\lim_{n\to\infty}\Pr(\vert \overline{X_n} - \mu \vert > \epsilon) 
                &\leq \lim_{n\to\infty} \frac{\operatorname{Var}(\overline{X_n})}{\epsilon^2}\\
                &= \lim_{n\to\infty} \frac{\sigma^2}{n\epsilon^2}\\
                &=0
\end{align*}
$$

:::

## åˆ†æ•£

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**: BienaymÃ© Equality**
<br>

äº’ã„ã«ç‹¬ç«‹ãªç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹

$$
\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y)
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$$
\begin{align*}
\operatorname{Var}(X+Y)
    &= \mathbb E[((X+Y) - (\mu_X+\mu_Y))^2]\\
    &= \mathbb E[((X- \mu_X)+(Y - \mu_Y))^2]\\
    &= \mathbb E[(X- \mu_X)^2] + 2\mathbb E[(X- \mu_X)(Y- \mu_Y)] + \mathbb E[(Y- \mu_Y)^2]\\
    &= \mathbb E[(X- \mu_X)^2] + 2\mathbb E[(X- \mu_X)]\mathbb E[(Y- \mu_Y)] + \mathbb E[(Y- \mu_Y)^2] \quad \because{\text{ç‹¬ç«‹æ€§}}\\ 
    &= \operatorname{Var}(X) + \operatorname{Var}(Y)
\end{align*}
$$

:::

ãªãŠï¼Œç¢ºç‡å¤‰æ•° $X, Y$ ãŒç‹¬ç«‹ã§ã¯ãªã„å ´åˆã¯

$$
\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X, Y)
$$

ãŒæˆç«‹ã—ã¾ã™ï¼

### æ¡ä»¶ä»˜ãåˆ†æ•£

ç¢ºç‡å¤‰æ•° $X, Y$ ã«ã¤ã„ã¦ã®æ¡ä»¶ä»˜ãåˆ†æ•£ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ„å‘³ã‚’æŒã¤

- $\operatorname{Var}(X\vert Y=y)$ ã¯ï¼Œ$Y = y$ ã¨å›ºå®šã—ãŸã¨ãã® $X$ ã®åˆ†æ•£
- $\operatorname{Var}(X\vert Y)$ ã¯ï¼Œ$Y$ ãŒãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚ŒãŸå€¤ã«å›ºå®šã•ã‚ŒãŸå ´åˆã® $X$ ã®åˆ†æ•£

$\operatorname{Var}(X\vert Y)$ ã¯ $Y$ ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã«ä¾å­˜ã—ãŸç¢ºç‡å¤‰æ•°ã§ã‚ã‚‹ä¸€æ–¹ï¼Œ
$\operatorname{Var}(X\vert Y=y)$ ã¯ $y$ ã®é–¢æ•°ã¨ã„ã†é•ã„ãŒã‚ã‚‹

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**æ¡ä»¶ä»˜ãåˆ†æ•£**
<br>

$$
\operatorname{Var}(Y\vert X) = \mathbb E[(Y^2\vert X)] - (\mathbb E[(Y\vert X)])^2 = \mathbb E[(Y - \mathbb E[Y\vert X])^2\vert X]
$$


:::

</div>

<br>

<div class="blog-custom-border">
::: {#thm- .custom_problem }
**Law of Total Variance**
<br>

$$
\operatorname{Var}(Y) = \operatorname{Var}(\mathbb E[Y\vert X]) + \mathbb E_X[\operatorname{Var}(Y\vert X)]
$$

:::

</div>

::: {.callout-note collapse="false" icon=false}
## Proof

$\epsilon = Y - E\mathbb E[Y\vert X]$ ã¨ã—ãŸã¨ãï¼Œ$\epsilon$ ã¨ $E\mathbb E[Y\vert X]$ ã¯ç„¡ç›¸é–¢ãªã®ã§ï¼Œ

$$
\operatorname{Var}(Y) = \operatorname{Var}(\mathbb E[Y\vert X]) + \operatorname{Var}(\epsilon)
$$

$\mathbb E[\epsilon] = 0$ ã‚ˆã‚Šï¼Œ

$$
\begin{align*}
\operatorname{Var}(\epsilon)
    &= \mathbb E[\epsilon^2] - (\mathbb E[\epsilon])^2\\
    &= \mathbb E[\epsilon^2]\\
    &= \mathbb E_X(\mathbb E[\epsilon^2\vert X])\\
    &= \mathbb E_X[\operatorname{Var}(Y\vert X)]
\end{align*}
$$

å¾“ã£ã¦ï¼Œ

$$
\operatorname{Var}(Y) = \operatorname{Var}(\mathbb E[Y\vert X]) + \mathbb E_X[\operatorname{Var}(Y\vert X)]
$$

:::

Law of Total Varianceã‚ˆã‚Š $Y$ ã®åˆ†æ•£ã¯ï¼ŒCEFã®åˆ†æ•£ + èª¤å·®é …ã®åˆ†æ•£ã«åˆ†è§£ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼
å®Ÿå‹™ã«ãŠã‘ã‚‹åˆ†æã«ãŠã„ã¦ï¼Œè³ƒé‡‘ã®ãƒãƒ©ãƒ„ã‚­ã‚’

- è³ƒé‡‘ã‚’èª¬æ˜ã™ã‚‹å„å€‹äººã®ç‰¹å¾´ã®ãƒãƒ©ãƒ„ã‚­
- ç‰¹å¾´ã§èª¬æ˜ã™ã‚‹ã“ã¨ã®ã§ããªã„è³ƒé‡‘ã®ãƒãƒ©ãƒ„ã‚­(=èª¤å·®é …)ã®æœŸå¾…å€¤

ã«åˆ†è§£ã—ã¦è€ƒå¯Ÿã™ã‚‹éš›ã«Law of Total Varianceã‚’ä½¿ç”¨ã—ãŸã‚Šã—ã¾ã™ï¼