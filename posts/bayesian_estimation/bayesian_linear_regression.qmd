---
title: "ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ç·šå½¢å›å¸°"
author: "Ryo Nakagami"
date: "2024-10-03"
date-modified: last-modified
number_sections: false
code-fold: true
comments:
    utterances:
         repo: RyoNakagami/statistics-for-regression-monkey
         label: discussion
jupyter: python3
---

## Recap of OLS

$(Y_i, \pmb{x}_i)$ ã‚’random vectorã¨ã—ã¦ï¼Œ$Y_i \in \mathbb R, \pmb{x}_i \in \mathbb R^k$ ã¨ã—ã¾ã™ï¼
ç·šå½¢å›å¸°ã§ã¯ï¼Œ$(Y_i, \pmb{x}_i)$ ã«ã¤ã„ã¦æ¬¡ã®ã‚ˆã†ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã«é–¢ã—ã¦ç·šå½¢ãªé–¢ä¿‚ãŒã‚ã‚‹ã¨ã—ã¦å›å¸°å•é¡Œã‚’è€ƒãˆã¾ã™ï¼

$$
\begin{align*}
Y_i &= \pmb{x}_i^T\pmb{\beta} + \epsilon_i \quad (\epsilon_i \text{: errors})
\end{align*}
$$

ã“ã‚Œã‚’è¡Œåˆ—è¡¨ç¾ã«ã—ãŸå ´åˆï¼Œ$\pmb{Y} = (Y_1, \cdots, Y_n)^T$, $\pmb{X} = (\pmb{x}_1, \cdots, \pmb{x}_n)^T$ ã¨ã—ã¦

$$
\pmb{Y} = \pmb{X}\pmb{\beta} + \pmb{\epsilon}
$$


<div class="blog-custom-border">
<strong>Assumption: OLSã®ä»®å®š</strong> <br>

(1) mutually independent and identical distribution

    $$
    (Y_i, \pmb{x_i^T}), i = 1, \cdots, n \text{ ã¯ i.i.d}
    $$

(2) Linear model

    $$
    \pmb{Y} = \pmb{X}\pmb{\beta} + \pmb{\epsilon}
    $$

(3) error term is conditional-mean zero

    $$
    \mathbb E[\epsilon_i\vert \pmb{x}_i] = 0
    $$

(4) finite moments

    $$
    \begin{align*}
        \mathbb E[Y_i^2] &< \infty\\
        \mathbb E[\|\pmb{x}_i^2\|] &< \infty
    \end{align*}
    $$

(5) Full rank condition

    $\operatorname{rank}(\pmb{X}) = k$ï¼Œã¤ã¾ã‚Šå®Œå…¨ãªå¤šé‡å…±ç·šæ€§ãŒãªã„

</div>

è­°è«–ã®å˜ç´”åŒ–ã®ãŸã‚ï¼Œä»¥ä¸‹ã§ã¯

$$
\epsilon_1, \cdots, \epsilon_m \overset{\mathrm{iid}}{\sim} N(0, \sigma^2)
$$ {#eq-normality}

ã¤ã¾ã‚Šï¼Œ Homoscedasticityã®ä»®å®šã®ä¸‹ã§è©±ã‚’é€²ã‚ã¾ã™ï¼

<strong > &#9654;&nbsp; $\pmb\beta$ ã®æ¨å®š</strong>

ä»®å®š (2), (3) ã‚ˆã‚Š $\mathbb E[Y_i\vert \pmb{x}_i] = \pmb{x}_i^T\pmb{\beta}$ ã§ã‚ã‚‹ã®ã§ï¼Œ

$$
\pmb{\beta} = \arg\min_{b} \mathbb E[(Y_i - \pmb{x}_i^T\pmb{b})^2]
$${#eq-OLS-loss-func}

the joint distribution $(Y_i, \pmb{x}_i)$ ã¯äº‹å‰ã«ã¯çŸ¥ã‚‰ã‚Œã¦ã„ãªã„ã®ã§ã€sample analogã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«æ¨å®šã—ã¾ã™

$$
\begin{align*}
\widehat{\pmb{\beta}} 
    &= \arg\min_{b} \frac{1}{N}\sum_{i=1}^N(Y_i - \pmb{x}_i^T\pmb{b})^2\\
    &= \arg\min_{b} (\pmb{Y} - \pmb{X}\pmb{\beta})^T(\pmb{Y} - \pmb{X}\pmb{\beta})
\end{align*}
$$

ä¸Šè¨˜ã«ã¤ã„ã¦ï¼Œ$b$ ã®FOCã‚’ã¨ã‚‹ã¨

$$
\pmb{X}^T(\pmb{Y} - \pmb{X}\widehat{\pmb{\beta}}) = 0
$$

ä»®å®š (5) ã‚ˆã‚Š $\operatorname{rank}(\pmb{X}) = k$, ã¤ã¾ã‚Šfull rankã§ã‚ã‚‹ã®ã§ $(\pmb{X}^T\pmb{X})^{-1}$ ãŒã¨ã‚Œã¾ã™ï¼å¾“ã£ã¦ï¼Œ

$$
\widehat{\pmb{\beta}} = (\pmb{X}^T\pmb{X})^{-1}(\pmb{X}^T\pmb{Y})
$${#eq-OLS-estimator}

<strong > &#9654;&nbsp; $\widehat{\pmb{\beta}}$ ã®æ¼¸è¿‘åˆ†æ•£</strong>

population $\pmb{\beta}$ ã‚’ç”¨ã„ã¦error termã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ã—ã¾ã™

$$
\begin{align*}
\pmb{Y}_i 
    &= \pmb{x}_i^T\pmb{\beta} + (\pmb{Y_i} - \pmb{x}_i^T\pmb{\beta})\\
    &= \pmb{x}_i^T\pmb{\beta} + \epsilon_i
\end{align*}
$$

ä¸¡è¾ºã« $[\pmb{X}^T\pmb{X}]^{-1}\pmb{X}^T$ ã‚’ã‹ã‘ã‚‹ã¨ @eq-OLS-estimator ã‚ˆã‚Š

$$
\widehat{\pmb{\beta}} = \pmb{\beta} + \left[\sum\pmb{x}_i\pmb{x}_i^T\right]^{-1}\sum \pmb{x}_i\epsilon_i
$$

ã¤ãã« $\sqrt{n}(\widehat{{\pmb{\beta}}} - \pmb{\beta})$ ã§æ¼¸è¿‘åˆ†æ•£ã‚’è€ƒãˆã‚‹ã¨ï¼Œ

$$
\sqrt{n}(\widehat{{\pmb{\beta}}} - \pmb{\beta}) = n\left[\sum\pmb{x}_i\pmb{x}_i^T\right]^{-1}\frac{1}{\sqrt{n}}\sum \pmb{x}_i\epsilon_i
$$

$\mathbb E[\pmb{x}_ie_i] = 0$ ã§ã‚ã‚‹ã®ã§ï¼Œ$\frac{1}{\sqrt{n}}\sum \pmb{x}_i\epsilon_i$ ã¯ location ã¯ 0ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼å¾“ã£ã¦ï¼Œ

$$
\frac{1}{\sqrt{n}}\sum \pmb{x}_i\epsilon_i = \sqrt{n}\left\{\frac{1}{n}\sum \pmb{x}_i\epsilon_i - 0\right\}\overset{\mathrm{d}}{\to} N(0, \mathbb E[\pmb{x}_i\pmb{x}^T\epsilon_i^2]) 
$$

ã¾ãŸï¼Œ$\frac{1}{n}\sum\pmb{x}_i\pmb{x}_i^T$ ãŒ $\mathbb E[\pmb{x}_i\pmb{x}_i^T]$ ã«ç¢ºç‡åæŸã™ã‚‹ã®ã§ï¼Œã‚¹ãƒ©ãƒ„ã‚­ãƒ¼ã®å®šç†ã¨CLTã«ã‚ˆã‚Š

$$
\begin{align*}
\sqrt{n}(\widehat{{\pmb{\beta}}} - \pmb{\beta}) \overset{\mathrm{d}}{\to}  N(0, \mathbb E[\pmb{x}_i\pmb{x}]^{-1}\mathbb E[\pmb{x}_i\pmb{x}^T\epsilon_i^2]\mathbb E[\pmb{x}_i\pmb{x}]^{-1})
\end{align*}
$$

@eq-normality ã‚ˆã‚Š

$$
\mathbb E[\pmb{x}_i\pmb{x}^T\epsilon_i^2] = \sigma^2\mathbb E[\pmb{x}_i\pmb{x}^T]
$$

ãŒæˆç«‹ã™ã‚‹ã®ã§

$$
\sqrt{n}(\widehat{{\pmb{\beta}}} - \pmb{\beta}) \overset{\mathrm{d}}{\to}  N(0, \sigma^2\mathbb E[\pmb{x}_i\pmb{x}]^{-1})
$$

$\pmb{X}$ ã§æ¡ä»¶ä»˜ã‘ãŸåˆ†æ•£ã¯

$$
\begin{align*}
\operatorname{Var}(\widehat{\pmb{\beta}}\vert \pmb{X})
    & = \mathbb E[(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb\epsilon\pmb\epsilon^T\pmb{X}(\pmb{X}^T\pmb{X})^{-1}\vert \pmb{X}]\\
&= (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\mathbb E[\pmb\epsilon\pmb\epsilon^T\vert \pmb{X}](\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\\
&= \sigma^2(\pmb{X}^T\pmb{X})^{-1}
\end{align*}
$$


<strong > &#9654;&nbsp; $\pmb{Y}$ ã®æ¡ä»¶ä»˜ãåŒæ™‚åˆ†å¸ƒ </strong>

@eq-normality ã‚ˆã‚Š$\pmb{Y}$ ã®æ¡ä»¶ä»˜ãåŒæ™‚ç¢ºç‡å¯†åº¦ã¯

$$
\begin{align*}
p&(Y_1, \cdots, Y_n\vert \pmb{x_1}, \cdots, \pmb{x_n}, \pmb{\beta}, \sigma^2)\\
 &= \prod p(Y_i\vert \pmb{x_i}, \pmb{\beta}, \sigma^2) \quad \because\text{i.i.d}\\
 &= (2\pi\sigma^2)^{-n/2}\exp\{- \frac{1}{2\sigma^2}\sum_{i=1}^n (Y_i - \pmb{x}_i^T\pmb{\beta})^2\}
\end{align*}
$${#eq-joint-normality}

ã¤ã¾ã‚Šï¼Œ

$$
\{\pmb{Y} \vert \pmb{X}, \pmb{\beta}, \sigma\} \sim \operatorname{N}(\pmb{X}\pmb{\beta}, \sigma^2\pmb{I}) 
$$

@eq-joint-normality ã®å°¤åº¦æœ€å¤§åŒ–ã¯

$$
\min\sum_{i=1}^n (Y_i - \pmb{x}_i^T\pmb{\beta})^2
$$

ã‚’è§£ãã“ã¨ã§å¾—ã‚‰ã‚Œã¾ã™ãŒï¼Œã“ã‚Œã¯ @eq-OLS-loss-func ã®å•é¡Œã¨å¯¾å¿œã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼

## Bayesian Linear Regression

<strong > &#9654;&nbsp; $\pmb\beta$ ã®äº‹å¾Œåˆ†å¸ƒã®å°å‡º</strong>

$\pmb{\beta}$ ã«ã¤ã„ã¦ã®äº‹å‰åˆ†å¸ƒã‚’

$$
\pmb{\beta} \sim \operatorname{N}(\pmb{\beta}_0, \pmb{\Sigma}_0)
$$

äº‹å¾Œåˆ†å¸ƒã¯

$$
\begin{align*}
p&(\pmb\beta\vert \pmb Y, \pmb X, \sigma^2)\\
&\propto p(\pmb Y\vert \pmb X, \pmb \beta, \sigma^2)\times p(\pmb\beta)\\
&\propto \exp\{-\frac{1}{2}(-2\pmb\beta^T\pmb X^T\pmb y/\sigma^2 + \pmb\beta^T\pmb X^T\pmb X\pmb\beta/\sigma^2) - \frac{1}{2}(-2 \pmb\beta^T\pmb\Sigma_0^{-1}\pmb\beta_0 + \pmb\beta^T\pmb\Sigma_0^{-1}\pmb\beta)\} \\
&= \exp\{\pmb\beta^T(\pmb\Sigma_0^{-1}\pmb\beta_0 + \pmb X^T\pmb Y/\sigma^2) - \frac{1}{2}\pmb\beta^T(\pmb\Sigma_0^{-1} + \pmb X^T\pmb X/\sigma^2)\pmb\beta\}
\end{align*}
$$

å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã—ã¦ã„ã‚‹ã®ã§ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«ã¾ã¨ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™

$$
\begin{align*}
\operatorname{Var}(\pmb\beta\vert\pmb Y, \pmb X, \sigma^2)
    &= (\pmb\Sigma_0^{-1} + \pmb X^T\pmb X/\sigma^2)^{-1}\\
\mathbb E[\pmb\beta\vert\pmb Y, \pmb X, \sigma^2]
    &= (\pmb\Sigma_0^{-1} + \pmb X^T\pmb X/\sigma^2)^{-1}(\pmb\Sigma_0^{-1}\pmb\beta_0 + \pmb X^T\pmb Y/\sigma^2)
\end{align*}
$${#eq-posterior-beta}

::: {.nte- .callout-tip icon="false"}
# ğŸµ Green Tea Break
@eq-posterior-beta ã«ã¤ã„ã¦

$$
\begin{gather}
\pmb\beta_0 = \pmb 0\\
\pmb\Sigma_0 = \frac{\sigma^2}{\lambda}\pmb I 
\end{gather}
$$

ã¨äº‹å‰åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹ã¨ï¼Œ

$$
\begin{align*}
\mathbb E[\pmb\beta\vert\pmb Y, \pmb X, \sigma^2] = (\lambda\pmb I + \pmb X^T\pmb X)^{-1}\pmb X^T\pmb Y
\end{align*}
$$

ã¨ãªã‚Šï¼Œæ¬¡ã«ã‚ˆã†ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ $\lambda$ ã®Ridge repressionæ¨å®šé‡ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼

Ridgeæ¨å®šé‡ã¯

$$
\pmb\beta_{ridge} =\arg\min (\pmb Y- \pmb X\pmb \beta)^T(\pmb Y-\pmb X\pmb \beta) + \lambda \|\pmb \beta\|^2
$$

ã®è§£ã¨ä¸€è‡´ã—ã¾ã™ï¼ã“ã‚Œã«ã¤ã„ã¦ï¼ŒFOCã‚’ã¨ã‚‹ã¨

$$
\pmb\beta_{ridge} = (\pmb X^T\pmb X + \lambda \pmb I)^{-1}\pmb X^T\pmb Y 
$$

å‚è€ƒã¾ã§ã§ã™ãŒï¼ŒRidgeæ¨å®šé‡ã®æ¡ä»¶ä»˜ãæ¼¸è¿‘åˆ†æ•£ã¯

$$
\begin{align*}
\operatorname{Var}(\pmb\beta_{ridge}\vert \pmb X) &= \sigma^2(\pmb X^T\pmb X + \lambda I_p)^{-1}\pmb X^T\pmb X(\pmb X^T\pmb X + \lambda \pmb I)^{-1}\\
&\leq \sigma^2(\pmb X^T\pmb X)^{-1} = V(\pmb\beta_{ols}\vert \pmb X) 
\end{align*}
$$

:::

<strong > &#9654;&nbsp; $\sigma^2$ ã®äº‹å‰åˆ†å¸ƒ</strong>

å¤šãã®æ­£è¦ãƒ¢ãƒ‡ãƒ«ã§ã¯ $\sigma^2$ ã®æº–å…±å½¹äº‹å‰åˆ†å¸ƒã¯é€†ã‚¬ãƒ³ãƒåˆ†å¸ƒã§ã™ï¼
$\gamma = 1/\sigma^2$ ã‚’è¦³æ¸¬ã®ç²¾åº¦ã¨ã¿ãªã—ï¼Œ

$$
\gamma \sim \operatorname{Ga}(v_0/2, v_0\sigma^2_0/2)
$$

ã¨äº‹å‰åˆ†å¸ƒã‚’è¨­å®šã™ã‚‹ã¨

$$
\begin{align*}
p&(\gamma\vert\pmb Y, \pmb X, \pmb\beta)\\[5pt]
&\propto p(\gamma)\times p(\pmb Y \vert \gamma, \pmb X, \pmb\beta)\\
&\propto \left[\gamma^{v_0/2 -1} \exp(-\gamma\times v_0\sigma^2_0/2)\right] \times \left[\gamma^{n/2}\exp(-\gamma \times \operatorname{SSR}(\pmb\beta)/2)\right]\\
&= \gamma^{(v_0+n)/2 -1} \exp(-\gamma [v_0\sigma^2_0 + \operatorname{SSR}(\pmb\beta)]/2)\\[5pt]
&\text{where }\quad \operatorname{SSR}(\pmb\beta) = (\pmb Y - \pmb X\pmb\beta)^T(\pmb Y - \pmb X\pmb\beta)
\end{align*}
$$

ã“ã‚Œã¯ã‚¬ãƒ³ãƒåˆ†å¸ƒã¨ã¿ãªã›ã‚‹ã®ã§ï¼Œ$\gamma = 1/\sigma^2$ ã‚ˆã‚Šï¼Œäº‹å¾Œåˆ†å¸ƒã¯

$$
\{\sigma^2\vert\pmb Y, \pmb X, \pmb\beta\} \sim \operatorname{inverse-gamma}((v_0+n)/2, [v_0\sigma^2_0 + \operatorname{SSR}(\pmb\beta)]/2)
$$

å¾“ã£ã¦ï¼Œãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ‰‹é †ã§è¡Œã„ã¾ã™.

:::{.nte- .callout-note}
## æ¨å®šæ‰‹é †

<strong > &#9654;&nbsp; Step 1: $\pmb\beta$ ã®æ›´æ–°</strong>
    
1. $\pmb V = \operatorname{Var}(\pmb\beta\vert \pmb Y, \pmb X, \sigma^{2}_{(s)}), \pmb m = \mathbb E[\pmb\beta\vert \pmb Y, \pmb X, \sigma^{2}_{(s)}]$ ã®è¨ˆç®—
2. $\pmb\beta_{(s+1)}$ ã‚’æ¬¡ã®ã‚ˆã†ã«ç”Ÿæˆ

$$
\pmb\beta_{(s+1)}\sim \operatorname{N}(\pmb m, \pmb V)
$$ 

<strong > &#9654;&nbsp; Step 2: $\sigma^2$ ã®æ›´æ–°</strong>

1. $\operatorname{SSR}(\pmb\beta_{(s+1)})$ ã‚’è¨ˆç®—
2. $\sigma^{2}_{(s+1)}$ ã‚’æ¬¡ã®ã‚ˆã†ã«ç”Ÿæˆ

$$
\sigma^{2}_{(s+1)}\sim \operatorname{inverse-gamma}((v_0+n)/2, [v_0\sigma^2_0 + \operatorname{SSR}(\pmb\beta_{(s+1)})]/2)
$$

:::