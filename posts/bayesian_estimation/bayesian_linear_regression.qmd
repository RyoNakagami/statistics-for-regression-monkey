---
title: "ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ç·šå½¢å›å¸°"
author: "Ryo Nakagami"
date: "2024-10-03"
date-modified: last-modified
number_sections: false
code-fold: true
comments:
    utterances:
         repo: RyoNakagami/statistics-for-regression-monkey
         label: discussion
jupyter: python3
---

## Recap of OLS

$(Y_i, \pmb{x}_i)$ ã‚’random vectorã¨ã—ã¦ï¼Œ$Y_i \in \mathbb R, \pmb{x}_i \in \mathbb R^k$ ã¨ã—ã¾ã™ï¼
ç·šå½¢å›å¸°ã§ã¯ï¼Œ$(Y_i, \pmb{x}_i)$ ã«ã¤ã„ã¦æ¬¡ã®ã‚ˆã†ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã«é–¢ã—ã¦ç·šå½¢ãªé–¢ä¿‚ãŒã‚ã‚‹ã¨ã—ã¦å›å¸°å•é¡Œã‚’è€ƒãˆã¾ã™ï¼

$$
\begin{align*}
Y_i &= \pmb{x}_i^T\pmb{\beta} + \epsilon_i \quad (\epsilon_i \text{: errors})
\end{align*}
$$

ã“ã‚Œã‚’è¡Œåˆ—è¡¨ç¾ã«ã—ãŸå ´åˆï¼Œ$\pmb{Y} = (Y_1, \cdots, Y_n)^T$, $\pmb{X} = (\pmb{x}_1, \cdots, \pmb{x}_n)^T$ ã¨ã—ã¦

$$
\pmb{Y} = \pmb{X}\pmb{\beta} + \pmb{\epsilon}
$$


<div class="blog-custom-border">
<strong>Assumption: OLSã®ä»®å®š</strong> <br>

(1) mutually independent and identical distribution

    $$
    (Y_i, \pmb{x_i^T}), i = 1, \cdots, n \text{ ã¯ i.i.d}
    $$

(2) Linear model

    $$
    \pmb{Y} = \pmb{X}\pmb{\beta} + \pmb{\epsilon}
    $$

(3) error term is conditional-mean zero

    $$
    \mathbb E[\epsilon_i\vert \pmb{x}_i] = 0
    $$

(4) finite moments

    $$
    \begin{align*}
        \mathbb E[Y_i^2] &< \infty\\
        \mathbb E[\|\pmb{x}_i^2\|] &< \infty
    \end{align*}
    $$

(5) Full rank condition

    $\operatorname{rank}(\pmb{X}) = k$ï¼Œã¤ã¾ã‚Šå®Œå…¨ãªå¤šé‡å…±ç·šæ€§ãŒãªã„

</div>

è­°è«–ã®å˜ç´”åŒ–ã®ãŸã‚ï¼Œä»¥ä¸‹ã§ã¯

$$
\epsilon_1, \cdots, \epsilon_m \overset{\mathrm{iid}}{\sim} N(0, \sigma^2)
$$ {#eq-normality}

ã¤ã¾ã‚Šï¼Œ Homoscedasticityã®ä»®å®šã®ä¸‹ã§è©±ã‚’é€²ã‚ã¾ã™ï¼

<strong > &#9654;&nbsp; $\pmb\beta$ ã®æ¨å®š</strong>

ä»®å®š (2), (3) ã‚ˆã‚Š $\mathbb E[Y_i\vert \pmb{x}_i] = \pmb{x}_i^T\pmb{\beta}$ ã§ã‚ã‚‹ã®ã§ï¼Œ

$$
\pmb{\beta} = \arg\min_{b} \mathbb E[(Y_i - \pmb{x}_i^T\pmb{b})^2]
$${#eq-OLS-loss-func}

the joint distribution $(Y_i, \pmb{x}_i)$ ã¯äº‹å‰ã«ã¯çŸ¥ã‚‰ã‚Œã¦ã„ãªã„ã®ã§ã€sample analogã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«æ¨å®šã—ã¾ã™

$$
\begin{align*}
\widehat{\pmb{\beta}} 
    &= \arg\min_{b} \frac{1}{N}\sum_{i=1}^N(Y_i - \pmb{x}_i^T\pmb{b})^2\\
    &= \arg\min_{b} (\pmb{Y} - \pmb{X}\pmb{\beta})^T(\pmb{Y} - \pmb{X}\pmb{\beta})
\end{align*}
$$

ä¸Šè¨˜ã«ã¤ã„ã¦ï¼Œ$b$ ã®FOCã‚’ã¨ã‚‹ã¨

$$
\pmb{X}^T(\pmb{Y} - \pmb{X}\widehat{\pmb{\beta}}) = 0
$$

ä»®å®š (5) ã‚ˆã‚Š $\operatorname{rank}(\pmb{X}) = k$, ã¤ã¾ã‚Šfull rankã§ã‚ã‚‹ã®ã§ $(\pmb{X}^T\pmb{X})^{-1}$ ãŒã¨ã‚Œã¾ã™ï¼å¾“ã£ã¦ï¼Œ

$$
\widehat{\pmb{\beta}} = (\pmb{X}^T\pmb{X})^{-1}(\pmb{X}^T\pmb{Y})
$${#eq-OLS-estimator}

<strong > &#9654;&nbsp; $\widehat{\pmb{\beta}}$ ã®æ¼¸è¿‘åˆ†æ•£</strong>

population $\pmb{\beta}$ ã‚’ç”¨ã„ã¦error termã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ã—ã¾ã™

$$
\begin{align*}
\pmb{Y}_i 
    &= \pmb{x}_i^T\pmb{\beta} + (\pmb{Y_i} - \pmb{x}_i^T\pmb{\beta})\\
    &= \pmb{x}_i^T\pmb{\beta} + \epsilon_i
\end{align*}
$$

ä¸¡è¾ºã« $[\pmb{X}^T\pmb{X}]^{-1}\pmb{X}^T$ ã‚’ã‹ã‘ã‚‹ã¨ @eq-OLS-estimator ã‚ˆã‚Š

$$
\widehat{\pmb{\beta}} = \pmb{\beta} + \left[\sum\pmb{x}_i\pmb{x}_i^T\right]^{-1}\sum \pmb{x}_i\epsilon_i
$$

ã¤ãã« $\sqrt{n}(\widehat{{\pmb{\beta}}} - \pmb{\beta})$ ã§æ¼¸è¿‘åˆ†æ•£ã‚’è€ƒãˆã‚‹ã¨ï¼Œ

$$
\sqrt{n}(\widehat{{\pmb{\beta}}} - \pmb{\beta}) = n\left[\sum\pmb{x}_i\pmb{x}_i^T\right]^{-1}\frac{1}{\sqrt{n}}\sum \pmb{x}_i\epsilon_i
$$

$\mathbb E[\pmb{x}_ie_i] = 0$ ã§ã‚ã‚‹ã®ã§ï¼Œ$\frac{1}{\sqrt{n}}\sum \pmb{x}_i\epsilon_i$ ã¯ location ã¯ 0ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼å¾“ã£ã¦ï¼Œ

$$
\frac{1}{\sqrt{n}}\sum \pmb{x}_i\epsilon_i = \sqrt{n}\left\{\frac{1}{n}\sum \pmb{x}_i\epsilon_i - 0\right\}\overset{\mathrm{d}}{\to} N(0, \mathbb E[\pmb{x}_i\pmb{x}^T\epsilon_i^2]) 
$$

ã¾ãŸï¼Œ$\frac{1}{n}\sum\pmb{x}_i\pmb{x}_i^T$ ãŒ $\mathbb E[\pmb{x}_i\pmb{x}_i^T]$ ã«ç¢ºç‡åæŸã™ã‚‹ã®ã§ï¼Œã‚¹ãƒ©ãƒ„ã‚­ãƒ¼ã®å®šç†ã¨CLTã«ã‚ˆã‚Š

$$
\begin{align*}
\sqrt{n}(\widehat{{\pmb{\beta}}} - \pmb{\beta}) \overset{\mathrm{d}}{\to}  N(0, \mathbb E[\pmb{x}_i\pmb{x}]^{-1}\mathbb E[\pmb{x}_i\pmb{x}^T\epsilon_i^2]\mathbb E[\pmb{x}_i\pmb{x}]^{-1})
\end{align*}
$$

@eq-normality ã‚ˆã‚Š

$$
\mathbb E[\pmb{x}_i\pmb{x}^T\epsilon_i^2] = \sigma^2\mathbb E[\pmb{x}_i\pmb{x}^T]
$$

ãŒæˆç«‹ã™ã‚‹ã®ã§

$$
\sqrt{n}(\widehat{{\pmb{\beta}}} - \pmb{\beta}) \overset{\mathrm{d}}{\to}  N(0, \sigma^2\mathbb E[\pmb{x}_i\pmb{x}]^{-1})
$$

$\pmb{X}$ ã§æ¡ä»¶ä»˜ã‘ãŸåˆ†æ•£ã¯

$$
\begin{align*}
\operatorname{Var}(\widehat{\pmb{\beta}}\vert \pmb{X})
    & = \mathbb E[(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb\epsilon\pmb\epsilon^T\pmb{X}(\pmb{X}^T\pmb{X})^{-1}\vert \pmb{X}]\\
&= (\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\mathbb E[\pmb\epsilon\pmb\epsilon^T\vert \pmb{X}](\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\\
&= \sigma^2(\pmb{X}^T\pmb{X})^{-1}
\end{align*}
$$


<strong > &#9654;&nbsp; $\pmb{Y}$ ã®æ¡ä»¶ä»˜ãåŒæ™‚åˆ†å¸ƒ </strong>

@eq-normality ã‚ˆã‚Š$\pmb{Y}$ ã®æ¡ä»¶ä»˜ãåŒæ™‚ç¢ºç‡å¯†åº¦ã¯

$$
\begin{align*}
p&(Y_1, \cdots, Y_n\vert \pmb{x_1}, \cdots, \pmb{x_n}, \pmb{\beta}, \sigma^2)\\
 &= \prod p(Y_i\vert \pmb{x_i}, \pmb{\beta}, \sigma^2) \quad \because\text{i.i.d}\\
 &= (2\pi\sigma^2)^{-n/2}\exp\{- \frac{1}{2\sigma^2}\sum_{i=1}^n (Y_i - \pmb{x}_i^T\pmb{\beta})^2\}
\end{align*}
$${#eq-joint-normality}

ã¤ã¾ã‚Šï¼Œ

$$
\{\pmb{Y} \vert \pmb{X}, \pmb{\beta}, \sigma\} \sim \operatorname{N}(\pmb{X}\pmb{\beta}, \sigma^2\pmb{I}) 
$$

@eq-joint-normality ã®å°¤åº¦æœ€å¤§åŒ–ã¯

$$
\min\sum_{i=1}^n (Y_i - \pmb{x}_i^T\pmb{\beta})^2
$$

ã‚’è§£ãã“ã¨ã§å¾—ã‚‰ã‚Œã¾ã™ãŒï¼Œã“ã‚Œã¯ @eq-OLS-loss-func ã®å•é¡Œã¨å¯¾å¿œã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼

## Bayesian Linear Regression

<strong > &#9654;&nbsp; $\pmb\beta$ ã®äº‹å¾Œåˆ†å¸ƒã®å°å‡º</strong>

$\pmb{\beta}$ ã«ã¤ã„ã¦ã®äº‹å‰åˆ†å¸ƒã‚’

$$
\pmb{\beta} \sim \operatorname{N}(\pmb{\beta}_0, \pmb{\Sigma}_0)
$$

äº‹å¾Œåˆ†å¸ƒã¯

$$
\begin{align*}
p&(\pmb\beta\vert \pmb Y, \pmb X, \sigma^2)\\
&\propto p(\pmb Y\vert \pmb X, \pmb \beta, \sigma^2)\times p(\pmb\beta)\\
&\propto \exp\{-\frac{1}{2}(-2\pmb\beta^T\pmb X^T\pmb y/\sigma^2 + \pmb\beta^T\pmb X^T\pmb X\pmb\beta/\sigma^2) - \frac{1}{2}(-2 \pmb\beta^T\pmb\Sigma_0^{-1}\pmb\beta_0 + \pmb\beta^T\pmb\Sigma_0^{-1}\pmb\beta)\} \\
&= \exp\{\pmb\beta^T(\pmb\Sigma_0^{-1}\pmb\beta_0 + \pmb X^T\pmb Y/\sigma^2) - \frac{1}{2}\pmb\beta^T(\pmb\Sigma_0^{-1} + \pmb X^T\pmb X/\sigma^2)\pmb\beta\}
\end{align*}
$$

å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã—ã¦ã„ã‚‹ã®ã§ï¼Œä»¥ä¸‹ã®ã‚ˆã†ã«ã¾ã¨ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™

$$
\begin{align*}
\operatorname{Var}(\pmb\beta\vert\pmb Y, \pmb X, \sigma^2)
    &= (\pmb\Sigma_0^{-1} + \pmb X^T\pmb X/\sigma^2)^{-1}\\
\mathbb E[\pmb\beta\vert\pmb Y, \pmb X, \sigma^2]
    &= (\pmb\Sigma_0^{-1} + \pmb X^T\pmb X/\sigma^2)^{-1}(\pmb\Sigma_0^{-1}\pmb\beta_0 + \pmb X^T\pmb Y/\sigma^2)
\end{align*}
$${#eq-posterior-beta}

::: {.nte- .callout-tip icon="false"}
# ğŸµ Green Tea Break
@eq-posterior-beta ã«ã¤ã„ã¦

$$
\begin{gather}
\pmb\beta_0 = \pmb 0\\
\pmb\Sigma_0 = \frac{\sigma^2}{\lambda}\pmb I 
\end{gather}
$$

ã¨äº‹å‰åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹ã¨ï¼Œ

$$
\begin{align*}
\mathbb E[\pmb\beta\vert\pmb Y, \pmb X, \sigma^2] = (\lambda\pmb I + \pmb X^T\pmb X)^{-1}\pmb X^T\pmb Y
\end{align*}
$$

ã¨ãªã‚Šï¼Œæ¬¡ã«ã‚ˆã†ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ $\lambda$ ã®Ridge repressionæ¨å®šé‡ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼

Ridgeæ¨å®šé‡ã¯

$$
\pmb\beta_{ridge} =\arg\min (\pmb Y- \pmb X\pmb \beta)^T(\pmb Y-\pmb X\pmb \beta) + \lambda \|\pmb \beta\|^2
$$

ã®è§£ã¨ä¸€è‡´ã—ã¾ã™ï¼ã“ã‚Œã«ã¤ã„ã¦ï¼ŒFOCã‚’ã¨ã‚‹ã¨

$$
\pmb\beta_{ridge} = (\pmb X^T\pmb X + \lambda \pmb I)^{-1}\pmb X^T\pmb Y 
$$

å‚è€ƒã¾ã§ã§ã™ãŒï¼ŒRidgeæ¨å®šé‡ã®æ¡ä»¶ä»˜ãæ¼¸è¿‘åˆ†æ•£ã¯

$$
\begin{align*}
\operatorname{Var}(\pmb\beta_{ridge}\vert \pmb X) &= \sigma^2(\pmb X^T\pmb X + \lambda I_p)^{-1}\pmb X^T\pmb X(\pmb X^T\pmb X + \lambda \pmb I)^{-1}\\
&\leq \sigma^2(\pmb X^T\pmb X)^{-1} = V(\pmb\beta_{ols}\vert \pmb X) 
\end{align*}
$$

:::

<strong > &#9654;&nbsp; $\sigma^2$ ã®äº‹å‰åˆ†å¸ƒ</strong>

å¤šãã®æ­£è¦ãƒ¢ãƒ‡ãƒ«ã§ã¯ $\sigma^2$ ã®æº–å…±å½¹äº‹å‰åˆ†å¸ƒã¯é€†ã‚¬ãƒ³ãƒåˆ†å¸ƒã§ã™ï¼
$\gamma = 1/\sigma^2$ ã‚’è¦³æ¸¬ã®ç²¾åº¦ã¨ã¿ãªã—ï¼Œ

$$
\gamma \sim \operatorname{Ga}(v_0/2, v_0\sigma^2_0/2)
$$

ã¨äº‹å‰åˆ†å¸ƒã‚’è¨­å®šã™ã‚‹ã¨

$$
\begin{align*}
p&(\gamma\vert\pmb Y, \pmb X, \pmb\beta)\\[5pt]
&\propto p(\gamma)\times p(\pmb Y \vert \gamma, \pmb X, \pmb\beta)\\
&\propto \left[\gamma^{v_0/2 -1} \exp(-\gamma\times v_0\sigma^2_0/2)\right] \times \left[\gamma^{n/2}\exp(-\gamma \times \operatorname{SSR}(\pmb\beta)/2)\right]\\
&= \gamma^{(v_0+n)/2 -1} \exp(-\gamma [v_0\sigma^2_0 + \operatorname{SSR}(\pmb\beta)]/2)\\[5pt]
&\text{where }\quad \operatorname{SSR}(\pmb\beta) = (\pmb Y - \pmb X\pmb\beta)^T(\pmb Y - \pmb X\pmb\beta)
\end{align*}
$$

ã“ã‚Œã¯ã‚¬ãƒ³ãƒåˆ†å¸ƒã¨ã¿ãªã›ã‚‹ã®ã§ï¼Œ$\gamma = 1/\sigma^2$ ã‚ˆã‚Šï¼Œäº‹å¾Œåˆ†å¸ƒã¯

$$
\{\sigma^2\vert\pmb Y, \pmb X, \pmb\beta\} \sim \operatorname{inverse-gamma}((v_0+n)/2, [v_0\sigma^2_0 + \operatorname{SSR}(\pmb\beta)]/2)
$$

å¾“ã£ã¦ï¼Œãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ‰‹é †ã§è¡Œã„ã¾ã™.

:::{.nte- .callout-note}
## æ¨å®šæ‰‹é †

<strong > &#9654;&nbsp; Step 1: $\pmb\beta$ ã®æ›´æ–°</strong>
    
1. $\pmb V = \operatorname{Var}(\pmb\beta\vert \pmb Y, \pmb X, \sigma^{2}_{(s)}), \pmb m = \mathbb E[\pmb\beta\vert \pmb Y, \pmb X, \sigma^{2}_{(s)}]$ ã®è¨ˆç®—
2. $\pmb\beta_{(s+1)}$ ã‚’æ¬¡ã®ã‚ˆã†ã«ç”Ÿæˆ

$$
\pmb\beta_{(s+1)}\sim \operatorname{N}(\pmb m, \pmb V)
$$ 

<strong > &#9654;&nbsp; Step 2: $\sigma^2$ ã®æ›´æ–°</strong>

1. $\operatorname{SSR}(\pmb\beta_{(s+1)})$ ã‚’è¨ˆç®—
2. $\sigma^{2}_{(s+1)}$ ã‚’æ¬¡ã®ã‚ˆã†ã«ç”Ÿæˆ

$$
\sigma^{2}_{(s+1)}\sim \operatorname{inverse-gamma}((v_0+n)/2, [v_0\sigma^2_0 + \operatorname{SSR}(\pmb\beta_{(s+1)})]/2)
$$

:::

## Bayesian Linear Regression with JAX


```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn import linear_model
from regmonkey_style import stylewizard as sw
from jax import numpy as jnp, scipy as jscipy
from jax import grad
import scipy.stats
```



```{python}
df = pd.read_stata(
    "https://github.com/QuantEcon/lecture-python/blob/master/source/_static/lecture_specific/ols/maketable1.dta?raw=true"
).dropna(subset=["logpgp95", "avexpr"])
df.head()
```

```{python}
sw.set_templates("regmonkey_scatter")
df.plot(x='avexpr', y='logpgp95', kind='scatter')
plt.show()

```

<strong > &#9654;&nbsp; MLE</strong>

```{python}
def LogLikelihood(y, x, theta):
    N = x.shape[0]
    return np.log(scipy.stats.multivariate_normal.pdf(y.ravel(), (theta*x).ravel(), np.eye(N)*sigma_n**2))

x = df['avexpr'].values
y = df['logpgp95'].values

# FWL
x_mle = x - np.mean(x)
y_mle = y - np.mean(y)

# MLE
np.random.seed(42)
sigma_n = 10

T = np.linspace(-10,10,1000)
LL = [LogLikelihood(y_mle, x_mle, t1) for t1 in T]
fig, ax = plt.subplots()
ax.plot(T, LL)

beta_mle = np.sum(x_mle*y_mle)/np.sum(np.square(x_mle))
ax.vlines(beta_mle,*ax.get_ylim(), linestyle='--', label='$\\theta_{MLE}$')
ax.set_xlabel("$\\theta$");ax.set_ylabel("Log Likelihood");
plt.legend(bbox_to_anchor=[1,1]);
print("MLE beta: {}".format(beta_mle))
```


<strong > &#9654;&nbsp; OLS</strong>

```{python}
df_ols = df.copy()
df_ols['const'] = 1
reg_ols = sm.OLS(endog=df_ols['logpgp95'], exog=df_ols[['const', 'avexpr']], \
    missing='drop')
results = reg_ols.fit()
print(results.summary())
```

<strong > &#9654;&nbsp; Rdige</strong>

```{python}
reg_l2 = sm.OLS(endog=df_ols['logpgp95'], exog=df_ols[['const', 'avexpr']], \
    missing='drop')
result_l2 = reg_l2.fit_regularized(L1_wt=0., alpha=[1/2,1/2])
result_l2.params
```

```{python}
reg_l2 = sm.OLS(endog=y_mle, exog=x_mle, \
    missing='drop')
result_l2 = reg_l2.fit_regularized(L1_wt=0., alpha=[1])
result_l2.params
```

```{python}
from sklearn.linear_model import Ridge
from sklearn.linear_model import ridge_regression
coef, intercept = ridge_regression(x.reshape(-1,1), y, alpha=1.0, return_intercept=True)
print(coef, intercept)
```






<strong > &#9654;&nbsp; bayesian regression</strong>


```{python}

from IPython.display import set_matplotlib_formats
import seaborn as sns

from jax import random, vmap
import jax.numpy as jnp
from jax.scipy.special import logsumexp

import numpyro
from numpyro import handlers
from numpyro.diagnostics import hpdi
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS


def model(x=None, y=None):
    #a = numpyro.sample("a", dist.Normal(0.0, 1))
    M, A = 0.0, 0.0
    if x is not None:
        bM = numpyro.sample("bM", dist.Normal(0.0, 1))
        M = bM * x
    sigma = numpyro.sample("sigma", dist.Gamma(1.0))
    #mu = a + M
    mu = M
    numpyro.sample("obs", dist.Normal(mu, sigma), obs=y)

# Start from this source of randomness. We will split keys for subsequent operations.
rng_key = random.PRNGKey(0)
rng_key, rng_key_ = random.split(rng_key)

kernel = NUTS(model)
num_samples = 200
mcmc = MCMC(kernel, num_warmup=100, num_samples=num_samples)
mcmc.run(
    rng_key_, x=x_mle, y=y_mle
)
mcmc.print_summary()
samples_1 = mcmc.get_samples()
```


```{python}
#| code-fold: false
def model(x=None, y=None):
    a = numpyro.sample("a", dist.Normal(0.0, 1))
    M, A = 0.0, 0.0
    if x is not None:
        bM = numpyro.sample("bM", dist.Normal(0.0, 1))
        M = bM * x
    sigma = numpyro.sample("sigma", dist.InverseGamma(1.0))
    mu = a + M
    numpyro.sample("obs", dist.Normal(mu, sigma), obs=y)

# Start from this source of randomness. We will split keys for subsequent operations.
rng_key = random.PRNGKey(42)
rng_key, rng_key_ = random.split(rng_key)

kernel = NUTS(model)
num_samples = 2000
mcmc = MCMC(kernel, num_warmup=1000, num_samples=num_samples)
mcmc.run(
    rng_key_, x=x, y=y
)
mcmc.print_summary()
samples_1 = mcmc.get_samples()
```



```{python}
def plot_regression(x_data, y_mean, y_hpdi, y_data):
    # Sort values for plotting by x axis
    idx = jnp.argsort(x_data)
    x = x_data[idx]
    mean = y_mean[idx]
    hpdi = y_hpdi[:, idx]
    y = y_data[idx]

    # Plot
    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))
    ax.plot(x, mean)
    ax.plot(x, y, "o")
    ax.fill_between(x, hpdi[0], hpdi[1], alpha=0.3, interpolate=True)
    return ax


# Compute empirical posterior distribution over mu
posterior_mu = (
    jnp.expand_dims(samples_1["a"], -1)
    + jnp.expand_dims(samples_1["bM"], -1) * x
)

mean_mu = jnp.mean(posterior_mu, axis=0)
hpdi_mu = hpdi(posterior_mu, 0.9)
ax = plot_regression(x, mean_mu, hpdi_mu, y)
ax.set(
    xlabel="x value", ylabel="y-value", title="Regression line with 90% CI"
);


```



```{python}
from numpyro.infer import Predictive

rng_key, rng_key_ = random.split(rng_key)
prior_predictive = Predictive(model, num_samples=100)
prior_predictions = prior_predictive(rng_key_, x)[
    "obs"
]
mean_prior_pred = jnp.mean(prior_predictions, axis=0)
hpdi_prior_pred = hpdi(prior_predictions, 0.9)

ax = plot_regression(x, mean_prior_pred, hpdi_prior_pred, y)
ax.set(xlabel="x-value", ylabel="y-value", title="Predictions with 90% CI");


```


```{python}
df_bayes = df.copy()
rng_key, rng_key_ = random.split(rng_key)
predictive = Predictive(model, samples_1)
predictions = predictive(rng_key_, x=x)["obs"]
df_bayes["Mean Predictions"] = jnp.mean(predictions, axis=0)
df_bayes.loc[:, ["shortnam", "logpgp95", "Mean Predictions"]].head()
```


```{python}
# Plot predicted values

fix, ax = plt.subplots()
ax.scatter(df_bayes['avexpr'], df_bayes["Mean Predictions"],
        label='predicted')

# Plot observed values

ax.scatter(df_bayes['avexpr'], df_bayes["logpgp95"],
        label='observed')

ax.legend(loc='upper left')
ax.set_title('Bayes predicted values')
ax.set_xlabel('avexpr')
ax.set_ylabel('logpgp95')
plt.show()
```



```{python}
def log_likelihood(rng_key, params, model, *args, **kwargs):
    model = handlers.condition(model, params)
    model_trace = handlers.trace(model).get_trace(*args, **kwargs)
    obs_node = model_trace["obs"]
    return obs_node["fn"].log_prob(obs_node["value"])


def log_pred_density(rng_key, params, model, *args, **kwargs):
    n = list(params.values())[0].shape[0]
    log_lk_fn = vmap(
        lambda rng_key, params: log_likelihood(rng_key, params, model, *args, **kwargs)
    )
    log_lk_vals = log_lk_fn(random.split(rng_key, n), params)
    return (logsumexp(log_lk_vals, 0) - jnp.log(n)).sum()

rng_key, rng_key_ = random.split(rng_key)
print(
    "Log posterior predictive density: {}".format(
        log_pred_density(
            rng_key_,
            samples_1,
            model,
            x=x,
            y=y,
        )
    )
)

```